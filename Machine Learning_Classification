#LyX 2.1 created this file. For more info see http://www.lyx.org/
\lyxformat 474
\begin_document
\begin_header
\textclass article
\begin_preamble
\usepackage[BoldFont,SlantFont,CJKnumber,fallback]{xeCJK}%使用TexLive自带的xeCJK宏包，并启用加粗、斜体、CJK数字和备用字体选项
\setCJKmainfont{Songti SC}%设置中文衬线字体,若没有该字体,请替换该字符串为系统已有的中文字体,下同
\setCJKsansfont{STXihei}%中文无衬线字体
\setCJKmonofont{SimHei}%中文等宽字体
%中文断行和弹性间距在XeCJK中自动处理了
%\XeTeXlinebreaklocale “zh”%中文断行
%\XeTeXlinebreakskip = 0pt plus 1pt minus 0.1pt%左右弹性间距
\usepackage{indentfirst}%段落首行缩进
\setlength{\parindent}{2em}%缩进两个字符
\end_preamble
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package auto
\inputencoding utf8-plain
\fontencoding global
\font_roman default
\font_sans default
\font_typewriter default
\font_math auto
\font_default_family default
\use_non_tex_fonts true
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100
\graphics default
\default_output_format pdf4
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref true
\pdf_bookmarks true
\pdf_bookmarksnumbered true
\pdf_bookmarksopen true
\pdf_bookmarksopenlevel 3
\pdf_breaklinks false
\pdf_pdfborder false
\pdf_colorlinks true
\pdf_backref section
\pdf_pdfusetitle true
\pdf_quoted_options "“unicode=false”"
\papersize default
\use_geometry true
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 0
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 0
\use_package mhchem 1
\use_package stackrel 0
\use_package stmaryrd 0
\use_package undertilde 0
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 2.5cm
\topmargin 2.5cm
\rightmargin 2.5cm
\bottommargin 2.5cm
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Machine Learning_Classification: 9/13/2016
\end_layout

\begin_layout Author
Fan Yang
\begin_inset Foot
status open

\begin_layout Plain Layout
First version: Feb
\begin_inset Formula $4{}^{th}$
\end_inset

, 2013
\end_layout

\end_inset


\end_layout

\begin_layout Part*
Book Reference
\end_layout

\begin_layout Itemize
Logit, GDA ,Naive Bayes etc: http://www.stat.washington.edu/courses/stat527/s14/sli
des/LDA-QDA-KDE-NaiveBayes.pdf
\end_layout

\begin_layout Itemize
http://www.cs.cmu.edu/~avrim/ML14/
\end_layout

\begin_layout Itemize
http://www.cs.cornell.edu/Courses/cs6783/2014fa/
\end_layout

\begin_layout Itemize
ISL
\begin_inset CommandInset label
LatexCommand label
name "“ISL”"

\end_inset

: text_classical_An Introduction to Statistical Learning with Applicationsin
 R.
\end_layout

\begin_layout Itemize
ITF: Introduction to Time Series and Forecasting
\end_layout

\begin_layout Itemize
TSAA-R: Time Series analysis and its application with R examples.
\end_layout

\begin_layout Itemize
http://en.wikipedia.org/wiki/Predictive_modelling
\end_layout

\begin_layout Itemize
http://en.wikipedia.org/wiki/Kaggle
\end_layout

\begin_layout Itemize
Probability and statistics EBook :
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Flex URL
status open

\begin_layout Plain Layout

http://wiki.stat.ucla.edu/socr/index.php/Probability_and_statistics_EBook
\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Itemize
http://www.ics.uci.edu/~welling/teaching/courses.html 深入浅出
\end_layout

\begin_layout Itemize
ICA: ref_ICA_tutorial.pdf:
\end_layout

\begin_deeper
\begin_layout Itemize
http://www.stat.ucla.edu/~yuille/courses/Stat161-261-Spring14/HyvO00-icatut.pdf
\end_layout

\begin_layout Itemize
http://research.ics.aalto.fi/ica/book/
\end_layout

\end_deeper
\begin_layout Part
Logistic
\end_layout

\begin_layout Section
Logistic
\end_layout

\begin_layout Standard
Also see Exponential Class.
\end_layout

\begin_layout Itemize
Assume
\begin_inset Formula $P(y|x,\theta)\sim ExponentialFamily(\eta)$
\end_inset


\end_layout

\begin_layout Itemize
Goal is to output (get the prediction)
\begin_inset Formula $h(x)=E(T(y)|x)$
\end_inset

, where
\begin_inset Formula $T(y)$
\end_inset

is the sufficent statistic of
\begin_inset Formula $P(y|x,\theta)$
\end_inset

.
\end_layout

\begin_layout Itemize
\begin_inset Formula $\theta$
\end_inset

is the canonical (典型的) parameter
\begin_inset Index idx
status open

\begin_layout Plain Layout
canonical parameter in GLM
\end_layout

\end_inset

.
\end_layout

\begin_layout Itemize
Design choice, to make the problem as GLM: make
\begin_inset Formula $\eta=\theta^{T}x$
\end_inset


\end_layout

\begin_layout Subsection
Logit vs Logistic
\end_layout

\begin_layout Itemize
Logit Function of any ratio 
\begin_inset Formula $p$
\end_inset

 between 0 - 1:
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula 
\[
logit(p)=\mbox{log}(\frac{p}{1-p})=\mbox{log}odds
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
If you have proportion data 
\begin_inset Formula $p_{i}$
\end_inset

 , you can assume the underlying distribution is eseentially counting data
 which is Possion, then you can use Logit-transformation then use OLS.
\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
Logistic Function of any number 
\begin_inset Formula $\alpha$
\end_inset


\end_layout

\begin_layout Itemize
\begin_inset Formula 
\[
\mbox{logistic(\alpha)}=\mbox{logit^{-1}}(\alpha)=\frac{1}{1+exp(-\alpha)}=\frac{exp(\alpha)}{exp(\alpha)+1}
\]

\end_inset


\end_layout

\begin_layout Subsection
Logistic Regression
\end_layout

\begin_layout Itemize
\begin_inset Formula 
\begin{eqnarray*}
h_{\theta}(x)=E[y|x,\theta] & = & P(y=1|x,\theta)\\
 & = & \theta\\
 & = & E(y|\eta)\\
 & = & \frac{1}{1+exp(-\eta)}\\
 & = & \frac{1}{1+exp(-x'\beta))}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Itemize
\begin_inset Formula 
\begin{eqnarray*}
P(y=0|x,\beta) & = & 1-\frac{1}{1+exp(-x'\beta)}\\
 & = & \frac{exp(-x'\beta)}{1+exp(-x'\beta)}\\
 & = & \frac{1}{1+exp(+x'\beta)}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Itemize
If we let 
\begin_inset Formula $y\in\{-1,1\}$
\end_inset

 instead of 
\begin_inset Formula $\{0,1\}$
\end_inset

, like in SMV, then we can write 
\end_layout

\begin_layout Itemize
\begin_inset Formula 
\[
P(y|x,\beta)=\frac{1}{1+exp(-yx'\beta)}
\]

\end_inset


\end_layout

\begin_layout Itemize
Log of Odds ratio is linear 
\begin_inset Formula $X'\beta$
\end_inset


\end_layout

\begin_layout Itemize
\begin_inset Formula 
\begin{eqnarray*}
odds & = & \frac{P(Y=1|X)}{1-P(Y=1|X)}\\
 & = & \frac{1/(1+e^{-X'\beta})}{1-1/(1+e^{-X'\beta})}\\
 & = & 1/(1+e^{-X'\beta})\times(1+e^{-X'\beta})/e^{-X'\beta}\\
 & = & exp(X'\beta)
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
So the true Odds would be
\end_layout

\begin_layout Standard
\begin_inset Formula $g(\eta)=E(y|\eta)=\frac{1}{1+exp(-\eta)}$
\end_inset

is the canonical response function
\end_layout

\begin_layout Standard
\begin_inset Formula $g^{-1}(\eta)$
\end_inset

is the canonical
\begin_inset Index idx
status open

\begin_layout Plain Layout
link function
\end_layout

\end_inset

link function.
\end_layout

\begin_layout Standard
Lecture 3 min73
\end_layout

\begin_layout Standard
Standard logistic sigmoid function
\begin_inset Formula $y=\frac{1}{1+exp(-x)}$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename Lyx_Picture/Logistic-curve.svg.png
	scale 30

\end_inset


\end_layout

\begin_layout Subsection
How to explain 
\begin_inset Formula $\beta$
\end_inset


\end_layout

\begin_layout Enumerate
If 
\begin_inset Formula $\Delta x=1$
\end_inset

, then 
\begin_inset Formula $\beta=\%\Delta odds=\Delta log(Odds)$
\end_inset

: change of log odds
\end_layout

\begin_layout Enumerate
If 
\begin_inset Formula $\%\Delta x=1$
\end_inset

, then 
\begin_inset Formula $\beta=\Delta odds$
\end_inset


\end_layout

\begin_layout Subsection
Stationarity in logit regression
\end_layout

\begin_layout Standard
Generally, Stationarity is not a problem here.
\end_layout

\begin_layout Itemize
For 
\begin_inset Formula $y$
\end_inset

: Account level charge-off data (binary time series with only at most one
 possible 
\begin_inset Formula $1$
\end_inset

 at the the end of time) is always stationary! You can test!
\end_layout

\begin_layout Itemize
For 
\begin_inset Formula $x$
\end_inset

: In this paper
\begin_inset Foot
status open

\begin_layout Plain Layout
Chang, Jiang and Park, 2004.
 Non-stationary Logistic Regression
\end_layout

\end_inset

, we consider the logistic regression model with an integrated regressor
 of the ARIMA type.
 It is shown that the model can be consistently estimated by the usual nonlinear
 least squares (NLS) method.
 The convergence rates of the NLS estimators are derived and their limiting
 distributions are also provided.
 The problem of asymptotic inference in the non-stationary logistic regression
 is also addressed.
\end_layout

\begin_layout Standard
\begin_inset Foot
status open

\begin_layout Plain Layout
R code: to see whether spurious regression happens in logit regression.
 You can run the code for several times to see whether b is significant
 by chance.
\end_layout

\begin_layout Plain Layout
# stationary in logit model # x: non-stationary # y: stationary and BINARY
\end_layout

\begin_layout Plain Layout
a=rnorm(100)>1 
\end_layout

\begin_layout Plain Layout
b=cumsum(rnorm(100)) 
\end_layout

\begin_layout Plain Layout
summary(glm(a~b,family = 'binomial'),link='logit')
\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Probit
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\Pr(Y=1\mid X)=\Phi(X'\beta)
\]

\end_inset


\end_layout

\begin_layout Subsection
MLE
\end_layout

\begin_layout Standard
Set
\begin_inset Formula $p(x_{i})=P(y_{i}=1)$
\end_inset

, then the likelihood function is
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
L(\beta) & = & \sum_{i}^{N}\mbox{log}P(y_{i}|\beta)\\
 & = & \sum_{i=1}^{N}\left\{ y_{i}\mbox{log}P(y_{i}=1;\beta)+(1-y_{i})\mbox{log}(1-P(y_{i}=1;\beta))\right\} \\
 & = & \sum_{i=1}^{N}\left\{ y_{i}\beta^{T}x_{i}-log(1+exp(\beta^{T}x_{i})\right\} 
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
Gradient Descent follows
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\theta:=\theta+\alpha\nabla_{\theta}LogL(\theta)
\]

\end_inset


\end_layout

\begin_layout Standard
where
\begin_inset Formula $\nabla_{\theta}LogL(\theta)=\sum(y^{i}-h_{\theta}(x^{i})x^{i}$
\end_inset


\end_layout

\begin_layout Standard
Or from minimize Loss perspective, here the loss function is Negative Logit
\end_layout

\begin_layout Standard
we min 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
-L(\beta) & = & -\sum_{i}^{N}\mbox{log}P(y_{i}|\beta)\\
 & = & -\sum\mathbf{logit}(y_{i}x'\beta)
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Subsection
L1 Regularized Logistic Regression
\end_layout

\begin_layout Standard
p141
\end_layout

\begin_layout Standard
The L1 penalty used in the lasso (Section 3.4.2) can be used for variable
 selection and shrinkage with any linear regression model.
 For logistic regression, we would maximize a penalized version of MLE function:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
L(\beta)=\sum_{i}^{N}\mbox{log}P(y_{i}|\beta)-\sum_{i=1}^{p}|\beta_{j}|
\]

\end_inset


\end_layout

\begin_layout Standard
Figure 4.13 shows the L1 regularization path for the South African heart
 disease data of Section 4.4.2.
 This was produced using the R package glmpath (Park and Hastie, 2007).
\end_layout

\begin_layout Subsection
Distribution Assumptions: NO
\end_layout

\begin_layout Itemize
I f
\begin_inset Formula $x|y\sim Normal\rightarrow p(y=1|x)\in Logistic$
\end_inset

 but inverse is not
\end_layout

\begin_layout Itemize
For example, if 
\begin_inset Formula $x|y\sim Poission$
\end_inset

 we can still get 
\begin_inset Formula $p(y=1|x)\in Logistic$
\end_inset


\end_layout

\begin_layout Section
Multinomial Logistic (Softmax) Regression
\end_layout

\begin_layout Standard
Multinomial logistic regression is a classification method that generalizes
 logistic regression to multi-class problems.
\end_layout

\begin_layout Subsection

\series bold
Assumption: Independence of irrelevant alternatives (IIA)
\end_layout

\begin_layout Standard
Key assumption:
\begin_inset Index idx
status open

\begin_layout Plain Layout
Independence of Irrelevant Alternatives (IIA)
\end_layout

\end_inset

Independence of Irrelevant Alternatives (IIA), which is not always desirable.
\end_layout

\begin_layout Itemize
This assumption states that the odds of preferring one class over another
 do not depend on the presence or absence of other “irrelevant” alternatives.
 For example, the relative probabilities of taking a car or bus to work
 do not change if a bicycle is added as an additional possibility.
\end_layout

\begin_layout Itemize
This allows the choice of K alternatives to be modeled as a set of K-1 independe
nt binary choices, in which 
\series bold
one alternative is chosen as a “pivot” and the other K-1
\series default
 compared against it, one at a time.
 (which means all other classes are seen as one group, or one alternative)
\end_layout

\begin_layout Itemize
The IIA hypothesis is a core hypothesis in rational choice theory
\end_layout

\begin_layout Subsection
Algorithm: Softmax Regression
\end_layout

\begin_layout Standard
See
\emph on
 Elements of Statistical Learning Section 4.4:
\end_layout

\begin_layout Standard
The logistic regression model arises from the desire to model the posterior
 probabilities of the K classes via linear functions in
\begin_inset Formula $x$
\end_inset

, while at the same time ensuring that they sum to one and remain in [0,1].The
 model has the form
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
log\frac{P(G=j|x)}{P(G=K|x)}=\beta_{j}^{T}x
\]

\end_inset


\end_layout

\begin_layout Standard
where the
\begin_inset Formula $K^{th}$
\end_inset

class is the base/pivot class.
\end_layout

\begin_layout Standard
Note that in softmax regression eventually we get 
\begin_inset Formula $K-1$
\end_inset

 sets of 
\begin_inset Formula $\beta$
\end_inset


\end_layout

\begin_layout Standard
Therefore, with the assumption that
\begin_inset Formula $\sum_{i}^{k}P(G=i|x)=1$
\end_inset

, for the
\begin_inset Formula $K^{th}$
\end_inset

class we have
\begin_inset Formula 
\[
P(G=K|x)=1-P(G=K|x)\sum_{j}^{K-1}exp(\beta_{j}^{T}x)
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
P(G=K|x)=\frac{1}{1+\sum_{j}^{k-1}exp(\beta_{j}^{T}x)}
\]

\end_inset


\end_layout

\begin_layout Standard
For the class
\begin_inset Formula $\ne K$
\end_inset

, we have
\begin_inset Formula 
\[
P(G=i|x)=exp(\beta_{j}^{T}x)P(G=K|x)
\]

\end_inset


\end_layout

\begin_layout Standard
When
\begin_inset Formula $K=2$
\end_inset

, it is the naive logit regression.
\end_layout

\begin_layout Subsection
Fitting: MLE
\end_layout

\begin_layout Standard
P136
\end_layout

\begin_layout Standard
If we have
\begin_inset Formula $N$
\end_inset

observations and
\begin_inset Formula $K$
\end_inset

classes (
\begin_inset Formula $G$
\end_inset

can have
\begin_inset Formula $K$
\end_inset

values, or you can think 
\begin_inset Formula $G$
\end_inset

 is a
\begin_inset Formula $K-1$
\end_inset

 vector and the when action is to choose first class then
\begin_inset Formula $G=\{1,0,0….0\}$
\end_inset

)
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
L(\theta)=\sum_{i=1}^{N}logP_{i}(G|x)
\]

\end_inset


\end_layout

\begin_layout Standard
ESL use simple logit mle as example, using newton iteration to max the likelihoo
d.
\end_layout

\begin_layout Standard
ESL did not touch MLE problem for softmax regression.
\end_layout

\begin_layout Subsection
R
\end_layout

\begin_layout Standard
The R package glmnet (Friedman et al., 2010) can fit very large logistic
 regression problems efficiently, both in N and p.
 Although designed to fit regularized models, options allow for unregularized
 fits.
\end_layout

\begin_layout Section
Inferences
\end_layout

\begin_layout Standard
This section applies in both logit and softmax.
\end_layout

\begin_layout Standard
ESL P141
\end_layout

\begin_layout Subsection
Residual sum-of-squares 
\end_layout

\begin_layout Standard
The weighted residual sum-of-squares is the familiar Pearson chi-square
 statistic 
\begin_inset Formula 
\[
\sum_{i}^{N}\frac{(y_{i}\text{−}\hat{p}_{i})}{\hat{p}_{i}(1\text{−}\hat{p}_{i})})
\]

\end_inset

 , (4.30) a quadratic approximation to the deviance
\end_layout

\begin_layout Subsection
Consistency
\end_layout

\begin_layout Standard
Asymptotic likelihood theory says that if the model is correct, then 
\begin_inset Formula $\hat{β}$
\end_inset

 is consistent (i.e., converges to the true β).
\end_layout

\begin_layout Subsection
Asymptotic
\end_layout

\begin_layout Standard
A central limit theorem then shows that the distribution of 
\begin_inset Formula $\hat{β}$
\end_inset

 converges to 
\begin_inset Formula 
\[
N(β,(X^{T}WX)^{−1})
\]

\end_inset

.
 This and other asymptotic can be derived directly from the weighted least
 squares fit by mimicking normal theory inference.
\end_layout

\begin_layout Standard
where 
\begin_inset Formula $W$
\end_inset

 is a N × N diagonal matrix of weights with ith diagonal element 
\begin_inset Formula $p(x_{i};β)(1-p(x_{i};β))$
\end_inset


\end_layout

\begin_layout Subsection
Test without iteration
\end_layout

\begin_layout Standard
Model building can be costly for logistic regression models, because each
 model fitted requires iteration.
 Popular shortcuts are the Rao score test which tests for inclusion of a
 term, and the Wald test which can be used to test for exclusion of a term.
 Neither of these require iterative fitting, and are based on the maximum-likeli
hood fit of the current model.
 It turns out that both of these amount to adding or dropping a term from
 the weighted least squares fit, using the same weights.
 Such computations can be done efficiently, without recomputing the entire
 weighted least squares fit.
\end_layout

\begin_layout Part
Trees
\end_layout

\begin_layout Itemize
ISL p320: Context
\end_layout

\begin_layout Itemize
ISL p343: R code.
\end_layout

\begin_layout Standard
Easier to interpret, and has a nice graphical representation.
\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename Lyx_Picture/tree.png

\end_inset


\end_layout

\begin_layout Standard
Box plot of the tree
\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename Lyx_Picture/tree_box.png

\end_inset


\end_layout

\begin_layout Section
Decision Tree Terms
\end_layout

\begin_layout Itemize

\series bold
Root Node:
\series default
 is the tree node from which all branches of the tree emanate.
\end_layout

\begin_layout Itemize

\series bold
Child Node:
\series default
 A child node in a tree is any node in the tree that has an immediate parent
 node above it.Hence, all nodes other than the root node are child nodes.
\end_layout

\begin_layout Itemize

\series bold
Leaf Node:
\series default
 A leaf node is any tree node that has no child nodes.
\end_layout

\begin_layout Section
Tree Algorithm
\end_layout

\begin_layout Subsection
Rule Representation (Non-Graphical)
\end_layout

\begin_layout Standard
Trees partition the space of joint predictor variable values into disjoint
 regions
\begin_inset Formula $R_{j}$
\end_inset

with constant predictor values assigned to each label (classification) /
 value (regression) 
\begin_inset Formula $\gamma_{j}$
\end_inset


\end_layout

\begin_layout Standard
Tree can be formally expressed as:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
T(x;\Theta)=\sum_{j=1}^{J}\gamma_{j}I(x\in R_{j})
\]

\end_inset

with parameters
\begin_inset Formula $\Theta=\{R_{j},\gamma_{j}\}_{i}^{J}$
\end_inset


\end_layout

\begin_layout Standard
with parameters
\begin_inset Formula $\Theta=\{R_{j},\gamma_{j}\}_{i}^{J}$
\end_inset


\end_layout

\begin_layout Itemize
This representation can be applied for both Classification Tree and Regression
 Tree.
\end_layout

\begin_layout Itemize
For prediction: As 
\begin_inset Formula $R_{j}$
\end_inset

 are disjoint from each other, when 
\begin_inset Formula $x\in R_{j}$
\end_inset

, we can write 
\begin_inset Formula $T(x_{J};\Theta_{J})=\gamma_{j}$
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Regression Tree: 
\begin_inset Formula $\hat{\gamma}_{j}=E(y_{i}|x_{i}\in R_{j})$
\end_inset

 where 
\begin_inset Formula $y_{i}$
\end_inset

 and 
\begin_inset Formula $x_{i}$
\end_inset

 are from the developing data.
\end_layout

\begin_layout Itemize
ClassificationTree: 
\begin_inset Formula $\hat{\gamma}_{j}=Mode(y_{i}|x_{i}\in R_{j})$
\end_inset

 where 
\begin_inset Formula $y_{i}$
\end_inset

 and 
\begin_inset Formula $x_{i}$
\end_inset

 are from the developing data.
\end_layout

\end_deeper
\begin_layout Standard
Parameters found by minimizing empirical risk:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\hat{\Theta}=\mbox{arg}min_{\Theta}\sum_{j=1}^{J}\sum_{x_{i}\in R_{j}}L(y_{i},\gamma_{l})
\]

\end_inset


\end_layout

\begin_layout Standard
This is a formidable combinatorial optimization problem.So we use a greedy,
 top-down recursive partitioning algorithm (e.g.Gini index for misclassification
 loss in growing of tree)
\end_layout

\begin_layout Subsection
Estimate the Tree: 
\series bold
Recursive binary splitting
\end_layout

\begin_layout Standard
ISL P321
\end_layout

\begin_layout Standard
The essence of tree problem is to group observations into
\begin_inset Formula $J$
\end_inset

 boxes, where each box belong to one of
\begin_inset Formula $K$
\end_inset

classes.
\end_layout

\begin_layout Standard
You need to
\end_layout

\begin_layout Enumerate
decide which box belongs to which class.
\end_layout

\begin_layout Enumerate
how many box (number of
\begin_inset Formula $J$
\end_inset

).
\end_layout

\begin_layout Standard
The goal is to find boxes
\begin_inset Formula $R_{1}$
\end_inset

, ...,
\begin_inset Formula $R_{J}$
\end_inset

that minimize the total Loss Functions of the tree
\end_layout

\begin_layout Standard

\series bold
Unfortunately
\series default
, it is computationally unfeasible to consider every possible partition
 of the feature space into J boxes.
 For this reason, we take a top-down, greedy approach that is known as recursive
 binary splitting.
\end_layout

\begin_layout Enumerate
The recursive binary splitting approach is top-down because it begins at
 the top of the tree (at which point all observations belong to a single
 region) and then successively splits the predictor space; each split is
 indicated via two new branches further down on the tree.
\end_layout

\begin_layout Enumerate
It is
\series bold
 greedy
\series default
 because at each step of the tree-building process,
\series bold
 the best split is made at that particular step, rather
\series default
 than looking ahead and picking a split that will lead to a better treeing
 some future step.
\end_layout

\begin_layout Subsection
Impurity Measure for a single Node
\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename Gini_impurity.png
	lyxscale 80
	scale 80

\end_inset


\end_layout

\begin_layout Standard
Node here can be the node in the middle of the tree or end of the tree.
\end_layout

\begin_layout Standard
Impurity measure defines how well classes are separated.In general the impurity
 measure should satisfy
\end_layout

\begin_layout Enumerate
Largest when data are split evenly for attribute values.
\end_layout

\begin_layout Enumerate
Should be 0 when all data belong to the same class
\end_layout

\begin_layout Standard
For one node, it may contains 
\begin_inset Formula $K$
\end_inset

 different classes
\end_layout

\begin_layout Itemize
Regression problem
\end_layout

\begin_deeper
\begin_layout Standard
\begin_inset Formula 
\[
\sum_{i\in R}(y_{i}-\hat{y}_{R})^{2}
\]

\end_inset


\end_layout

\begin_layout Standard
where
\begin_inset Formula $\hat{y}_{R_{j}}$
\end_inset

is the mean response for the training observations within the node, 
\begin_inset Formula $y_{i}$
\end_inset

 is the actual data's response within the node.
\end_layout

\end_deeper
\begin_layout Itemize
Classification problem’s 
\begin_inset Formula $RSS$
\end_inset

 is better measured by 
\series bold
Gini (impurity) index
\end_layout

\begin_deeper
\begin_layout Standard
\begin_inset Formula 
\[
G=\sum_{k}^{K}\hat{p}_{k}(1-\hat{p}_{k})
\]

\end_inset


\end_layout

\begin_layout Itemize
where
\begin_inset Formula $K$
\end_inset

 is total number of different classes in the node, 
\begin_inset Formula $\hat{p}_{mk}$
\end_inset

 represents the proportion of training observations are from its claimed
 
\begin_inset Formula $k^{th}$
\end_inset

 class.
\end_layout

\end_deeper
\begin_layout Itemize
Another Impurity Measure for Classification problem:
\series bold
 Information Index
\series default
: (start with negative sign)
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
I=\sum_{k}^{K}(-\hat{p}_{k}log\hat{p}_{k})
\]

\end_inset


\end_layout

\begin_layout Subsection
Weighted Impurity
\end_layout

\begin_layout Standard
When there are weights assigned to each observation 
\begin_inset Formula $i$
\end_inset

 in the Node 
\begin_inset Formula $A$
\end_inset

, then the 
\begin_inset Formula $p_{k}$
\end_inset

 in 
\begin_inset Formula $G=\sum_{k}^{K}\hat{p}_{k}(1-\hat{p}_{k})$
\end_inset

 becomes 
\begin_inset Formula 
\[
p_{k}=\frac{\sum_{i\in A\&i\in k}w_{i}n_{i}}{\sum_{i\in A}w_{i}n_{i}}
\]

\end_inset


\end_layout

\begin_layout Standard
where 
\begin_inset Formula $n_{i}$
\end_inset

 is # of observations.
\end_layout

\begin_layout Subsection
Algorithm: Split One Node:
\end_layout

\begin_layout Standard
As we want to split the node 
\begin_inset Formula $A$
\end_inset

 into two subnodes: 
\begin_inset Formula $A_{L}$
\end_inset

 and 
\begin_inset Formula $A_{R}$
\end_inset

, the goal is to 
\series bold
maximize the impurity
\series default
 
\series bold
reduction
\series default
 by selecting the correct variable at its correct cut off.
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\Delta I=I(A)-p(A_{L})I(A_{L})-p(A_{R})I(A_{R})
\]

\end_inset

where 
\begin_inset Formula $p(A_{L})=\frac{\#A_{L}}{\#A}$
\end_inset

 and 
\begin_inset Formula $p(A_{R})==\frac{\#A_{R}}{\#A}$
\end_inset

.
 
\end_layout

\begin_layout Standard
Coding: 
\end_layout

\begin_layout Enumerate
For each feature 
\begin_inset Formula $q$
\end_inset

, 
\begin_inset Formula $\Delta_{q}I=max_{cut}[-\Delta I]$
\end_inset

 at its best cutoff point.
\end_layout

\begin_layout Enumerate
Then choose the the variable 
\begin_inset Formula $q_{max}$
\end_inset

 that has max 
\begin_inset Formula $\Delta_{q}I$
\end_inset

.
\end_layout

\begin_layout Subsection
Variable Importance for a single tree
\end_layout

\begin_layout Standard
http://stackoverflow.com/questions/15810339/how-are-feature-importances-in-random
forestclassifier-determined
\end_layout

\begin_layout Standard
The usual way to compute the feature importance values of a single tree
 is as follows:
\end_layout

\begin_layout Standard
you initialize an array feature_importance of all zeros with size n_features.
 you traverse the tree: for each internal node that splits on feature i
 you compute the error reduction ／ impurity reduction of that node multiplied
 by the number of samples that were routed to the node and add this quantity
 to feature_importance[i].
\end_layout

\begin_layout Itemize
Finally after you get feature_importance[i] for each 
\begin_inset Formula $i$
\end_inset

, you need to normalize them by making their sum = 1.
\end_layout

\begin_layout Itemize
You can choose the impurity method you want.
\end_layout

\begin_layout Standard
So basically Variable Importance 
\begin_inset Formula $i$
\end_inset

 is the weighted Impurity Reduction amount, where the weights are the number
 of training sample passing through the splitting nodes that are using variable
 
\begin_inset Formula $i$
\end_inset


\end_layout

\begin_layout Section
Tree Pruning
\end_layout

\begin_layout Standard
Therefore, ONE strategy is to grow a very large tree
\begin_inset Formula $T_{0}$
\end_inset

, and then prune it back in order to obtain a subtree.
\end_layout

\begin_layout Subsection
\begin_inset Index idx
status open

\begin_layout Plain Layout
Ockham’s Razor
\end_layout

\end_inset

Ockham’s Razor.
\end_layout

\begin_layout Standard
A note will not be partitioned further if the resulting decision tree is
 not improved significantly by the splitting.
\end_layout

\begin_layout Subsection
Cost complexity pruning—also known as weakest link pruning
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\sum_{j}^{T}\sum_{i\in R_{j}}(y_{i}-\hat{y}_{R_{j}})^{2}+\alpha T
\]

\end_inset


\end_layout

\begin_layout Standard
where 
\begin_inset Formula $T$
\end_inset

 means the number of terminals.
 The tuning parameter α controls a trade-off between the subtree’s complexity
 and its fit to the training data.We can select a value of α using a validation
 set or using cross-validation.
\end_layout

\begin_layout Section

\series bold
Empirical
\end_layout

\begin_layout Itemize
Consider performing dimensionality reduction (PCA, ICA, or Feature selection)
 beforehand to give your tree a better chance of finding features that are
 discriminative.
 
\end_layout

\begin_layout Itemize
Remember that the number of samples required to populate the tree doubles
 for each additional level the tree grows to.
\end_layout

\begin_layout Itemize
Balance your data set before training to prevent the tree from being biased
 toward the classes that are dominant.
 Decision tree learners create biased trees if some classes dominate
\end_layout

\begin_layout Subsection
Advantages of tree
\end_layout

\begin_layout Itemize
Uses a 
\series bold
white box model.

\series default
 If a given situation is observable in a model, the explanation for the
 condition is easily explained by Boolean logic.
 By contrast, in a 
\series bold
black box model 
\series default
(e.g., in an artificial neural network), results may be more difficult to
 interpret.
 
\end_layout

\begin_layout Itemize
Able to handle both numerical and categorical data.
 Other techniques are usually specialized in analyzing data sets that have
 only one type of variable.
 See algorithms for more information.
 Able to handle multi-output problems.
\end_layout

\begin_layout Section
C4.5
\end_layout

\begin_layout Standard
see the complete text book: C4.5_Programs for Marching Learning.pdf
\end_layout

\begin_layout Standard
Top 10 algorithms in data mining
\end_layout

\begin_layout Standard
http://www.cnblogs.com/superhuake/archive/2012/07/25/2609124.html
\end_layout

\begin_layout Standard
http://www.cs.princeton.edu/courses/archive/spr07/cos424/papers/mitchell-dectrees.pd
f
\end_layout

\begin_layout Standard
Decision Tree Discovery Ron Kohavi Ross Quinlan
\end_layout

\begin_layout Standard
Supervised Machine Learning: A Review of Classification Techniques, Informatica
 
\end_layout

\begin_layout Standard
https://www.jair.org/media/279/live-279-1538-jair.pdf
\end_layout

\begin_layout Standard
https://books.google.com/books?id=HExncpjbYroC&pg=PA1&source=gbs_toc_r&cad=4#v=one
page&q=prunning&f=false
\end_layout

\begin_layout Standard
A Backward Adjusting Strategy for the C4.5 Decision Tree Classifier
\end_layout

\begin_layout Subsection
Entropy of subset 
\begin_inset Formula $S$
\end_inset


\end_layout

\begin_layout Standard
Let 
\begin_inset Formula $P_{S}(C_{j})$
\end_inset

 as the probability of class 
\begin_inset Formula $j$
\end_inset

 in 
\begin_inset Formula $S$
\end_inset

 (in finite sample it is 
\begin_inset Formula $\frac{\#C_{j}}{\#S}$
\end_inset

), then the Entropy of 
\begin_inset Formula $S$
\end_inset

 (information in 
\begin_inset Formula $S$
\end_inset

) is
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
Entropy(S)=Infor(S)=-\sum_{j}^{K}P_{S}(C_{j})\mbox{log}(P_{S}(C_{j}))
\]

\end_inset


\end_layout

\begin_layout Subsection
Information Gain:
\end_layout

\begin_layout Standard
After test 
\begin_inset Formula $X$
\end_inset

, If the data 
\begin_inset Formula $T$
\end_inset

 is separated into 
\begin_inset Formula $N$
\end_inset

 groups, then the sum of Entropy of those 
\begin_inset Formula $T$
\end_inset

 groups are
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
infor_{X}(T)=\sum_{i=1}^{N}\frac{|T_{i}|}{|T|}\times Entropy(T_{i})
\]

\end_inset

information that is gained by partitioning T in accordance with the test
 X:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
Gain=infor(T)-infor_{X}(T)
\]

\end_inset


\end_layout

\begin_layout Standard
The gain criterion, then, selects a test to maximize this information gain
 (which is also known as the mutual·information between the test X and the
 class).
\end_layout

\begin_layout Subsection
Information Gain Ratio:
\end_layout

\begin_layout Standard
For some years the selection of a test in ID3 was made on the basis of the
 gain criterion.
 Although it gave quite good results, this criterion has a serious deficiency-it
 has a strong bias in favor of tests with many outcomes.
 If we make all subgroups as 1-case subgroup and all of these one case subsets
 necessarily contain cases of a single class, infox (T) = 0, so information
 gain from using this attribute to partition the set of training cases is
 maximal.
\end_layout

\begin_layout Standard
Consider the information content of a message pertaining to a case that
 indicates not the class to which the case belongs, but the outcome of the
 test.
 By analogy with the definition of info(S), we have
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
splitinfo(X)=\sum_{i=1}^{N}\frac{|T_{i}|}{|T|}\times log\frac{|T_{i}|}{|T|}
\]

\end_inset


\end_layout

\begin_layout Standard
This represents the potential information generated by dividing T into n
 subsets, nothing about correct or wrong of classification.
\end_layout

\begin_layout Standard
Then 
\begin_inset Formula 
\[
gainratio(X)=gain(X)/splitinfo(X)
\]

\end_inset


\end_layout

\begin_layout Standard
expresses the proportion of information generated by the split that is useful,
 i.e., that appears helpful for classification.
 If the split is near trivial, split information will be small and this
 ratio will be unstable.
 To avoid this, the gain ratio criterion selects a test to maximize the
 ratio above, subject to the constraint that the information gain must be
 large-at least as great as the average gain over all tests examined.
\end_layout

\begin_layout Subsection
Example of Information Gain
\end_layout

\begin_layout Standard
P32 text_C4.5
\end_layout

\begin_layout Standard
\begin_inset listings
inline false
status open

\begin_layout Plain Layout

Outlook Temp (° F) Humidity (%) Windy? Class
\end_layout

\begin_layout Plain Layout

sunny, 75 70 true Play
\end_layout

\begin_layout Plain Layout

sunny 90 true Don't Play
\end_layout

\begin_layout Plain Layout

sunny 1 85 85 false Don't Play
\end_layout

\begin_layout Plain Layout

sunny 72 95 false Don't Play
\end_layout

\begin_layout Plain Layout

sunny 69 70 false Play 
\end_layout

\begin_layout Plain Layout

overcas 72 90 true Play
\end_layout

\begin_layout Plain Layout

overcast 83 78 false Play
\end_layout

\begin_layout Plain Layout

overcast 64 65 true Play
\end_layout

\begin_layout Plain Layout

overcast 81 75 false Play
\end_layout

\begin_layout Plain Layout

rain 71 80 true Don't Play
\end_layout

\begin_layout Plain Layout

rain 65 70 true Don't Play
\end_layout

\begin_layout Plain Layout

rain 75 80 false Play
\end_layout

\begin_layout Plain Layout

rain 68 80 false Play
\end_layout

\begin_layout Plain Layout

rain 70 96 false Play
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

# Corresponding Tree
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

outlook = sunny
\end_layout

\begin_layout Plain Layout

	humidity <= 75: Play
\end_layout

\begin_layout Plain Layout

	humidity > 75: Don't Play
\end_layout

\begin_layout Plain Layout

outlook = overcast: Play
\end_layout

\begin_layout Plain Layout

outlook = rain:
\end_layout

\begin_layout Plain Layout

	windy = true: Don't Play
\end_layout

\begin_layout Plain Layout

	windy = false: Play
\end_layout

\begin_layout Plain Layout

	
\end_layout

\end_inset


\end_layout

\begin_layout Standard
As a concrete illustration, consider again the training set of Figure 2-1.
 There are two classes, nine cases belonging to Play and five to Don't Play,
 so
\end_layout

\begin_layout Standard
\begin_inset Formula $info(T)=-9/14\times log_{2}(9/14)-5/14\times log_{2}(5/14)=0.940bits$
\end_inset


\end_layout

\begin_layout Standard
(Remember, this represents the average information needed to identify the
 class of a case in T.) After using 
\series bold
outlook
\series default
 to divide T into three subsets, the result is given by
\begin_inset Formula 
\begin{eqnarray*}
infox(T) & = & 5/14\times(-2/5\times log2(2/5)-3/5\times log2(3/5))+\\
 &  & 4/14\times(-4/4\times log2(4/4)-0/4Xlog2(0/4))+\\
 &  & 5/14\times(-3/5\times log2(3/5)-2/5\times log2(2/5))\\
 & = & 0.694bits.\\
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
The information gained by this test is therefore 0.940 - 0.694 = 0.246 bits.
\end_layout

\begin_layout Subsection
Possible Tests
\end_layout

\begin_layout Standard
Possible ways to separate a data set.
\end_layout

\begin_layout Standard
C4.5 contains mechanisms for proposing three types of tests:
\end_layout

\begin_layout Itemize
The standard" test on a discrete attribute, with one outcome and branch
 for each possible value of that attribute --- attribute'sunique values
 are in one group（outcome）。
\end_layout

\begin_layout Itemize
A more complex test, based on a discrete attribute, in which the possible
 values are allocated to a variable number of groups with one outcome for
 each group rather than each value.
 Tests of this form, discussed in Chapter 7, must be invoked by an option.
 attribute's multiple value as one group（outcome）。
\end_layout

\begin_layout Itemize
If attribute A has continuous numeric values, a binary test with outcomes
 A.::; Z and A > Z, based on comparing the value of A against a threshold
 value Z.
\end_layout

\begin_layout Subsection
Tests on continuous attributes
\end_layout

\begin_layout Standard
The training cases T are first sorted on the values of the attribute 
\begin_inset Formula $A$
\end_inset

 being considered.
\end_layout

\begin_layout Standard
There are only a finite number of these values, so let us denote them in
 order as {v1,v2 , ••• ,
\begin_inset Formula $v_{m}$
\end_inset

}· Any threshold value lying between vi and vi+1 will have the same effect
 of dividing the cases into those whose value of the attribute A lies in
 { v1 , v2 , ••• , vi} and those whose value is in { vi+l' vi+2 , ...
 , vm}.
\end_layout

\begin_layout Standard
There are thus only m- 1 possible splits on A, 
\series bold
all of which are examined
\series default
.
 (It might seem expensive to examine all 
\begin_inset Formula $m-1$
\end_inset

 such thresholds, but, when the cases have been sorted as above, this can
 be carried out in one pass, updating the class distributions to the left
 and right of the threshold on the fly.)
\end_layout

\begin_layout Standard
It is usual to choose the midpoint of each interval as the representative
 threshold, the 
\begin_inset Formula $i^{th}$
\end_inset

 possible threshold such being 
\begin_inset Formula $\frac{v_{i}+v_{i+1}}{2}$
\end_inset


\end_layout

\begin_layout Subsection
Error-Based Prunning
\end_layout

\begin_layout Enumerate
Decision trees are usually simplified by discarding one or more subtrees
 and replacing them with leaves;
\end_layout

\begin_deeper
\begin_layout Enumerate
If replacement of this subtree with a leaf, or with its most frequently
 used branch, would lead to a lower predicted error rate, then prune the
 tree accordingly
\end_layout

\end_deeper
\begin_layout Enumerate
In addition, C4.5 allows replacement of a subtree by one of its branches.
\end_layout

\begin_layout Standard
P48 text_C4.5
\end_layout

\begin_layout Standard

\series bold
Original Tree:
\end_layout

\begin_layout Standard
(For the unpruned tree, recall that the (N) or (N/ E) appearing after a
 leaf indicates that the leaf covers N training cases, E erroneously; similar
 numbers for the pruned tree are explained below.)
\end_layout

\begin_layout Standard
\begin_inset listings
inline false
status open

\begin_layout Plain Layout

physician fee freeze = n:
\end_layout

\begin_layout Plain Layout

	adoption of the budget resolution = y: democrat (115-1)
\end_layout

\begin_layout Plain Layout

	adoption of the budget resolution = u: democrat {1)
\end_layout

\begin_layout Plain Layout

	adoption of the budget resolution = n:
\end_layout

\begin_layout Plain Layout

		education spending= n: democrat (6)
\end_layout

\begin_layout Plain Layout

		education spending= y: democrat (9)
\end_layout

\begin_layout Plain Layout

		education spending= u: republican {1)
\end_layout

\begin_layout Plain Layout

physician fee freeze = y:
\end_layout

\begin_layout Plain Layout

	synfuels corporation cutback = n: republican (97 /3}
\end_layout

\begin_layout Plain Layout

	synfuels corporation cutback= u: republican (4)
\end_layout

\begin_layout Plain Layout

	synfuels corporation cutback = y:
\end_layout

\begin_layout Plain Layout

		duty free exports = y: democrat (2)
\end_layout

\begin_layout Plain Layout

		duty free exports = u: republican (1)
\end_layout

\begin_layout Plain Layout

		duty free exports = n:
\end_layout

\begin_layout Plain Layout

			education spending= n: democrat (5/2)
\end_layout

\begin_layout Plain Layout

			education spending= y: republican (13/2)
\end_layout

\begin_layout Plain Layout

			education spending= u: democrat (1)
\end_layout

\begin_layout Plain Layout

physician fee freeze= u:
\end_layout

\begin_layout Plain Layout

	water project cost sharing= n: democrat (0)
\end_layout

\begin_layout Plain Layout

	water project cost sharing = y: democrat ( 4)
\end_layout

\begin_layout Plain Layout

	water project cost sharing= u:
\end_layout

\begin_layout Plain Layout

		mx missile= n: republican (0)
\end_layout

\begin_layout Plain Layout

		mx missile= y: democrat (3/1)
\end_layout

\begin_layout Plain Layout

		mx missile= u: republican (2)
\end_layout

\end_inset


\end_layout

\begin_layout Standard
How can we predict these error rates? It is clear that error rate on the
 training set from which the tree was built ( re-substitutign...
 error, in the terminology of Breiman et al.) does not provide a suitable
 estimate.
\end_layout

\begin_layout Standard
This search for a way of predicting error rates leads once again to two
 families of techniques.
 The first family predicts the error rate of the tree and its subtrees using
 a new set of cases that is distinct from the training set.
 Since these cases were not examined at the time the tree was constructed,
 the estimates obtained from them are clearly unbiased and, if there are
 enough of them, reliable.
 The drawback associated with this family of techniques is simply that some
 of the available data must be reserved for the separate set, so the original
 tree must be constructed from a smaller training set.
 This may not be much of a disadvantage when data is abundant, but can lead
 to inferior trees when data is.
 scarce.
\end_layout

\begin_layout Standard
The approach taken in C4.5 belongs to the second family of techniques that
 use only the training set from which the tree was built.
\end_layout

\begin_layout Standard
When N training cases are covered by a leaf, E of them incorrectly, the
 resubstitution error rate for this leaf is E/N.
 However, we can regard this somewhat naively as observing E "events" in
 N trials.
 If this set of N training cases could be regarded as a sample (which, of
 course, it is not), we could ask what this result tells us about the probabilit
y of an event (error) over the entire population of cases covered by this
 leaf.
 The probability of error cannot be determined exactly, but has itself a
 (posterior) probability distribution that is usually summarized by a pair
 of confidence limits.
 For a given confidence level C F, the upper limit on this probability can
 be found from the confidence limits.ior the binomial distribution; this
 upper limit is here written 
\begin_inset Formula $U_{cp}(E,N)$
\end_inset


\end_layout

\begin_layout Standard

\emph on
4.4 Model Overfitting 183 ­ https://www-users.cs.umn.edu/~kumar/dmbook/ch4.pdf
 Introduction to Data Mining
\end_layout

\begin_layout Standard

\emph on
Introduction to Data Mining Pang-Ning Tan, Michigan State University, Michael
 Steinbach, University of Minnesota Vipin Kumar, University of Minnesota
 
\end_layout

\begin_layout Standard
By approximating a binomial distribution with a normal distribution, the
 following upper bound of the error rate e can be derived: 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
e_{upper}(N,E,\alpha)=\frac{e+z_{\alpha/2}^{2}/2N+z_{\alpha/2}\sqrt{e(1-e)/N+z_{\alpha/2}^{2}/4N^{2}}}{1+z_{\alpha/2}^{2}/N}
\]

\end_inset

where α is the confidence level, 
\begin_inset Formula $z_{\alpha/2}$
\end_inset

is the standardized value from a standard normal distribution, and N is
 the total number of training records used to compute 
\begin_inset Formula $e$
\end_inset

.
 
\end_layout

\begin_layout Subsection
Missing when Building Tree
\end_layout

\begin_layout Standard
See Application in Machine Learning 1
\end_layout

\begin_layout Subsection
Missing when Forecasting
\end_layout

\begin_layout Standard
See Application in Machine Learning 1
\end_layout

\begin_layout Section

\series bold
Bagging & 
\series default
Boosting
\end_layout

\begin_layout Standard
see Bagging in Bootstrapping section for detail
\end_layout

\begin_layout Subsection

\series bold
Bagging
\end_layout

\begin_layout Standard

\series bold
Bagging in decision tree
\series default
: build several trees: For a given test observation, we can record the class
 predicted by each of the B trees, and take a majority vote: Thus, bagging
 improves prediction accuracy at the expense of interpret ability.
\end_layout

\begin_layout Subsection
Boosting
\end_layout

\begin_layout Standard
Details can be seen at ISL p 336
\end_layout

\begin_layout Standard
Boosting works in a similar way, except that the trees are grown sequentially:
 each tree is grown using information from previously grown trees.
 
\series bold
Boosting does not involve bootstrap sampling
\series default
; instead each tree is fraction a modified version (residuals of the last
 tree) of the original data set.
\end_layout

\begin_layout Part
Random Forests
\end_layout

\begin_layout Standard
Belong to the Ensemble Method: random subspace
\end_layout

\begin_layout Standard
Random forests provide an improvement over bagged trees by way of a small
 tweak that decorrelates the trees.
\end_layout

\begin_layout Standard
But in the collection of bagged trees, most or all of the trees will use
 this strong predictor in the top split.
 Consequently, all of the bagged trees will look quite similar to each other.Unfo
rtunately, averaging many highly correlated quantities does not lead to
 as large of a reduction in variance as averaging many uncorrelated quantities.
\end_layout

\begin_layout Standard
Random forests overcome this problem by forcing each split to consider only
 a subset of the predictors.
\end_layout

\begin_layout Section
Algorithm
\end_layout

\begin_layout Subsection
Algorithm
\end_layout

\begin_layout Standard
For b = 1 to B:
\end_layout

\begin_layout Enumerate
For b = 1 to B:
\end_layout

\begin_deeper
\begin_layout Enumerate
Draw a bootstrap sample
\begin_inset Formula $Z^{∗}$
\end_inset

of size N from the training data.
\end_layout

\begin_layout Enumerate
Grow a random-forest tree 
\begin_inset Formula $T_{b}$
\end_inset

 to the bootstrapped data, by recursively repeating the following steps
 for each terminal node of the tree, until the minimum node size n min is
 reached.
\end_layout

\begin_deeper
\begin_layout Enumerate
Select 
\begin_inset Formula $m$
\end_inset

 variables at random from the
\begin_inset Formula $p$
\end_inset

variables.
\end_layout

\begin_layout Enumerate
Pick the best variable/split-point among the 
\begin_inset Formula $m$
\end_inset

.
\end_layout

\begin_layout Enumerate
Split the node into two daughter nodes.
\end_layout

\begin_layout Enumerate
repeat till you get the tree size
\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
Output the ensemble of trees
\begin_inset Formula ${Tb}_{1}^{B}$
\end_inset

.To make a prediction at a new point x:
\end_layout

\begin_deeper
\begin_layout Itemize
Regression:
\begin_inset Formula $f(x)=\frac{1}{B}=\sum_{b=1}^{B}T_{b}(x)$
\end_inset


\end_layout

\begin_layout Itemize
Classification: majority vote
\end_layout

\end_deeper
\begin_layout Standard

\series bold
So: random forest is just bagging with random selected subset of predictors.
\end_layout

\begin_layout Subsection
Out-of-bag (OOB) prediction/error and its convergence in training.
\end_layout

\begin_layout Standard
For each observation
\begin_inset Formula $z_{i}=(x_{i},y_{i})$
\end_inset

, construct its random forest predictor by averaging only those trees correspond
ing to bootstrap samples in which 
\begin_inset Formula $z_{i}$
\end_inset

 did not appear.
\end_layout

\begin_layout Itemize
OOB classifier is the aggregation of votes ONLY over trees such that it
 does not contain 
\begin_inset Formula $z_{i}$
\end_inset

.
\end_layout

\begin_layout Itemize
Out-of-bag estimate for the generalization error is the error rate of the
 out-of-bag classifier on the training set (compare it with known yi’s).
\end_layout

\begin_deeper
\begin_layout Itemize
An OOB estimate is almost identical to that obtained by N-fold cross validation;
 see Exercise 15.2.
 Hence unlike many other nonlinear estimators, random forests can be fit
 in one sequence, with cross-validation being performed along the way.
\end_layout

\end_deeper
\begin_layout Standard

\series bold
Once the oob error stabilizes, the training can be terminated.
\end_layout

\begin_layout Section
Permutation Variable Importance
\end_layout

\begin_layout Standard
See the Chapter 6 in paper 
\begin_inset Quotes eld
\end_inset

Statistical Issues in Machine Learning – Towards Reliable Split Selection
 and Variable Importance Measures
\begin_inset Quotes erd
\end_inset


\end_layout

\begin_layout Standard
P609 ESL is not clear.
\end_layout

\begin_layout Subsection
Definitions you need to know before progress:
\end_layout

\begin_layout Itemize
OOB trees for observation
\begin_inset Formula $z_{i}$
\end_inset

: trees that do not contain
\begin_inset Formula $z_{i}$
\end_inset


\end_layout

\begin_layout Itemize

\series bold
Permutation
\series default
: A permutation, also called an “arrangement number” or “order,”is a rearrangeme
nt of the elements of an ordered list S into a one-to-one correspondence
 with S itself.
 The number of permutations on a set of n elements is given by n! (n factorial;
 Uspensky 1937, p.18).
 For example, there are 2!=2·1=2 permutations of {1,2}, namely {1,2} and{2,1},
 and 3!=3·2·1=6 permutations of {1,2,3}, namely {1,2,3}, {1,3,2},{2,1,3},
 {2,3,1}, {3,1,2}, and {3,2,1}.
\end_layout

\begin_layout Itemize
A random permutation is a permutation containing a fixed number 
\begin_inset Formula $n$
\end_inset

 of a random selection from a given set of elements.
\end_layout

\begin_deeper
\begin_layout Itemize
For the 
\begin_inset Formula $j^{th}$
\end_inset

 variable 
\begin_inset Formula $X_{j,i}$
\end_inset

 where 
\begin_inset Formula $i\in\{1…N\}$
\end_inset

 and 
\begin_inset Formula $i$
\end_inset

 means 
\begin_inset Formula $i^{th}$
\end_inset

 observation, random permutation on 
\begin_inset Formula $X_{j,i}$
\end_inset

means you randomly re-order all the
\begin_inset Formula $N$
\end_inset

 
\begin_inset Formula $X_{j,i}$
\end_inset

.
 Thus plug those data into the OOB trees
\end_layout

\end_deeper
\begin_layout Subsection
Core Idea
\end_layout

\begin_layout Standard
The most advanced variable importance measure available in random forests
 is the “
\series bold
permutation accuracy importance
\series default
” measure (termed “permutation importance” hereafter).
 Its rationale is the following: by means of randomly permuting the predictor
 variable
\begin_inset Formula $X_{j}$
\end_inset

 by some permutation 
\begin_inset Formula $\psi_{j}$
\end_inset

, its original association with the response Y is broken.
\end_layout

\begin_layout Standard
When the permuted variable
\begin_inset Formula $X_{j}$
\end_inset

, together with the remaining non-permuted predictor variables, are used
 as input and run through the OOB tree to get 
\begin_inset Formula $y_{j}$
\end_inset

, the prediction accuracy (i.e.the number of observations classified correctly)
 decreases substantially if the original variable
\begin_inset Formula $X_{j}$
\end_inset

 was associated with the response.
\end_layout

\begin_layout Standard
Thus, Breiman (2001a) suggests the difference in prediction accuracy beforehand
 after permuting 
\begin_inset Formula $X_{j}$
\end_inset

 , averaged over all its OOB trees, as a measure for variable importance,
 that we formalize as follows: Let
\begin_inset Formula $\bar{B}(t)$
\end_inset

 be the out-of-bag sample for a tree
\begin_inset Formula $t$
\end_inset

, with
\begin_inset Formula $t\in{1,…,ntree}$
\end_inset

.
\end_layout

\begin_layout Subsection
Variable Importance
\end_layout

\begin_layout Standard
Then the variable importance of variable
\begin_inset Formula $X_{j}$
\end_inset

 in tree
\begin_inset Formula $t$
\end_inset

 is
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
VI^{t}(X_{j})=\frac{\sum_{i\in\bar{B}^{t}}I(y_{i}=\hat{y}_{i}^{t})}{|\bar{B}^{t}|}-\frac{\sum_{i\in\bar{B}^{t}}I(y_{i}=\hat{y}_{i,\psi_{j}}^{t})}{|\bar{B}^{t}|}
\]

\end_inset

where
\begin_inset Formula $\hat{y}_{i,\psi_{j}}^{t}$
\end_inset

is the predicted class for observation 
\begin_inset Formula $i$
\end_inset

 after permuting its value of variable 
\begin_inset Formula $X_{j}$
\end_inset

 , i.e.
\end_layout

\begin_layout Standard
The raw variable importance score for each variable is then computed haste
 average importance over all trees
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
VI(X_{j})=\frac{\sum_{t}^{ntree}VI^{t}(X_{j})}{ntree}
\]

\end_inset


\end_layout

\begin_layout Standard
If each individual variable importance VI (t) has standard deviation
\begin_inset Formula $\sigma$
\end_inset

, the average importance from ntree replications has standard error
\begin_inset Formula $\sigma/\sqrt{ntree}$
\end_inset

ntree.The standardized or scaled importance is then computed as
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\tilde{VI}(X_{j})=VI(X_{j})/\frac{\hat{\sigma}}{\sqrt{ntree}}
\]

\end_inset


\end_layout

\begin_layout Standard
When the central limit theorem is applied to the mean importance VI (xj),Breiman
 and Cutler (2008) argue that the z-score is asymptotically normal.This property
 will be used explicitly for the statistical test that is critically investigate
d in Chapter 7.
\end_layout

\begin_layout Section
Practical Issues
\end_layout

\begin_layout Standard
In our experience random forests do remarkably well, with very little tuning
 required.
\end_layout

\begin_layout Standard
Not all estimators can be improved by shaking up the data like this.It seems
 that
\series bold
 highly nonlinear
\series default
 estimators, such as trees, benefit the most.
\end_layout

\begin_layout Subsection
R package
\end_layout

\begin_layout Standard
There is a randomForest package in R, maintained by Andy Liaw, available
 from the CRAN website.
\end_layout

\begin_layout Subsubsection
Random Froest can generate probalistic prediction
\end_layout

\begin_layout Standard
Just count how many tress predicts 1 and how many predicts 0
\end_layout

\begin_layout Subsubsection
where to stop
\series bold
 training
\end_layout

\begin_layout Standard
As the number of trees grow, OOB will shrink.
 Once the OOB error stabilizes, the training can be terminated.
\end_layout

\begin_layout Subsubsection
How to choose 
\begin_inset Formula $m$
\end_inset

 and node size of each tree 
\begin_inset Formula $m$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula $m$
\end_inset

 is the number of variables you select for each step.
\end_layout

\begin_layout Itemize
For classification, the default value for m is ⌊√p⌋ and the minimum node
 size is one.
\end_layout

\begin_layout Itemize
For regression, the default value for m is ⌊p/3⌋ and the minimum node size
 is five.
\end_layout

\begin_layout Subsubsection
Extremely Randomized Trees
\end_layout

\begin_layout Standard
As in random forests, a random subset of candidate features is used, but
 instead of looking for the most discriminative thresholds, thresholds are
 drawn at random for each candidate feature and the best of these randomly-gener
ated thresholds is picked as the splitting rule.
 This usually allows to reduce the variance of the model a bit more, at
 the expense of a slightly greater increase in bias:
\end_layout

\begin_layout Subsubsection
Overfitting
\end_layout

\begin_layout Standard

\series bold
Basic conclusion: Random Forest is robust to Overfitting.
\end_layout

\begin_layout Enumerate
When the number of variables is large, but the fraction of relevant variables
 small, random forests are likely to perform poorly with small 
\begin_inset Formula $m$
\end_inset

: as at each split the chance can be small that the relevant variables will
 be selected.
 According to Figure 15.7, this does not hurt the performance of random forests
 compared with boosting.
 This robustness is largely due to the relative insensitivity of misclassificati
on cost to the bias and variance of the probability estimates in each tree.
\end_layout

\begin_layout Enumerate
It is certainly true that increasing B does not cause the random forest
 sequence to overfit
\end_layout

\begin_layout Enumerate
Segal (2004) demonstrates small gains in performance by controlling the
 depths of the individual trees grown in random forests.Our experience is
 that using full-grown trees seldom costs much, and result sin one less
 tuning parameter.
\end_layout

\begin_layout Section
Application
\end_layout

\begin_layout Standard
Used in 
\end_layout

\begin_layout Itemize
Prediction
\end_layout

\begin_layout Itemize
Can give you the probability: just look at votes of all different tress
\end_layout

\begin_layout Itemize
Suffer to the unbalanced data.
\end_layout

\begin_layout Part
Generative Learning Algorithm
\end_layout

\begin_layout Section
Ideas
\end_layout

\begin_layout Standard
Lecture 5
\end_layout

\begin_layout Itemize

\series bold
Discriminative
\series default
 learning algorithms: they try to model 
\begin_inset Formula $P(y|x,\theta)$
\end_inset

, which is the map between features 
\begin_inset Formula $x$
\end_inset

 to class 
\begin_inset Formula $y$
\end_inset

.
\end_layout

\begin_layout Itemize

\series bold
Generative
\series default
 learning algorithm: Here, we’ll talk about algorithms that instead try
 to model 
\begin_inset Formula $p(x|y)$
\end_inset

 (and 
\begin_inset Formula $p(y)$
\end_inset

).
\end_layout

\begin_deeper
\begin_layout Itemize
First, looking at elephants, we can build a model of what elephants look
 like 
\begin_inset Formula $P(x|y=elephant)$
\end_inset

.Then, looking at dogs, we can build a separate model of what dogs look like
\begin_inset Formula $P(x|y=dog)$
\end_inset

.
\end_layout

\begin_layout Itemize
Finally, to classify a new animal, we can match the new animal against the
 elephant model, and match it against the dog model, to see whether the
 new animal looks more like the elephants or more like the dogs we had seen
 in the training set:
\begin_inset Formula $P(x|y=elephant)/P(x|y=dog)$
\end_inset


\end_layout

\end_deeper
\begin_layout Standard
In LDA, though 
\series bold
it starts from
\series default
 
\begin_inset Formula $P(y|x,\theta)$
\end_inset

, you still need to consider 
\begin_inset Formula $P(x|y=i)$
\end_inset

, so it is a Generative Learning Algorithm.
\end_layout

\begin_layout Standard
Steps of Generative Learning:
\end_layout

\begin_layout Enumerate
modeling 
\begin_inset Formula $p(x|y)$
\end_inset

 for each 
\begin_inset Formula $y$
\end_inset


\end_layout

\begin_layout Enumerate
Find the 
\begin_inset Formula $y$
\end_inset

 to max 
\begin_inset Formula $p(x|y)$
\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
OR, use Bayes Rule to get the classification/forecasting equation.
\end_layout

\end_deeper
\begin_layout Standard

\family typewriter
\begin_inset Formula 
\[
P(y|x)=\frac{P(x|y)P(y)}{P(x)}
\]

\end_inset


\end_layout

\begin_layout Standard
Actually if were calculating 
\begin_inset Formula $p(y|x)$
\end_inset

 in order to make a prediction, then we don’t actually need to calculate
 the denominator 
\begin_inset Formula $P(x)$
\end_inset

, since we only want to find a
\begin_inset Formula $y$
\end_inset

to
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
argmax_{y}P(y|x) & =argmax_{y} & \frac{P(x|y)P(y)}{P(x)}\\
 & =argmax_{y} & P(x|y)P(y)
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
Essentially that means we can do comparison between 
\begin_inset Formula $P(y=i|x)$
\end_inset

vs
\begin_inset Formula $P(y=j|x)$
\end_inset

to see which is biggest.
\end_layout

\begin_layout Subsection
Examples
\end_layout

\begin_layout Itemize
Linear and quadratic discriminative analysis (LDA and QDA)
\end_layout

\begin_layout Itemize
Mixture of Gaussian (saw in BNP module)
\end_layout

\begin_layout Itemize
Non-parametric density estimation for 
\begin_inset Formula $f_{k}$
\end_inset

(x)
\end_layout

\begin_layout Itemize
Naive Bayes
\end_layout

\begin_layout Section
LDA is the simplistic form of GDA
\end_layout

\begin_layout Standard

\series bold
See ISL p152.
 Clear enough
\end_layout

\begin_layout Standard
LDA as a restricted Gaussian classifier
\end_layout

\begin_layout Subsection
Usage
\end_layout

\begin_layout Standard
Similar usage like logit regression: to predict which group
\begin_inset Formula $y$
\end_inset

 belongs
\end_layout

\begin_layout Subsection
Idea
\end_layout

\begin_layout Standard
We model the distribution of the predictors X separately in each of the
 response classes (i.e.given Y ), and then use Bayes’ theorem to flip these
 around into estimates for Pr(Y = k|X = x).
\end_layout

\begin_layout Standard
Let
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
f_{k}(X)=P(X|Y=k)
\]

\end_inset


\end_layout

\begin_layout Standard
Then we assign the 
\begin_inset Formula $k$
\end_inset

 (thus do the forecasting) to max:
\begin_inset Formula 
\[
P(Y=k|X)=\frac{\pi_{k}f_{k}(X)}{\sum\pi_{i}f_{i}(X)}
\]

\end_inset


\end_layout

\begin_layout Standard

\series bold
Most of time we do not need to do the maximization, we just need to compare
 
\begin_inset Formula $\pi_{k}f_{k}(X)$
\end_inset

 between all possible 
\begin_inset Formula $k$
\end_inset

 to decide the label.
\end_layout

\begin_layout Subsection

\series bold
MLE to Estimate Prior and Likelihood
\end_layout

\begin_layout Standard

\series bold
This section introduces an informal way to estimate parameters in Prior
 and Likelihood.
 Formally you should use MLE, same as in Gaussian Discriminate.
\end_layout

\begin_layout Enumerate
How to choose
\series bold
 Prior
\series default
 
\begin_inset Formula $\pi_{i}$
\end_inset

: Sometimes we have knowledge of the class membership probabilities 
\begin_inset Formula $π_{1}$
\end_inset

,..., 
\begin_inset Formula $π_{K}$
\end_inset

 , which can be used directly.
 In the absence of any additional information, LDA estimates 
\begin_inset Formula $π_{k}$
\end_inset

 using the proportion of the training observations that belong to the kth
 class:
\begin_inset Formula $\pi_{i}=\frac{n_{K}}{N}$
\end_inset

.
\end_layout

\begin_layout Enumerate
How to choose
\series bold
 Likelihood
\series default
 
\begin_inset Formula $f_{K}(X)$
\end_inset

: multivariate Gaussian.See P157 ISL or P138 ESL for exact formulas,
\end_layout

\begin_deeper
\begin_layout Standard
\begin_inset Formula 
\[
f_{k}(X)=P(X|Y=k)=\frac{1}{(2\pi)^{p/2}|\Sigma|^{1/2}}exp\left\{ -\frac{1}{2}(x-\mu_{k})^{T}\Sigma^{-1}(x-\mu_{k})\right\} 
\]

\end_inset


\end_layout

\begin_layout Enumerate
where
\begin_inset Formula $u_{k}$
\end_inset

is the group mean of points in class
\begin_inset Formula $k$
\end_inset


\end_layout

\end_deeper
\begin_layout Subsection
LDA and Bayes Classifier
\end_layout

\begin_layout Standard
Thus LDA is the estimated Bayes classifier if the observations are multi-variate
 Gaussian in each class, with a common covariance matrix.
 Since this assumption is unlikely to be true, this might not seem to be
 much of a virtue.
\end_layout

\begin_layout Subsection
Relations with other model
\end_layout

\begin_layout Standard
We see that in terms of ability to classify, having the 
\begin_inset Formula $f_{K}(x)$
\end_inset

 (Generative Learning ) is almost equivalent to having the quantity 
\begin_inset Formula $Pr(G=k|X=x)$
\end_inset

 (
\series bold
Discriminative
\series default
 learning).
\end_layout

\begin_layout Standard
Many techniques are based on models for the class densities: 
\end_layout

\begin_layout Itemize
linear and quadratic discriminant analysis use Gaussian densities; 
\end_layout

\begin_layout Itemize
more flexible mixtures of Gaussian allow for nonlinear decision boundaries
 (Section 6.8);
\end_layout

\begin_layout Itemize
general non parametric density estimates for each class density allow the
 most flexibility (Section 6.6.2); 
\end_layout

\begin_layout Itemize
Naive Bayes models are a variant of the previous case, and assume that each
 of the class densities are products of marginal densities; that is, they
 assume that the inputs are conditionally independent in each class (Section
 6.6.3).
\end_layout

\begin_layout Subsection

\series bold
(
\series default
Gaussian
\series bold
) Discriminant Function: Compare posterior: to Forecast/ draw Decision Boundary
\end_layout

\begin_layout Standard
Assuming
\begin_inset Formula 
\[
f_{k}(x)=\frac{1}{(2\pi)^{p/2}|\Sigma|^{1/2}}exp\{-\frac{1}{2}(x-\mu_{k})^{T}\Sigma^{-1}(x-\mu_{k})^{T}\}
\]

\end_inset

, then
\series bold
 discriminant function 
\begin_inset Formula $\delta_{k}(x)$
\end_inset

 (see p125) is from the log ratio between two posteriors’ classes.
\end_layout

\begin_layout Standard

\series bold
\begin_inset Formula 
\begin{eqnarray*}
log\left[P(Y=k|x)/P(Y=l|x)\right] & = & log\frac{\pi_{k}}{\pi_{l}}+log\frac{f_{k}(x)}{f_{l}(x)}\\
 & = & log\frac{\pi_{k}}{\pi_{l}}-\frac{1}{2}(\mu_{k}+\mu_{l})^{T}\Sigma^{-1}(\mu_{k}-\mu_{l})+x^{T}\Sigma^{-1}(\mu_{k}-\mu_{l})\\
 & = & \delta_{k}(x)-\delta_{l}(x)
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
where 
\series bold

\begin_inset CommandInset label
LatexCommand label
name "eq:discriminant_function"

\end_inset

 is (ESL P126 formula 4.12)
\series default
 
\begin_inset Formula 
\[
\delta_{k}(x)=x^{T}\Sigma^{-1}\mu_{K}-\frac{1}{2}\mu_{K}^{T}\Sigma^{-1}\mu_{K}+log\pi_{K}
\]

\end_inset


\end_layout

\begin_layout Standard

\series bold
Prediction
\end_layout

\begin_layout Itemize
If under the same
\begin_inset Formula $X$
\end_inset

,
\begin_inset Formula $P(Y=k|X)=P(Y=j|X)$
\end_inset

, then we call this exact observation is on the
\series bold
 decision boundary
\series default
.
\end_layout

\begin_layout Subsection
Why LDA is linear
\end_layout

\begin_layout Standard
P128 ESL.(formula 4.11)
\end_layout

\begin_layout Enumerate
Under the multivariate Gaussian assumption, log of the
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
 
\begin_inset Formula $P(Y=k|X)$
\end_inset

 is a linear function of 
\begin_inset Formula $X$
\end_inset

, or the 
\family default
\series bold
\shape default
\size default
\emph default
\bar default
\strikeout default
\uuline default
\uwave default
\noun default
\color inherit
discriminant function
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:discriminant_function"

\end_inset

 is linear.
 That is why we call it as
\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\uuline default
\uwave default
\noun default
\color inherit
 Linear Discriminant Analysis.
\end_layout

\begin_layout Standard
It is easy to show that the coefficient vector from least squares is proportiona
l to the LDA direction given in (4.11).
 Since this derivation of the LDA direction via least squares does not use
 a Gaussian assumption for the features, its applicability extends beyond
 the realm of Gaussian data.However the derivation of the particular intercept
 or cut-point given in(4.11) does require Gaussian data.
\end_layout

\begin_layout Section
Computations for LDA
\end_layout

\begin_layout Standard
See ESL p129 (4.3.2 Computations for LDA) (eigen values)
\end_layout

\begin_layout Standard

\series bold
For the discriminative function:
\series default
 
\begin_inset Formula 
\[
\delta_{k}(x)=x^{T}\Sigma^{-1}\mu_{K}-\frac{1}{2}\mu_{K}^{T}\Sigma^{-1}\mu_{K}+log\pi_{K}
\]

\end_inset


\end_layout

\begin_layout Standard
We can rewrite
\end_layout

\begin_layout Itemize
as we know the matrix diagonalizing: 
\begin_inset Formula $\sum_{k}=U_{k}D_{k}U^{T}$
\end_inset

, thus 
\begin_inset Formula $\mu_{K}^{T}\Sigma^{-1}\mu_{K}=\left[U_{k}^{T}(x-\mu_{k})\right]^{T}D_{k}^{-1}\left[U_{k}^{T}(x-\mu_{k})\right]$
\end_inset

 
\end_layout

\begin_layout Itemize
\begin_inset Formula $log|\Sigma_{k}|=\sum_{l}\mbox{log}d_{kl}$
\end_inset

 where 
\begin_inset Formula $d_{kl}$
\end_inset

 are the positive eigen values for matrix 
\begin_inset Formula $\Sigma_{k}$
\end_inset


\end_layout

\begin_layout Subsection
between-groups covariance and within-groups covariance
\end_layout

\begin_layout Standard
When the between-groups covariance and within-groups covariance for two
 variables have opposite signs, it indicates that a better separation between
 groups can be obtained by using a linear combination of those two variables
 than by using either variable on its own.
\end_layout

\begin_layout Subsection
Forecasting for LDA
\end_layout

\begin_layout Standard
See ESL p132.
\end_layout

\begin_layout Standard
Sphere of the data meaning see Math.lyx
\end_layout

\begin_layout Standard
SEE http://little-book-of-r-for-multivariate-analysis.readthedocs.org/en/latest/sr
c/multivariateanalysis.html
\end_layout

\begin_layout Subsection
Coefficients of LDA in lda() from R package MASS
\end_layout

\begin_layout Standard
See ISL p175
\end_layout

\begin_layout Standard
The coefficients of linear discriminant output provides the linear combination
 of Lag1 and Lag2 that are used to form the LDA decision rule.
 In other words, these are the multipliers of the elements of X = x in (4.19).
 
\end_layout

\begin_layout Standard
If −0.642×Lag1−0.514×Lag2 is large, then the LDA classifier will predict market
 increase, and if it is small, then the LDA classifier will predicts market
 decline.
\end_layout

\begin_layout Standard
But there are further complex calculations after you calculate
\series bold
 coeff%*%x.
 So the use of coeffs are only to judge the relative effect of each vat,
 cannot be used to do prediction direction/
\end_layout

\begin_layout Section
Quadratic Discriminant Analysis
\end_layout

\begin_layout Standard
See ISL p163.
\end_layout

\begin_layout Standard
However, unlike LDA, QDA assumes that each class has its own covariance
 matrix.Then the classifier in high dimension will become a quadratic curve,
 thusserving a much flexible classifier than LDA.
\end_layout

\begin_layout Standard
ESL P126
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\delta_{k}(x)=-(x-\mu_{K})^{T}\Sigma_{k}^{-1}(x-\mu_{K})-\frac{1}{2}log|\Sigma_{K}|+log\pi_{K}
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename ../../Desktop/Lyx_Picture/QDA.png
	lyxscale 70
	scale 70

\end_inset


\end_layout

\begin_layout Section
Regularized Discriminant Analysis (LDA)
\end_layout

\begin_layout Standard
The regularized covariance matrices have the form
\begin_inset Formula 
\[
Σ_{k}(α)=αΣ_{k}+(1−α)Σ
\]

\end_inset


\end_layout

\begin_layout Standard
where Σ is the pooled covariance matrix as used in LDA.
 Here α ∈ [0, 1] allows a continuum of models between LDA and QDA', and
 needs to be specified.
 In practice α can be chosen based on the performance of the model on validation
 data, or by cross-validation.
 
\end_layout

\begin_layout Section
Naive Bayes (with Bernoulli as single word probability)
\end_layout

\begin_layout Standard
Another name: Bernoulli Event model
\end_layout

\begin_layout Itemize
Naive Bayes is one of Generative Learning Theory.
\end_layout

\begin_layout Itemize
Naive Bayes also use Exponential Class distribution.
 
\end_layout

\begin_layout Itemize
Any model based on Exponential Class distribution is Linear Classier.
\end_layout

\begin_layout Subsection
Notations
\end_layout

\begin_layout Description
\begin_inset Formula $i$
\end_inset

 email, superscript
\end_layout

\begin_layout Description
\begin_inset Formula $p,q$
\end_inset

 words, subscript
\end_layout

\begin_layout Subsection
Motivations and Assumption
\end_layout

\begin_layout Itemize
To model 
\begin_inset Formula $P(x|y)$
\end_inset

 and avoid the 
\begin_inset Formula $2^{50000}-1$
\end_inset

-dimensional parameter vector, we will therefore make a very strong assumption:
\series bold
 the 
\begin_inset Formula $x$
\end_inset

’s are conditionally independent given 
\begin_inset Formula $y$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Itemize
This assumption is called the
\series bold
 Naive Bayes (NB) assumption: given we know the email is spam or not, the
 probability of each word’ appearance is independent with each other.
\end_layout

\end_deeper
\begin_layout Itemize
For instance, if y = 1 means spam email; “buy” is word 2087 and “price”is
 word 39831; then we are assuming that if I tell you 
\begin_inset Formula $y=1$
\end_inset

 (that a particular piece of email is spam), then knowledge of x2087 (knowledge
 of whether“buy” appears in the message) will have no effect on your beliefs
 about the value of x39831 (whether “price” appears).
\end_layout

\begin_deeper
\begin_layout Itemize
That means, for any email 
\begin_inset Formula $i$
\end_inset

, the appearance of 
\begin_inset Formula $p^{th}$
\end_inset

 word is independent with appearance of 
\begin_inset Formula $q^{th}$
\end_inset

 word
\series bold
 given 
\begin_inset Formula $y$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
P(x_{p}^{i}|y^{i})=P(x_{p}^{i}|y^{i},x_{q}^{i})
\]

\end_inset


\end_layout

\end_deeper
\begin_layout Subsection
Likelihood
\end_layout

\begin_layout Itemize
Likelihood of a single word: In Naive base, we model each word with Bernoulli:
\end_layout

\begin_layout Itemize
\begin_inset Formula 
\[
P(x_{j}|y)=1_{j=1}\phi_{j=1|y}+1_{j=0}(1-\phi_{j=1|y})
\]

\end_inset


\end_layout

\begin_layout Itemize
The likelihood for one email (one words vector) Thus we can simplify 
\begin_inset Formula $P(x^{i}|y^{i})$
\end_inset

 as: 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
P(x^{i}|y^{i})=\prod_{j=1}^{K=5000}P(x_{j}^{i}|y^{i})\label{eq:Likelihood_one_email-1}
\end{equation}

\end_inset


\end_layout

\begin_layout Itemize
word 
\begin_inset Formula $j$
\end_inset

 may show up in email 
\begin_inset Formula $i$
\end_inset

 -- 
\begin_inset Formula $x_{j}^{i}=1$
\end_inset

 or not show up -- 
\begin_inset Formula $x_{j}^{i}=0$
\end_inset

.
 
\begin_inset Formula $K$
\end_inset

 is the length of dictionary.
\end_layout

\begin_deeper
\begin_layout Itemize
In LDA's examples, we model the whole word vector 
\begin_inset Formula $P(x_{j}|y_{j})$
\end_inset

 with Multi-Normal
\end_layout

\end_deeper
\begin_layout Subsection
Prediction the class of email 
\begin_inset Formula $i$
\end_inset


\end_layout

\begin_layout Standard
For new email 
\begin_inset Formula $i$
\end_inset

, compare 
\begin_inset Formula 
\[
\phi_{y_{i}}P(x^{i}|y^{i}=1)
\]

\end_inset

 and 
\begin_inset Formula 
\[
(1-\phi_{y_{i}})P(x^{i}|y^{i}=0)
\]

\end_inset

 to decide whether email 
\begin_inset Formula $i$
\end_inset

 is spam or not.
\end_layout

\begin_layout Subsection

\series bold
Parameters of mode
\series default
ls 
\begin_inset Formula $\phi_{j|y=1}$
\end_inset

,
\begin_inset Formula $\phi_{j|y=0}$
\end_inset

 and 
\begin_inset Formula $\phi_{y}=P(y=1)$
\end_inset

.
\end_layout

\begin_layout Standard
Given we we know Likelihood of one email
\begin_inset Formula $i$
\end_inset

 from
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:Likelihood_one_email-1"

\end_inset

, likelihood of all emails are 
\begin_inset Formula $L(\phi_{y},\phi_{j|y=0},\phi_{j|y=1})=\prod_{i=1}^{N}P(x^{i}|y^{i})P(y^{i})$
\end_inset

, then we can solve the MLE analytically, like in GDA
\end_layout

\begin_layout Itemize
Probability of word 
\begin_inset Formula $j$
\end_inset

 appearing in any spam email: is just the empirical chances 
\begin_inset Formula $j$
\end_inset

 appears in a spam.
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\phi_{j|y=1}=\frac{\sum_{i=1}^{m}1\{x_{j}=1\&y=1\}}{\sum y}
\]

\end_inset


\end_layout

\begin_layout Itemize
Prior Probability of an email being spam
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\phi_{y}=\frac{\sum y}{N}
\]

\end_inset


\end_layout

\begin_layout Subsection
Laplace Smooth Estimator: If a word never appear
\end_layout

\begin_layout Standard
For the
\begin_inset Formula $i^{th}$
\end_inset

word in your list, it never appears in your training sample, thus
\begin_inset Formula 
\[
P(x_{i}=1|y=1)=P(x_{i}=1|y=0)=0
\]

\end_inset

.Then
\begin_inset Formula $P(y=1|x)=\frac{0}{0}$
\end_inset

.
\end_layout

\begin_layout Standard
To solve this problem, as we know the unconditional probability of word
\begin_inset Formula $j$
\end_inset

appearing is
\begin_inset Formula $\phi_{j}=\frac{\sum^{m}1_{j=1}}{N}$
\end_inset

.In Laplace Smooth, we can re-write it as They we can let the conditional
 probability of word
\begin_inset Formula $i$
\end_inset

be
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
P(j=1)=\phi_{k}=\frac{1+\sum^{m}1_{j=1}}{\mathbf{K+}N}
\]

\end_inset

where
\begin_inset Formula $K=2$
\end_inset

as
\begin_inset Formula $K$
\end_inset

is the possible values of 
\begin_inset Formula $y$
\end_inset

.
 Therefore, for those words
\begin_inset Formula $i$
\end_inset

that never appeared, it has
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
P(j=i)=\phi_{k}=\frac{1+\sum^{m}1_{i=1}}{\mathbf{K+}N}=\frac{1}{K+N}
\]

\end_inset


\end_layout

\begin_layout Standard
You can still verify that
\begin_inset Formula $\sum_{j=1}^{k}\phi_{j}=1$
\end_inset

.
\end_layout

\begin_layout Standard
If there are five times
\begin_inset Formula $k=1$
\end_inset

and no
\begin_inset Formula $k=0$
\end_inset

, then
\begin_inset Formula $P(k=1)=\frac{6}{7}$
\end_inset

.
\end_layout

\begin_layout Standard
This can also be used in Multi-Normal representation or Naive Bayes.
\end_layout

\begin_layout Subsection
Key difference between Naive Bayes and Gaussian Discriminate (GDA)/LDA
\end_layout

\begin_layout Standard
Here we see the key difference between Naive Bayes and Gaussian Discriminate
 (GD).
\end_layout

\begin_layout Itemize
GD use multi-normal to describe joint of all words in one email.
\end_layout

\begin_deeper
\begin_layout Itemize

\series bold
In GD, you cannot write likelihood of a single word.
\end_layout

\end_deeper
\begin_layout Itemize
Naive Bayes only assume words are independent from each other given 
\begin_inset Formula $y$
\end_inset

, thus the joint of all words is just their 
\begin_inset Formula $\prod$
\end_inset

 of likelihood of each word.
\end_layout

\begin_deeper
\begin_layout Itemize
In Naive Bayes, the
\series bold
 likelihood of a single word 
\begin_inset Formula $j$
\end_inset

 is:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\phi_{j|y=1}=P(x_{j}=1|y=1)
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\phi_{j|y=0}=P(x_{j}=1|y=0)
\]

\end_inset


\end_layout

\end_deeper
\begin_layout Section
Naive Bayes: Multinomial 
\end_layout

\begin_layout Standard
Lecture 6
\end_layout

\begin_layout Standard
If a word appears multiple times: word 
\begin_inset Formula $j$
\end_inset

 appears in the email 
\begin_inset Formula $i$
\end_inset

 
\begin_inset Formula $n_{j}^{i}$
\end_inset

 times.
 Naive Bayes with Bernoulli cannot deal with this.
\end_layout

\begin_layout Subsubsection
Likelihood
\end_layout

\begin_layout Standard
There are 
\begin_inset Formula $n_{i}$
\end_inset

 words in the email 
\begin_inset Formula $i$
\end_inset

 (
\begin_inset Formula $n_{i}$
\end_inset

 does not equal to the dictionary length 
\begin_inset Formula $N$
\end_inset

), and word 
\begin_inset Formula $j$
\end_inset

 appears 
\begin_inset Formula $x_{j}^{i}$
\end_inset

 times in the email 
\begin_inset Formula $i$
\end_inset

, then the likelihood of email 
\begin_inset Formula $i$
\end_inset

 is
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
P(x^{i}|y)=\frac{n_{i}}{\prod^{N}x_{j}^{i}!}\prod^{N}P(x_{j}|y)^{x_{j}^{i}}\propto\prod^{N}P(x_{j}|y)^{x_{j}^{i}}
\]

\end_inset


\end_layout

\begin_layout Standard
Note here we still need Naive Bayes assumptions: appearance of word 
\begin_inset Formula $j$
\end_inset

 does not depend on other words.
 (Mathematically, Multinomial insures that every category in the trail is
 independent)
\end_layout

\begin_layout Standard
For the total likelihood of data:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
P(x,y)=\left\{ \prod_{i=1}^{n}P(x_{i}|y)\right\} P(y)
\]

\end_inset


\end_layout

\begin_layout Subsection
Application
\end_layout

\begin_layout Standard
Can also deal with continues values: Discretize the continuous values into
 
\begin_inset Formula $K$
\end_inset

 values
\end_layout

\begin_layout Part
Prototype Methods: K-Means & KNN
\end_layout

\begin_layout Standard
Prototype Methods is simple and essentially 
\series bold
model-free
\series default
 methods for classification and pattern recognition.
 Because they are 
\series bold
highly
\series default
 
\series bold
unstructured
\series default
, they typically are not useful for understanding the nature of the relationship
 between the 
\series bold
features
\series default
 and 
\series bold
class outcome
\series default
.
 
\end_layout

\begin_layout Standard
However, 
\series bold
as black box prediction engines
\series default
, they can be very effective, an dare often among the best performers in
 real data problems.
 The nearest-neighbor technique can also be used in regression; this was
 touched on in Chapter 2 and works reasonably well for low-dimensional problems.
\end_layout

\begin_layout Standard
Each 
\series bold
prototype
\series default
 has an associated class label, and classification of a query point 
\begin_inset Formula $x$
\end_inset

 is made to the class of the 
\series bold
closest
\series default
 
\series bold
prototype
\series default
.
 “Closest” is usually defined by Euclidean distance in the feature space,after
 each feature has been standardized to have overall mean 0 and variance1
 in the training sample.
\end_layout

\begin_layout Itemize
KNN is supervised
\end_layout

\begin_layout Itemize
K-means is unsupervised
\end_layout

\begin_layout Section
K-Means Clustering
\end_layout

\begin_layout Standard
Also see the
\begin_inset Quotes eld
\end_inset

Mixture Model
\begin_inset Quotes erd
\end_inset

part for the algorithm
\end_layout

\begin_layout Standard
K-means is an unsupervised method, by which you don’t know the label in
 training data at all.
 The goal is to to assign labels using this K-means method.
\end_layout

\begin_layout Itemize

\series bold
PCA looks to find a low-dimensional representation of the ALL observations,
 and that representation can explain a good fraction of the variance;
\end_layout

\begin_layout Itemize
Clustering looks to find 
\series bold
homogeneous subgroups
\series default
 among the 
\series bold
observations
\series default
.
\end_layout

\begin_layout Standard
K-means has a number of interesting theoretical properties.
 (https://en.wikipedia.org/wiki/File:KMeans-density-data.svg)
\end_layout

\begin_layout Itemize
First, it partitions the data space into a structure known as a Voronoi
 diagram.
 
\end_layout

\begin_layout Itemize
Second, it is conceptually close to nearest neighbor classification, and
 as such is popular in machine learning.
\end_layout

\begin_layout Subsection
In Unsupervised Learning: find Prototype Features.
\end_layout

\begin_layout Standard
k-means clustering aims to partition 
\begin_inset Formula $n$
\end_inset

 observations into
\begin_inset Formula $k$
\end_inset

clusters in which each observation belongs to the cluster with the nearest
 mean, serving as
\series bold
 a prototype of the cluster
\series default
.
\end_layout

\begin_layout Standard
Goal is to
\end_layout

\begin_layout Enumerate
get the prototype (essentially the prototype feature) of each class
\end_layout

\begin_layout Enumerate
once you get the prototypes, you can assign observations to each class
\end_layout

\begin_layout Standard
The goal is to minimize
\series bold
 “within cluster” point scatter
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
W(C)=\frac{1}{2}\sum_{k=1}^{K}\sum_{C(i)=k}d(x_{i},\mu_{j})
\]

\end_inset

where
\begin_inset Formula $\frac{1}{2}$
\end_inset

 is to deal with the double-counting when sum all the distances between
 points in class
\begin_inset Formula $k$
\end_inset

.
 
\begin_inset Formula $C(i)=k$
\end_inset

 means the label of observation 
\begin_inset Formula $i$
\end_inset

 is 
\begin_inset Formula $k$
\end_inset

.
\end_layout

\begin_layout Itemize
Input
\begin_inset Formula $\{x^{1},x^{2}…x^{m}\}$
\end_inset


\end_layout

\begin_layout Enumerate
Initialize
\begin_inset Formula $k$
\end_inset

 cluster centroids 
\begin_inset Formula $\{\mu_{1}…\mu_{k}\}$
\end_inset

 randomly
\end_layout

\begin_layout Enumerate
Repeat till convergence {
\end_layout

\begin_deeper
\begin_layout Enumerate
For every observation
\begin_inset Formula $i$
\end_inset

, set 
\begin_inset Formula $c^{i}=argmin_{j}||x^{i}-\mbox{\mu}_{j}||^{2}$
\end_inset

(assign a class
\begin_inset Formula $j$
\end_inset

to
\begin_inset Formula $x^{i}$
\end_inset

, note that
\begin_inset Formula $x^{i}$
\end_inset

might be a multi-dimentional vector, so you need to pre-define the distance
 between
\begin_inset Formula $x^{i}$
\end_inset

to
\begin_inset Formula $u_{j}$
\end_inset

)
\end_layout

\begin_layout Enumerate
For every class
\begin_inset Formula $j$
\end_inset

, set 
\begin_inset Formula $\mu_{j}=\frac{\sum1\{c^{i}=j\}x^{i}}{\sum1\{c^{i}=j\}}$
\end_inset

 (re-calculate the center/,mean of new class
\begin_inset Formula $j$
\end_inset

)
\end_layout

\end_deeper
\begin_layout Standard
Each of the step is to reduce the value of the following loss function (directly
 from
\series bold
“within cluster” point scatter
\series default
, see ESL P526).
 Thus K-means is guaranteed to converge.
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
W(C)=\sum^{K}N_{j}\sum_{C(i)=j}||x_{i}-u_{j}||^{2}
\]

\end_inset


\end_layout

\begin_layout Standard
where 
\begin_inset Formula $N_{j}$
\end_inset

 is the number of points in class 
\begin_inset Formula $j$
\end_inset

.
\end_layout

\begin_layout Standard
Therefore,
\begin_inset Formula $u_{j}$
\end_inset

 for each 
\begin_inset Formula $j$
\end_inset

 is the
\series bold
 prototype features.
\end_layout

\begin_layout Subsection
Empirical:
\end_layout

\begin_layout Standard
How to decide number of classes
\begin_inset Formula $K$
\end_inset


\end_layout

\begin_layout Itemize
Try or plot the data and look or business sense, OR, try different
\begin_inset Formula $K$
\end_inset

to see who gets the smallest 
\begin_inset Formula $W(C)$
\end_inset

.
\end_layout

\begin_layout Itemize
See the trend of 
\begin_inset Formula $W(C)$
\end_inset

 when you increase
\begin_inset Formula $K$
\end_inset

.
 ESL P535
\end_layout

\begin_deeper
\begin_layout Itemize
The intuition underlying the approach is that if there are actually 
\begin_inset Formula $K^{∗}$
\end_inset

 distinct groupings of the observations (as defined by the dissimilarity
 measure),then for 
\begin_inset Formula $K<K^{∗}$
\end_inset

 the clusters returned by the algorithm will each contain subset of the
 true underlying groups.
 That is, the solution will not assign observations in the same naturally
 occurring group to different estimated clusters.
 To the extent that this is the case, the solution criterion value will
 tend to decrease substantially with each successive increase in the number
 of specified clusters,
\end_layout

\begin_layout Itemize
For 
\begin_inset Formula $K>K^{∗}$
\end_inset

, one of the estimated clusters must partition at least one of the natural
 groups into two subgroups.
 This will tend to provide a smaller decrease in the criterion as K is
\end_layout

\end_deeper
\begin_layout Standard
where to start the
\begin_inset Formula $u_{j}$
\end_inset


\end_layout

\begin_layout Itemize
one should start the algorithm with many different random choices for the
 starting means, and choose the solution having smallest value of the objective
 function.
\end_layout

\begin_layout Subsection
Vector Quantization in Image Compression?
\end_layout

\begin_layout Standard
ESL P530.
\end_layout

\begin_layout Standard
The K-means clustering algorithm represents a key tool in the apparently
 unrelated area of image and signal compression, particularly in vector
 quantization or VQ
\end_layout

\begin_layout Standard
Why do we expect VQ to work at all? The reason is that for typical everyday
 images like photographs, many of the blocks look the same.
 In this case there are many almost pure white blocks, and similarly pure
 gray blocks of various shades.
 These require only one block each to represent them, and then multiple
 pointers to that block.
\end_layout

\begin_layout Subsection
Hierarchical Clustering: Bottom to Top
\end_layout

\begin_layout Standard
ESL P526
\end_layout

\begin_layout Standard
As the name suggests, they produce hierarchical representations in which
 the clusters at each level of the hierarchy are created by merging clusters
 at the next lower level.
 
\end_layout

\begin_layout Itemize
At the lowest level, each cluster contains a single observation.
 
\end_layout

\begin_layout Itemize
At the highest level there is only one cluster containing all of the data.
\end_layout

\begin_layout Standard
Each level of the hierarchy represents a particular grouping of the data
 into disjoint clusters of observations.
 The entire hierarchy represents an ordered sequence of such groupings .It
 is up to the user to decide which level (if any) actually represents a
 “natural” clustering in the sense that observations within each of its
 groups are sufficiently more similar to each other than to observations
 assigned to different groups at that level.
\end_layout

\begin_layout Itemize
That means, fit K-Means in order from large 
\begin_inset Formula $K_{1}$
\end_inset

 (lower hierarchy) to small 
\begin_inset Formula $K_{m}$
\end_inset

 (higher hierarchy): thus 
\begin_inset Formula $K_{1}....K_{m}$
\end_inset

 
\end_layout

\begin_layout Itemize
In step 
\begin_inset Formula $i$
\end_inset

, it treats groups in 
\begin_inset Formula $i-1$
\end_inset

 as 
\begin_inset Formula $K_{i-1}$
\end_inset

points, to insure inheriting of Hierarchies.
\end_layout

\begin_layout Subsection
K-Means in Supervised Learning
\end_layout

\begin_layout Standard
Goal is to get 
\begin_inset Formula $R$
\end_inset

 appropriate prototype for each class
\end_layout

\begin_layout Standard

\series bold
Procedure:
\end_layout

\begin_layout Enumerate
apply 
\begin_inset Formula $K-means$
\end_inset

 clustering to the training data in each class separately, using R prototypes
 per class;
\end_layout

\begin_layout Enumerate
assign a class label to each of the
\begin_inset Formula $K×R$
\end_inset

 prototypes;
\end_layout

\begin_layout Enumerate
Prediction: classify a new feature 
\begin_inset Formula $x$
\end_inset

 to the class of the closest prototype.
\end_layout

\begin_layout Section
KNN
\end_layout

\begin_layout Itemize
KNN is non-parametric!
\end_layout

\begin_layout Itemize
k-NN is a type of instance-based learning, or lazy learning, where the function
 is only approximated locally and all computation is deferred until classificati
on.
 The k-NN algorithm is among the simplest of all machine learning algorithms.
\end_layout

\begin_deeper
\begin_layout Itemize
KNN is to be used directly in Prediction.
 Estimate happens in prediction,
\end_layout

\end_deeper
\begin_layout Standard
KNN is non-parametric!
\end_layout

\begin_layout Standard
Let 
\begin_inset Formula $X$
\end_inset

 be the train data, and 
\begin_inset Formula $x_{0}$
\end_inset

 is a new data point.
 KNN method will predict the classification of 
\begin_inset Formula $x_{0}$
\end_inset

, 
\begin_inset Formula $Y$
\end_inset

, using the most possible classification of 
\begin_inset Formula $K$
\end_inset

 nearest points in 
\begin_inset Formula $X$
\end_inset

 to 
\begin_inset Formula $x_{0}$
\end_inset

.
\end_layout

\begin_layout Enumerate
Given a positive integer K , the KNN classifier first identifies the 
\begin_inset Formula $K$
\end_inset

 points in the training data that are closest to
\begin_inset Formula $x_{0}$
\end_inset

,.
 represented by
\begin_inset Formula $N_{0}$
\end_inset

.
\end_layout

\begin_layout Enumerate
It then estimates the conditional probability for class 
\begin_inset Formula $j$
\end_inset

 as the fraction of those 
\begin_inset Formula $K$
\end_inset

 points in 
\begin_inset Formula $N_{0}$
\end_inset

 with response values equal to 
\begin_inset Formula $j$
\end_inset

:
\end_layout

\begin_deeper
\begin_layout Standard
\begin_inset Formula 
\begin{equation}
P(Y=j|X=x_{0})=\frac{1}{K}\sum_{i\in N_{0}}I(y_{i}=j)\label{eq:Prediction_KNN}
\end{equation}

\end_inset


\end_layout

\end_deeper
\begin_layout Enumerate
In Prediction, KNN applies Bayes rule and classifies the test observation
 
\begin_inset Formula $x_{0}$
\end_inset

 to the class
\series bold
 with the largest probability.
\end_layout

\begin_layout Subsection
Prepare: Scaling Features
\end_layout

\begin_layout Standard
The accuracy of the k-NN algorithm can be severely degraded by the presence
 of noisy or irrelevant features, or if the feature scales are not consistent
 with their importance.
 Much research effort has been put into selecting or scaling features to
 improve classification.
 A particularly popular[citation needed] approach is the use of evolutionary
 algorithms to optimize feature scaling.[6] Another popular approach is to
 scale features by the mutual information of the training data with the
 training classes.[citation needed]
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
{\displaystyle I(X;Y)=\sum_{y\in Y}\sum_{x\in X}p(x,y)\log\left(\frac{p(x,y)}{p(x)\,p(y)}\right)}
\]

\end_inset


\end_layout

\begin_layout Subsection
Computational Concern
\end_layout

\begin_layout Standard

\series bold
It is computationally intensive for large training sets.
\end_layout

\begin_layout Standard
k-NN is a type of instance-based learning, or lazy learning, where the function
 is only approximated locally and all computation is deferred until classificati
on.
 The k-NN algorithm is among the simplest of all machine learning algorithms.
\end_layout

\begin_layout Standard
It is called instance-based because it constructs hypotheses directly from
 the training instances themselves.[2] This means that the hypothesis complexity
 can grow with the data:[2] in the worst case, a hypothesis is a list of
 n training items and the computational complexity of classifying a single
 new instance is O(n)
\end_layout

\begin_layout Subsection
Overfitting Concern
\end_layout

\begin_layout Standard
A shortcoming of the k-NN algorithm is that it is sensitive to the local
 structure of the data.[citation needed] The algorithm is not to be confused
 with k-means, another popular machine learning technique.
\end_layout

\begin_layout Subsection
Noise Feature Concern
\end_layout

\begin_layout Standard
The accuracy of the k-NN algorithm can be severely degraded by the presence
 of noisy or irrelevant features, or if the feature scales are not consistent
 with their importance.
 Much research effort has been put into selecting or scaling features to
 improve classification.
 
\end_layout

\begin_layout Itemize
Trim down the feature!
\end_layout

\begin_layout Itemize
PCA
\end_layout

\begin_layout Subsection
Data reduction
\end_layout

\begin_layout Standard
Data reduction is one of the most important problems for work with huge
 data sets.
 Usually, only some of the data points are needed for accurate classification.
 Those data are called the prototypes and can be found as follows:
\end_layout

\begin_layout Standard
Select the class-outliers, that is, training data that are classified incorrectl
y by k-NN (for a given k) Separate the rest of the data into two sets: 
\end_layout

\begin_layout Itemize
the prototypes that are used for the classification decisions 
\end_layout

\begin_layout Itemize
the absorbed points that can be correctly classified by k-NN using prototypes.
 The absorbed points can then be removed from the training set.
 
\end_layout

\begin_deeper
\begin_layout Itemize
One can apply the 1-nearest neighbor classifier on the cluster centers obtained
 by k-means to classify new data into the existing clusters.
 This is known as nearest centroid classifier or Rocchio algorithm.
\end_layout

\end_deeper
\begin_layout Subsection
KNN for multiple variables
\end_layout

\begin_layout Standard
You have to think about a way to define the nearest distance in a multiple
 dimension: like when you want to measure distance between two points, 
\begin_inset Formula $x_{0}$
\end_inset

 and 
\begin_inset Formula $X_{1}$
\end_inset

 in a 2-D word:
\begin_inset Formula $\sqrt{(x_{0,i}-X_{1,i})^{2}+(x_{0,j}-X_{1,j})^{2}}$
\end_inset


\end_layout

\begin_layout Subsection
How to choose 
\begin_inset Formula $K$
\end_inset


\end_layout

\begin_layout Itemize
When K = 1, the decision boundary is overly flexible and finds patterns
 in the data that don’t correspond to the Bayes decision boundary.
 This corresponds to a classifier that has low bias but very high variance.
 As K grows, the method becomes less flexible and produces a decision boundary
 that is close to linear.
\end_layout

\begin_layout Itemize
In binary (two class) classification problems, it is helpful to choose k
 to be an odd number as this avoids tied votes.
 One popular way of choosing the empirically optimal k in this setting is
 via bootstrap method.[7]
\end_layout

\begin_layout Subsection
Application: Document Classification
\end_layout

\begin_layout Standard
Intro_Infor_Retireval p324
\end_layout

\begin_layout Standard
Suppose 
\begin_inset Formula $d$
\end_inset

 is training documents and 
\begin_inset Formula $d'$
\end_inset

 is the new document, 
\end_layout

\begin_layout Standard
We can use 
\begin_inset Formula $cos(v(d\text{′}),v(d))$
\end_inset

 to measure the distance between 
\begin_inset Formula $d'$
\end_inset

 and 
\begin_inset Formula $d$
\end_inset

, (smaller the cos, larger the distance!)
\end_layout

\begin_layout Standard
Thus, a class label as 
\begin_inset Formula $c$
\end_inset

 has score:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
score(c,d)=\sum_{d'\in S_{k}(d)}I_{c}(d')cos(v(d'),v(d))
\]

\end_inset


\end_layout

\begin_layout Standard
where 
\begin_inset Formula $S_{k}(d)$
\end_inset

 is the set of d’s k nearest neighbors and 
\begin_inset Formula $I_{c}(d')$
\end_inset

 iff d′ is in class c and 0 otherwise.
 We then assign the document to the class with the highest score.
\end_layout

\begin_layout Part
Clustering
\end_layout

\begin_layout Itemize
Clustering is for unsupervised
\end_layout

\begin_layout Itemize
Classification is supervised
\end_layout

\begin_layout Section
K-Means
\end_layout

\begin_layout Standard
See Prototype Methods
\end_layout

\begin_layout Section
Mixture Models
\end_layout

\begin_layout Itemize
“Mixture models” are used to make statistical inferences about the properties
 of the sub-populations given only observations on the pooled population,
 without sub-population identity information (or without proper label for
 the sub-population).
\end_layout

\begin_layout Itemize
Similar as PCA, this is a Unsupervised method, 
\series bold
mainly to explore the data.
\end_layout

\begin_layout Subsection
Multivariate Gaussian Mixture model
\end_layout

\begin_layout Standard
A Bayesian Gaussian mixture model is commonly extended to fit a vector of
 unknown parameters (denoted in bold), or multivariate normal distributions.
 In a multivariate distribution (i.e.
 one modeling a vector 
\begin_inset Formula $\boldsymbol{x}$
\end_inset

 with 
\begin_inset Formula $N$
\end_inset

 random variables) one may model a vector of parameters (such as several
 observations of a signal or patches within an image) using a Gaussian mixture
 model prior distribution on the vector of estimates given by
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
p(\boldsymbol{\theta})=\sum_{i=1}^{K}\phi_{i}\mathcal{N}(\boldsymbol{\mu_{i},\Sigma_{i}})
\]

\end_inset


\end_layout

\begin_layout Standard
where the ith vector component is characterized by normal distributions
 with weights 
\begin_inset Formula $\phi_{i}$
\end_inset

, means 
\begin_inset Formula $\boldsymbol{\mu_{i}}$
\end_inset

 and covariance matrices 
\begin_inset Formula $\boldsymbol{\Sigma_{i}}$
\end_inset

.To incorporate this prior into a Bayesian estimation, the prior is multiplied
 with the known distribution 
\begin_inset Formula $p(\boldsymbol{x|\theta})$
\end_inset

 of the data 
\begin_inset Formula $\boldsymbol{x}$
\end_inset

 conditioned on the parameters 
\begin_inset Formula $\boldsymbol{\theta}$
\end_inset

 to be estimated.
 With this formulation, the posterior distribution 
\begin_inset Formula $p(\boldsymbol{\theta|x})$
\end_inset

 is also a Gaussian mixture model of the form
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
p(\boldsymbol{\theta|x})=\sum_{i=1}^{K}\tilde{\phi_{i}}\mathcal{N}(\boldsymbol{\tilde{\mu_{i}},\tilde{\Sigma_{i}}})
\]

\end_inset


\end_layout

\begin_layout Subsection
Example: House prices -- Neighborhood Cluster 
\end_layout

\begin_layout Standard
House prices[edit]
\end_layout

\begin_layout Standard
Assume that we observe the prices of N different houses.
 Different types of houses in different neighborhoods will have vastly different
 prices, but the price of a particular type of house in a particular neighborhoo
d (e.g., three-bedroom house in moderately upscale neighborhood) will tend
 to 
\series bold
cluster
\series default
 fairly closely around the mean.
 One possible model of such prices would be to assume that the prices areaccurat
ely described by a mixture model with 
\begin_inset Formula $K$
\end_inset

 different components, each distributed as a normal distribution with unknown
 mean and variance, with each component specifying a particular combination
 of house type/neighborhood.
\end_layout

\begin_layout Standard
Fitting this model to observed prices, e.g., using the expectation-maximization
 algorithm (treating the component label as latent varable), would tend
 to cluster the prices according to house type/neighborhood and reveal the
 spread of prices in each type/neighborhood.
 (Note that for values such as prices or incomes that are guaranteed tobe
 positive and which tend to grow exponentially, a log-normal distributionmight
 actually be a better model than a normal distribution.)
\end_layout

\begin_layout Subsection
When Mixture Gaussian fails
\end_layout

\begin_layout Standard
See the 
\begin_inset Quotes eld
\end_inset

K-means Limitations Illustrated
\begin_inset Quotes erd
\end_inset

 page at Data Clustering: K-means and Hierarchical Clustering
\end_layout

\begin_layout Standard
https://www.cs.utah.edu/~piyush/teaching/4-10-print.pdf
\end_layout

\begin_layout Standard
University of Utah
\end_layout

\begin_layout Itemize
Strong Assumption needed: 
\end_layout

\begin_deeper
\begin_layout Itemize
Distribution-based clustering produces complex models for clusters that
 can capture correlation and dependence between attributes.
 However, these algorithms put an extra burden on the user: for many real
 data sets, there may be no concisely defined mathematical model (e.g.
 assuming Gaussian distributions is a rather strong assumption on the data).
\end_layout

\begin_layout Itemize
For K-means, there is no such assumptions needed
\end_layout

\end_deeper
\begin_layout Section
EM in Mixture Gaussian
\end_layout

\begin_layout Standard
Lecture 13
\end_layout

\begin_layout Standard
Under the assumption that the data is generated by a mixure of Gaussin:
 In Gaussian Mixture Model, 
\begin_inset Formula $z^{i}$
\end_inset

 is the sub-population label for observation 
\begin_inset Formula $j$
\end_inset

, the goal is to estimate the subgroups’s parameter 
\begin_inset Formula $\phi_{j}$
\end_inset

 , 
\begin_inset Formula $\mu_{j}$
\end_inset

 and 
\begin_inset Formula $\Sigma_{j}$
\end_inset

.
 Thus we will treat 
\begin_inset Formula $z^{i}$
\end_inset

 as an latent random variable and get rid of it in the E-step
\end_layout

\begin_layout Standard
Prior assumption
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
z^{i} & \sim & MultiNomial(\phi,N)\\
 &  & \mbox{where }\phi=\{\phi_{1}..\phi_{K}\}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
x^{i}|z^{i}\sim N(\mu_{j},\Sigma_{j})
\]

\end_inset


\end_layout

\begin_layout Standard
Likelihood is 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
L(\phi,\mu,\Sigma) & = & \sum_{i}^{N}\mbox{log}P(x^{i}|\phi,\mu,\Sigma)\\
 & = & \sum_{i}^{N}\mbox{log}\sum_{z^{i}=1}^{K}P(x^{i}z^{i}|\mu_{z^{i}},\Sigma_{z^{i}})\\
 & = & \sum_{i}^{N}\mbox{log}\sum_{z^{i}=1}^{K}P(x^{i}|z^{i},\mu_{z^{i}},\Sigma_{z^{i}})P(z^{i}|\phi)\mbox{ (rewrite using latent label)}\\
 & \ge & \sum_{i}^{N}\sum_{z^{i}=1}^{K}P(z^{i}|\phi)\mbox{log}P(x^{i}|z^{i},\mu_{z^{i}},\Sigma_{z^{i}})
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Subsection
When 
\begin_inset Formula $z^{i}$
\end_inset

 is known: Mixture Gaussian collapes as Gaussian Discriminate
\end_layout

\begin_layout Standard
When 
\begin_inset Formula $z^{i}$
\end_inset

 is known for each 
\begin_inset Formula $i$
\end_inset

 in training data, then 
\begin_inset Formula $L(\phi,\mu,\Sigma)$
\end_inset

 converges to 
\begin_inset Formula $\sum_{i}^{N}\mbox{log}P(x^{i}|z^{i},\mu_{z^{i}},\Sigma_{z^{i}})P(z^{i}|\phi)$
\end_inset

, which is the same as Gaussian Discriminate.
\end_layout

\begin_layout Standard
But in Gaussian Mixture, the problem is 
\begin_inset Formula $z^{i}$
\end_inset

 is unknown in training data.
\end_layout

\begin_layout Subsection
EM Algorithm in Mixture Gaussian
\end_layout

\begin_layout Standard
This is a problem that you need to estimate the following population parameters
\end_layout

\begin_layout Enumerate
\begin_inset Formula $\phi_{j}$
\end_inset

, fraction of data coming from subgroup 
\begin_inset Formula $j$
\end_inset


\end_layout

\begin_layout Enumerate
\begin_inset Formula $\mu_{j}$
\end_inset

, mean of subgroup 
\begin_inset Formula $j$
\end_inset

; 
\begin_inset Formula $\sum_{j}$
\end_inset

 covarance of subgroup 
\begin_inset Formula $j$
\end_inset

.
\end_layout

\begin_layout Standard
given you don't know the subgroup labels 
\begin_inset Formula $z^{i}$
\end_inset

 for each observation 
\begin_inset Formula $x^{i}$
\end_inset

.
\end_layout

\begin_layout Enumerate

\series bold
(E-step) compute the probability of label 
\begin_inset Formula $z^{i}$
\end_inset

 is 
\begin_inset Formula $j$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Enumerate
\begin_inset Formula $w_{j}^{i}$
\end_inset

 is the Posterior probability that the point 
\begin_inset Formula $i$
\end_inset

 came from Gaussin 
\begin_inset Formula $j$
\end_inset

 (rather than a clear label with exact fixed value)
\begin_inset Formula 
\[
w_{j}^{i}=P(z^{i}=j|x^{i};\phi,\mu,\sum)=\frac{P(x^{i}|z^{i}=j)P(z^{i}=j)}{\sum_{z^{i}=l}P(x^{i}|z^{i}=l)P(z^{i}=l)}
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Enumerate
Thus for each point 
\begin_inset Formula $i$
\end_inset

, you got each 
\begin_inset Formula $j$
\end_inset

.
 So there are 
\begin_inset Formula $i\times j$
\end_inset

 
\begin_inset Formula $w_{j}^{i}$
\end_inset

!
\end_layout

\end_deeper
\begin_layout Enumerate
The likelihood for group 
\begin_inset Formula $j$
\end_inset

 is 
\begin_inset Formula $l(\theta_{i},\mu_{i},\sum_{i})=\prod_{i}^{N}w_{j}^{i}$
\end_inset


\end_layout

\begin_layout Itemize
Note that 
\begin_inset Formula $P(x^{i}|z^{i}=j)=N(\mu_{\theta},\Sigma_{\theta})$
\end_inset

 and 
\begin_inset Formula $P(z^{i}=j)=\theta_{j}$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Enumerate

\series bold
(M-step) update the estimates of parameters 
\begin_inset Formula $\theta$
\end_inset

, 
\begin_inset Formula $\mu$
\end_inset

 and 
\begin_inset Formula $\sum$
\end_inset

 for 
\begin_inset Formula $P(x^{i}|z^{i}=j)$
\end_inset

 through 
\begin_inset Formula 
\[
max_{\theta,\mu,\sum}L(\theta,\mu,\sum)=\prod_{j}\prod_{i}P(x^{i}|z^{i}=j)\times P(z^{i}=j)
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Enumerate
Update the 
\begin_inset Formula $\phi_{j}$
\end_inset

, fraction of data coming from 
\begin_inset Formula $j$
\end_inset

 ( 
\begin_inset Formula $N$
\end_inset

 is the sample size), whose max likelihood is the weighted possibility of
 each point 
\begin_inset Formula $i$
\end_inset

 coming from this 
\begin_inset Formula $j$
\end_inset


\begin_inset Formula 
\[
\phi_{j}:=\frac{1}{N}\sum w_{j}^{i}
\]

\end_inset


\end_layout

\begin_layout Enumerate
Adjust the mean of group 
\begin_inset Formula $j$
\end_inset

: Update 
\begin_inset Formula $\mu_{j}$
\end_inset

 for each Gaussin, where 
\begin_inset Formula $w_{j}^{i}$
\end_inset

 is the weight, or the probability of 
\begin_inset Formula $i$
\end_inset

 in 
\begin_inset Formula $j$
\end_inset

.
\begin_inset Formula 
\[
\mu_{j}=\frac{\sum_{i}w_{j}^{i}x^{i}}{\sum w_{j}^{i}}
\]

\end_inset


\end_layout

\begin_layout Enumerate
Adjust the covaraince of group 
\begin_inset Formula $j$
\end_inset

: Here we use diferent coverance for different Gassuins (rember in Gaussian
 Discriminative analysis, we use the same coverance by convention)
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Formula 
\[
\sum:=\frac{\sum w_{j}^{i}(x^{i}-\mu_{j})(x^{i}-\mu_{j})^{T}}{\sum w_{j}^{i}}
\]

\end_inset


\end_layout

\begin_layout Section
EM in Text Clustering: Mixture of Naive Bayes
\end_layout

\begin_layout Standard
Also refer to Intro_Information_Retireval p407
\end_layout

\begin_layout Standard
Multi-variant Bounulli Model:
\end_layout

\begin_layout Standard
Data: Documents represented using 
\begin_inset Formula $tf-idf$
\end_inset

 shceme: 
\begin_inset Formula $x_{j}^{i}=1$
\end_inset

 {word 
\begin_inset Formula $j$
\end_inset

 appears in document 
\begin_inset Formula $i$
\end_inset

}.
\end_layout

\begin_layout Itemize
Goal: 
\end_layout

\begin_deeper
\begin_layout Itemize
Estimate the probability of a document belonging to topic 
\begin_inset Formula $z$
\end_inset

: 
\begin_inset Formula $\phi_{z}$
\end_inset

 (like News clustering).
 For simplicity, we assume there are only two topics 
\begin_inset Formula $z=\{1,0\}$
\end_inset


\end_layout

\begin_layout Itemize
Estimate the probability of a word 
\begin_inset Formula $j$
\end_inset

 belonging to topic 
\begin_inset Formula $z$
\end_inset

, 
\begin_inset Formula $\phi_{j|z}$
\end_inset

 
\end_layout

\end_deeper
\begin_layout Itemize
Latent random variable (
\begin_inset Formula $z$
\end_inset

 is the label of cluster) 
\begin_inset Formula $z^{i}=\{0,1\}$
\end_inset

.
 That means we want to cluster all words into two classes.
\end_layout

\begin_layout Standard
\begin_inset Formula $z^{i}\sim Bernoulli(\phi)$
\end_inset

 and 
\begin_inset Formula $P(x_{j}^{i}=1|z^{i}=0)=\phi_{j|z=0}$
\end_inset

.
\end_layout

\begin_layout Standard
The the total likelihood (
\begin_inset Formula $M$
\end_inset

 documents)
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
logL(d^{i}|\Theta) & = & \sum_{j}^{M}\mbox{log}P(d^{i}|\Theta)\\
 & = & \sum_{j}^{M}\sum_{z}\mbox{log}P(d^{i},z^{i}|\Theta)\\
 & = & \sum_{j}^{M}\sum_{z}P(z^{i})\mbox{log}P(d^{i}|z^{i}\Theta)\\
 & = & \sum_{j}^{M}\sum_{z}P(z^{i})\mbox{log}\prod_{i\in d^{i}}P(x_{j}^{i}|z^{i},\Theta)\left(\prod_{i\notin d^{i}}(1-P(x_{j}^{i}|z^{i},\Theta))\right)
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
Here inside each document, 
\begin_inset Formula $P(d^{i}|z^{i}\Theta)$
\end_inset

 is made up by multiplication of P for each word, based on Naive Bayes.
\end_layout

\begin_layout Paragraph
E-Step
\end_layout

\begin_layout Standard
Take a best guess (posterior for 
\begin_inset Formula $z^{i}$
\end_inset

) to see whether document 
\begin_inset Formula $i$
\end_inset

 came from class 1 or 0, given probability of words in that document
\begin_inset Formula $w^{i}=Q^{i}=P(z^{i}=1|x^{i},\phi_{j|z},\phi)$
\end_inset


\end_layout

\begin_layout Paragraph
M-Step
\end_layout

\begin_layout Standard
Update the probability of document class 
\begin_inset Formula $1$
\end_inset

 and 
\begin_inset Formula $0$
\end_inset

, given all the words in Dictionary.
\end_layout

\begin_layout Itemize
Probability that word 
\begin_inset Formula $j$
\end_inset

 is coming from topic 
\begin_inset Formula $1$
\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
\begin_inset Formula 
\[
\phi_{j|z=1}=\frac{\sum_{i}^{m}\times w^{i}\mbox{1}\{x_{j}^{i}=1\}}{\sum_{i}^{m}w^{i}}
\]

\end_inset


\end_layout

\end_deeper
\begin_layout Itemize
Probability that word 
\begin_inset Formula $j$
\end_inset

 is coming from topic 0
\end_layout

\begin_deeper
\begin_layout Standard
\begin_inset Formula 
\[
\phi_{j|z=0}=\frac{\sum_{i}^{m}(1-w^{i})\times\mbox{1}\{x_{j}^{i}=1\}}{\sum_{i}^{m}(1-w^{i})}
\]

\end_inset


\end_layout

\end_deeper
\begin_layout Itemize
Probability that a document 
\begin_inset Formula $j$
\end_inset

 is coming from topic 
\begin_inset Formula $1$
\end_inset


\begin_inset Formula 
\[
\phi_{z=1}=\frac{\sum w^{i}}{m}
\]

\end_inset


\end_layout

\begin_layout Paragraph
Empirical Advice
\end_layout

\begin_layout Standard
\begin_inset Formula $w^{i}$
\end_inset

would be either very close to 1 or
\begin_inset Formula $0$
\end_inset

.
\end_layout

\begin_layout Section
Density-Based Clustering
\end_layout

\begin_layout Standard
In density-based clustering,[9] clusters are defined as areas of higher
 density than the remainder of the data set.
 Objects in these sparse areas - that are required to separate clusters
 - are usually considered to be noise and border points.
\end_layout

\begin_layout Standard
In this diagram, minPts = 4.
 Point A and the other red points are core points, because the area surrounding
 these points in an ε radius contain at least 4 points (including the point
 itself).
 Because they are all reachable from one another, they form a single cluster.
 Points B and C are not core points, but are reachable from A (via other
 core points) and thus belong to the cluster as well.
 Point N is a noise point that is neither a core point nor density-reachable.
\end_layout

\begin_layout Standard
Picture:
\end_layout

\begin_layout Standard
https://upload.wikimedia.org/wikipedia/commons/a/af/DBSCAN-Illustration.svg
\end_layout

\begin_layout Section
Hierarchical Clustering
\end_layout

\begin_layout Standard
Algorithm -- From bottom to top:
\end_layout

\begin_layout Standard
Every time find the closest 
\series bold
two
\series default
 points/cluster, and merge them into one cluster.
\end_layout

\begin_layout Standard
https://upload.wikimedia.org/wikipedia/commons/a/ad/Hierarchical_clustering_simple
_diagram.svg
\end_layout

\begin_layout Subsection
Difference from K-Means / Clustering Gaussian
\end_layout

\begin_layout Itemize
Time Complexity: To add to Franck Dernoncourt and Vishal Bhargava's answer
 on execution time, K-means is linear in the number of data objects i.e.
 O(n), where n is the number of data objects.
 The time complexity of most of the hierarchical clustering algorithms is
 quadratic i.e.
 O(n2).
 Therefore, for the same amount of data, hierarchical clustering will take
 quadratic amount of time.
 Imagine clustering 1 million records?
\end_layout

\begin_layout Itemize
Shape of Clusters: K-means works well when the shape of clusters are hyper-spher
ical (or circular in 2 dimensions).
 If the natural clusters occurring in the dataset are non-spherical then
 probably K-means is not a good choice.
\end_layout

\begin_layout Itemize
Repeatability: K-means starts with a random choice of cluster centers, therefore
 it may yield different clustering results on different runs of the algorithm.
 Thus, the results may not be repeatable and lack consistency.
 However, with hierarchical clustering, you will most definitely get the
 same clustering results.
\end_layout

\begin_layout Itemize
Prior-Knowledge/ Hyper Parameter: Off course, K-means clustering requires
 prior knowledge of K (or number of clusters), whereas in hierarchical clusterin
g you can stop at whatever level (or clusters) you wish
\end_layout

\begin_layout Section
Density Estimation (Abnormal Identify)
\end_layout

\begin_layout Itemize
To identify the outliers (from the subgroup 'outlier') and probability of
 that appearance of that outlier (data is muti-dimentional)
\end_layout

\begin_layout Itemize
Assume data is generated by two Gaussians, you want to identify which data
 comes from which Gaussian.
 (you don’t have the access to the labels of data)
\end_layout

\begin_layout Standard
There’s a laten random variable
\begin_inset Formula $z$
\end_inset

.
\begin_inset Formula 
\[
z^{i}\sim MultiNormial(\phi)
\]

\end_inset

(
\begin_inset Formula $\phi_{j}>0$
\end_inset

 and 
\begin_inset Formula $\sum\phi_{j}=1$
\end_inset

).
\begin_inset Formula $z^{i}$
\end_inset

is just the latend label.
\begin_inset Formula $\phi_{j}$
\end_inset

is the probability that a random point 
\begin_inset Formula $i$
\end_inset

 comes from class 
\begin_inset Formula $j$
\end_inset

, its max-likelihood estimator is just the actual fraction of your data
 that come from
\begin_inset Formula $j$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\phi_{j}=P(z^{i}=j)=\frac{1}{m}\sum_{i}1\{z^{i}=j\}
\]

\end_inset


\end_layout

\begin_layout Part
Factor Analysis
\end_layout

\begin_layout Section
Factor Analysis Model with EM algorithm ??
\end_layout

\begin_layout Paragraph*
Situation
\end_layout

\begin_layout Itemize
It is possible that variations in
\begin_inset Formula $4$
\end_inset

 observed variables mainly reflect the variations in
\begin_inset Formula $2$
\end_inset

 unobserved variables.
\end_layout

\begin_layout Itemize
The observed variables are modelled as
\series bold
 linear combinations of the potential factors
\series default
, plus “error” terms.
\end_layout

\begin_layout Paragraph*
Use:
\end_layout

\begin_layout Standard
The information gained about the interdependencies between observed variable
 scan be used later to reduce the set of variables in a dataset.
\end_layout

\begin_layout Standard
Lecture 13: 31m
\end_layout

\begin_layout Itemize
Not as wide as mixture Gaussin/Bayes
\end_layout

\begin_layout Itemize
But derivation of math is interesting.
\end_layout

\begin_layout Itemize
Mixture Gaussin is applicable when sample size
\begin_inset Formula $m$
\end_inset

 is much larger than cluster
\begin_inset Formula $n$
\end_inset

.

\series bold
 Factor Analysis Model can be used when 
\begin_inset Formula $m$
\end_inset

 is near 
\begin_inset Formula $n$
\end_inset

.
 In that case, matrix for variance of Gaussian is non-invertable!
\end_layout

\begin_layout Enumerate
We can assume for different clusters, there is no correlation, thus coverance
 matrix
\begin_inset Formula $\Sigma$
\end_inset

is Diagonal, with diagonal value as variances of Gaussians of differentclusters.
\end_layout

\begin_deeper
\begin_layout Enumerate
Thus covaiance matrix is non-singular
\end_layout

\begin_layout Enumerate
But no correlation bad assumption
\end_layout

\end_deeper
\begin_layout Subsection
Setting: 
\end_layout

\begin_layout Standard
Suppose for some unknown constants 
\begin_inset Formula $l_{ij}$
\end_inset

and 
\begin_inset Formula $k$
\end_inset

 unobserved random variables 
\begin_inset Formula $F_{j}$
\end_inset

, where
\end_layout

\begin_layout Paragraph

\series bold
Parameters of this model is
\begin_inset Formula $\mu$
\end_inset

,
\begin_inset Formula $\lambda$
\end_inset

 and 
\begin_inset Formula $\Psi$
\end_inset

.
\end_layout

\begin_layout Standard
as in factor analysis, we assume
\begin_inset Formula 
\[
x=\mu+\lambda z+\epsilon
\]

\end_inset

where
\begin_inset Formula $\epsilon\sim N(0,\Psi)$
\end_inset

 and the
\begin_inset Formula $d-dimention$
\end_inset

 vector,
\begin_inset Formula $z$
\end_inset

 is the key random variables driving all 
\begin_inset Formula $n-dimentional$
\end_inset

 vector
\begin_inset Formula $x$
\end_inset

.
\end_layout

\begin_layout Itemize
\begin_inset Formula $z$
\end_inset

 is a vector random latent variable
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
z\sim N(0,I)
\]

\end_inset


\end_layout

\begin_layout Itemize
\begin_inset Formula $\lambda$
\end_inset

 is a vecotor constant number
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
x|z\sim N(\mu+\lambda z,\Psi)
\]

\end_inset


\end_layout

\begin_layout Paragraph*
Non-matrix Setting (form wiki)
\end_layout

\begin_layout Itemize
Suppose we have a set of 
\begin_inset Formula $p$
\end_inset

 observable random variables, 
\begin_inset Formula $x_{1},\dots,x_{p}$
\end_inset

 with means 
\begin_inset Formula $\mu_{1},\dots,\mu_{p}$
\end_inset

.
\end_layout

\begin_layout Itemize
Suppose for some unknown constants 
\begin_inset Formula $l_{ij}$
\end_inset

and 
\begin_inset Formula $k$
\end_inset

 unobserved random variables 
\begin_inset Formula $F_{j}$
\end_inset

 (the unknown underlying variable), where 
\begin_inset Formula $i\in{1,\dots,p}$
\end_inset

 and 
\begin_inset Formula $j\in{1,\dots,k}$
\end_inset

, where 
\begin_inset Formula $k<p$
\end_inset

, we have
\end_layout

\begin_deeper
\begin_layout Standard
\begin_inset Formula 
\[
x_{i}-\mu_{i}=l_{i1}F_{1}+\cdots+l_{ik}F_{k}+\varepsilon_{i}
\]

\end_inset


\end_layout

\end_deeper
\begin_layout Paragraph
Matrix Setting
\end_layout

\begin_layout Itemize
Unlabelled trainning set
\begin_inset Formula $\{x^{1}…x^{m}\}$
\end_inset

, each
\begin_inset Formula $x^{i}\in R^{n}$
\end_inset

(there are
\begin_inset Formula $n$
\end_inset

variables)
\end_layout

\begin_layout Itemize
Latent random variable
\begin_inset Formula $z\sim N(0,1)$
\end_inset

,
\begin_inset Formula $z\in R^{d}$
\end_inset

(there are
\begin_inset Formula $d$
\end_inset

 potential factors, normally
\begin_inset Formula $d<n$
\end_inset

)
\end_layout

\begin_layout Itemize
Those
\begin_inset Formula $n$
\end_inset

variables are essentially driven by
\begin_inset Formula $d$
\end_inset

latent variable:
\begin_inset Formula 
\[
x=\mu+\lambda z+\epsilon
\]

\end_inset

where
\begin_inset Formula $\epsilon$
\end_inset

is noise:
\begin_inset Formula $\epsilon\sim N(0,\Psi)$
\end_inset

where
\begin_inset Formula $\Psi$
\end_inset

is diagonal.
\end_layout

\begin_layout Standard
Given
\begin_inset Formula $x=\mu+\lambda z+\epsilon$
\end_inset

, we can know the jointly distribution:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\left\{ \begin{array}[t]{c}
z\\
x
\end{array}\right\} \sim N\left\{ \begin{array}[t]{c}
0\\
\mu
\end{array},\begin{array}[t]{cc}
1 & \lambda^{T}\\
\lambda & \lambda\lambda^{T}+\Psi
\end{array}\right\} 
\]

\end_inset


\end_layout

\begin_layout Standard
Likelihood of paprameters for (M-step)
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\prod_{i}^{m}P(x^{i}|\lambda,\mu,\Psi)
\]

\end_inset


\end_layout

\begin_layout Subsection
EM
\end_layout

\begin_layout Standard
Essentially we are doing
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
max_{\theta}\prod^{m}P(x^{i}|\theta)=max_{\theta}\prod\int_{z^{i}}P(x^{i}|z^{i},\theta)d(z^{i})
\]

\end_inset


\end_layout

\begin_layout Paragraph
E-step: Posterior of the Latent Factors
\end_layout

\begin_layout Standard
From the join distribution of
\begin_inset Formula $x,z$
\end_inset

 we know:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
Q_{i}(z^{i})=P(z^{i}|x^{i},\theta)
\]

\end_inset


\end_layout

\begin_layout Standard
where 
\begin_inset Formula $P(z^{i}|x^{i})\sim N(\mu_{z^{i}|x^{i}},\Sigma_{z^{i}|x^{i}})$
\end_inset

 and 
\begin_inset Formula $\mu_{z^{i}|x^{i}}=”\mu_{1}-\epsilon_{12}\epsilon_{22}^{-1}”$
\end_inset

.
 See notes for detail.
\end_layout

\begin_layout Paragraph
M-step
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
argmax_{\theta}\sum_{i}\int_{z^{i}}Q(z^{i})log\frac{P(x^{i}|z^{i},\theta)}{Q(z^{i})}dz^{i}
\]

\end_inset


\end_layout

\begin_layout Standard
Here
\begin_inset Formula $\theta$
\end_inset

refers to
\begin_inset Formula $\lambda$
\end_inset

 and 
\begin_inset Formula $\Psi$
\end_inset

.
 See notes for detail.
\end_layout

\begin_layout Section
Principle Component Analysis (PCA)
\end_layout

\begin_layout Standard
http://www.stat.cmu.edu/~cshalizi/uADA/12/
\end_layout

\begin_layout Standard
When number of features
\begin_inset Formula $P$
\end_inset

 is large, PCA can be used to lower the dimention.
 
\end_layout

\begin_layout Standard

\series bold
The first PCA is the linear combination of all 
\begin_inset Formula $P$
\end_inset

 of 
\begin_inset Formula $X$
\end_inset

 that explain largest variance within 
\begin_inset Formula $X$
\end_inset

.
\end_layout

\begin_layout Standard
Note that this is an un-supervised algorithm, thus it does not need 
\begin_inset Formula $y$
\end_inset

.
\end_layout

\begin_layout Itemize

\emph on
Lecture 14 Ng
\end_layout

\begin_layout Itemize
ISL P270.R code
\end_layout

\begin_layout Standard
Similar motivation as Factor analysis
\end_layout

\begin_layout Itemize
Unlabelled trainning set
\begin_inset Formula $\{x^{1}…x^{m}\}$
\end_inset

, each 
\begin_inset Formula $x^{i}\in R^{n}$
\end_inset

(there are
\begin_inset Formula $n$
\end_inset

variables)
\end_layout

\begin_layout Itemize
Goal: reduce it to low dimentional-
\begin_inset Formula $k$
\end_inset

 dataset, 
\begin_inset Formula $k<n$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Itemize
you may want to Find the variables of interests.
 Then other variables are just noises or dependent on those variables ofinterest
s.
\end_layout

\begin_layout Itemize
you may have more features than data 
\begin_inset Formula $p>n$
\end_inset

, so you may want to do dimention reduction to make OLS or other method
 possible.
\end_layout

\begin_deeper
\begin_layout Itemize
Use PCAs as new features in OLS
\end_layout

\end_deeper
\end_deeper
\begin_layout Subsection
Goal
\end_layout

\begin_layout Standard
Paper: Robust Principal Component Analysis?
\end_layout

\begin_layout Standard
To identify a low-rank representation of data matrix 
\begin_inset Formula $M$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
M=L_{0}+N_{0}
\]

\end_inset

where 
\begin_inset Formula $L_{0}$
\end_inset

 has low-rank and 
\begin_inset Formula $N_{0}$
\end_inset

 is a small perturbation matrix.
\end_layout

\begin_layout Standard
Classical Principal Component Analysis (PCA) seeks the best (in an 
\begin_inset Formula $l^{2}$
\end_inset

 sense) rank-k estimate of 
\begin_inset Formula $L_{0}$
\end_inset

 by solving
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
min||M-L||\\
subject & to & rank(L)\le k
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Subsection
Pre-pocess data
\end_layout

\begin_layout Enumerate
Normaliazed the data into zerio mean and unit variance (PCA is sensitive
 to scale)
\end_layout

\begin_layout Enumerate
Compute the covarance matrix 
\begin_inset Formula $\xi$
\end_inset

 of your normailzed data.
\end_layout

\begin_layout Enumerate
Find the top eigen vectors of 
\begin_inset Formula $\xi$
\end_inset

.
\end_layout

\begin_layout Subsection
Dimentions
\end_layout

\begin_layout Itemize
\begin_inset Formula $X=n\times p$
\end_inset

, 
\end_layout

\begin_deeper
\begin_layout Itemize
for single observation 
\begin_inset Formula $x_{i}=p\times1$
\end_inset


\end_layout

\end_deeper
\begin_layout Itemize
projected data 
\begin_inset Formula $XV_{q}=n\times q$
\end_inset

, 
\end_layout

\begin_deeper
\begin_layout Itemize
for single observation 
\begin_inset Formula $V_{q}^{T}x_{i}=q\times1$
\end_inset


\end_layout

\end_deeper
\begin_layout Subsection
Method 1: Find the principle axis (Loading Vectors 
\begin_inset Formula $\mu$
\end_inset

) that max variance
\end_layout

\begin_layout Standard
Need to find the projection line, where projections of data have largestvariance.
OR minimize the sum of squared of projection lengths
\end_layout

\begin_layout Standard
Let
\begin_inset Formula $||u||$
\end_inset

, vector 
\begin_inset Formula $x^{i}$
\end_inset

 projected on 
\begin_inset Formula $u$
\end_inset

 has length
\begin_inset Formula $(x^{i})u$
\end_inset

,
\end_layout

\begin_layout Standard
Choose 
\begin_inset Formula $u$
\end_inset

 to max the variance (here 
\begin_inset Formula $x$
\end_inset

 is standardized, so the variance formula below does not need to be minusmean)
\begin_inset Formula 
\begin{eqnarray*}
max_{u}\frac{1}{m}Var(PAC)^{2} & = & max_{u}\frac{1}{m}\sum\left[(x^{i})^{T}\mu\right]^{2}\\
 & = & max_{\mu}\frac{1}{m}||(X\mu)^{T}(X\mu)||
\end{eqnarray*}

\end_inset

subject
\begin_inset Formula $u^{T}u=1$
\end_inset

.
 Note that 
\begin_inset Formula $\sum=\frac{1}{m}\sum_{i=1}^{m}x^{i}(x^{i})^{T}$
\end_inset

.
\end_layout

\begin_layout Standard
Lagrange
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\nabla_{u}L=\Sigma\mu-\lambda u=0
\]

\end_inset


\end_layout

\begin_layout Standard

\series bold
This implies that
\begin_inset Formula $u$
\end_inset

 must be the pricinple eigen vector.
\end_layout

\begin_layout Subsection
Method 2: Error Minimization
\end_layout

\begin_layout Standard
Regarding 
\begin_inset Formula $XV_{q}$
\end_inset

 and 
\begin_inset Formula $XV_{q}V_{q}^{T}$
\end_inset

, see Math -- Projection.
\end_layout

\begin_layout Standard
Note that ths plane below in 
\begin_inset Formula $R^{N}$
\end_inset

 space is made by points 
\begin_inset Formula $XV_{q}V_{q}^{T}$
\end_inset

, not 
\begin_inset Formula $XV_{q}$
\end_inset

(which is only in 
\begin_inset Formula $q$
\end_inset

 space)
\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename Lyx_Picture/PCA.png
	lyxscale 80
	scale 70

\end_inset


\end_layout

\begin_layout Standard
Minimize 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
(X-XV_{q}V_{q}^{T})^{2} & = & X^{T}X-X^{T}XV_{q}V_{q}^{T}+V_{q}V_{q}^{T}X^{T}X+V_{q}V_{q}^{T}X^{T}XV_{q}V_{q}^{T}\\
 & = & X^{T}X+V_{q}V_{q}^{T}X^{T}XV_{q}V_{q}^{T}\\
 & = & X^{T}X+(XV_{q}V_{q}^{T})^{2}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
where 
\begin_inset Formula $X^{T}XV_{q}V_{q}^{T}=V_{q}V_{q}^{T}X^{T}X$
\end_inset

 as both are sysmetrical matrix.
\end_layout

\begin_layout Standard
As 
\begin_inset Formula $\partial X^{T}X+(XV_{q}V_{q}^{T})^{2}/\partial V_{q}=2V_{q}X^{T}XV_{q}V_{q}^{T}$
\end_inset

, the Lagrange is 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
2V_{q}X^{T}XV_{q}V_{q}^{T}-2\lambda V_{q} & = & 0\\
X^{T}XV_{q}V_{q}^{T} & = & \lambda(\text{左乘}V_{q}^{T})\\
X^{T}XV_{q} & = & \lambda V_{q}(\text{右乘}V_{q})
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard

\series bold
This implies that 
\begin_inset Formula $V_{q}$
\end_inset

 must be the pricinple eigen vector.
\end_layout

\begin_layout Subsection
Second PCA
\end_layout

\begin_layout Standard
The second principal component function is defined in a similar way, with
 the additional constraint that orthogonal to First PCA
\end_layout

\begin_layout Standard
The kth component can be found by subtracting the first k − 1 principal
 components from X:
\begin_inset Formula 
\[
{\displaystyle \mathbf{\hat{X}}_{k}=\mathbf{X}-\sum_{s=1}^{k-1}\mathbf{X}\mathbf{w}_{(s)}\mathbf{w}_{(s)}^{{\rm {T}}}}
\]

\end_inset

 and then finding the loading vector which extracts the maximum variance
 from this new data matrix
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
{\displaystyle \mathbf{w}_{(k)}} & = & arg\,max\left\{ \Vert\mathbf{\hat{X}}_{k}\mathbf{w}\Vert^{2}\right\} \\
 & = & \arg\,max\,\left\{ \tfrac{\mathbf{w}^{T}\mathbf{\hat{X}}_{k}^{T}\mathbf{\hat{X}}_{k}\mathbf{w}}{\mathbf{w}^{T}\mathbf{w}}\right\} 
\end{eqnarray*}

\end_inset

It turns out that this gives the remaining eigenvectors of XTX, with the
 maximum values for the quantity in brackets given by their corresponding
 eigenvalues.
 Thus the loading vectors are eigenvectors of XTX.
\end_layout

\begin_layout Standard
The kth component of a data vector x(i) can therefore be given as a
\end_layout

\begin_layout Subsection
Projected data are de-correlated in new basis
\end_layout

\begin_layout Standard
we can now show that the projected data
\begin_inset Formula $y$
\end_inset

 in the 
\begin_inset Formula $q$
\end_inset

 dimention (by first 
\begin_inset Formula $q$
\end_inset

 PCAs) are de-correlated in this new basis
\end_layout

\begin_layout Itemize
http://www.ics.uci.edu/~welling/classnotes/papers_class/Kernel-PCA.pdf
\end_layout

\begin_layout Itemize
\begin_inset Formula $U_{k}$
\end_inset

 means the 
\begin_inset Formula $d\times k$
\end_inset

 sub-matrix containing the first k eigenvectors as columns
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\frac{1}{N}\sum y_{i}y_{i}^{T}=\frac{1}{N}\sum U_{k}x_{i}x_{i}^{T}U_{k}=U_{k}^{T}CU_{k}=U_{k}^{T}U\Lambda U^{T}U_{k}=\Lambda_{k}
\]

\end_inset


\end_layout

\begin_layout Subsection
Diagonalization of 
\begin_inset Formula $\Sigma$
\end_inset


\end_layout

\begin_layout Standard
where 
\begin_inset Formula $\Sigma$
\end_inset

 is the covarance matrix:
\begin_inset Formula 
\[
\sum=\frac{1}{m}\sum_{i=1}^{m}x^{i}(x^{i})^{T}
\]

\end_inset


\end_layout

\begin_layout Standard

\series bold
It is a symmetric matrix and so it can be diagonalized: 
\begin_inset Formula 
\[
\Sigma=VLV^{T}
\]

\end_inset


\series default
, where V is a matrix of eigenvectors (each column is an eigenvector) and
 L is a diagonal matrix with eigenvalues λi in the decreasing order on the
 diagonal.
 
\end_layout

\begin_layout Subsection

\series bold
Principal Components and Eigenvectors
\end_layout

\begin_layout Itemize
The eigenvectors are called principal axes or principal directions of the
 data.
 
\end_layout

\begin_layout Itemize

\series bold
As 
\begin_inset Formula $X=USV^{T}$
\end_inset

, and 
\begin_inset Formula $V_{q}$
\end_inset

 are the first 
\begin_inset Formula $q$
\end_inset

 eigen vectors of 
\begin_inset Formula $X$
\end_inset

, which are the first 
\begin_inset Formula $q$
\end_inset

 columns of 
\begin_inset Formula $V$
\end_inset

.
 Therefore Principal Components/scpoes
\series default
 = 
\begin_inset Formula $XV=US$
\end_inset

.
 Projections of the data on the principal axes are called principal components,
 also known as PC scores; these can be seen as new, transformed, variables.
 
\end_layout

\begin_deeper
\begin_layout Standard
More generally, if we wish to project o`ur data into a k-dimensional subspace(k
 < n), we should choose u1, ..., uk to be the top k eigenvectors of .
\end_layout

\begin_layout Standard

\series bold
Thus, the
\series default
 
\series bold

\begin_inset Formula 
\[
PCA(1)=X\mu_{1}
\]

\end_inset

 where 
\begin_inset Formula $\mu_{1}$
\end_inset

 is the first eigen vector, or first loading vector.
\end_layout

\end_deeper
\begin_layout Subsection
Visuliazation
\end_layout

\begin_layout Standard
PCA is to find the a unit vector 
\begin_inset Formula $u_{1}$
\end_inset

 to represent the directional line, thus data point 
\begin_inset Formula $x^{i}$
\end_inset

 can be projected onto that line 
\begin_inset Formula $x^{i}u_{1}$
\end_inset

(from 
\begin_inset Formula $N$
\end_inset

 space to
\begin_inset Formula $1-dimention$
\end_inset

 space) and to make 
\begin_inset Formula $x^{i}u_{1}$
\end_inset

 for all 
\begin_inset Formula $i$
\end_inset

 to have largest variance.
 
\end_layout

\begin_layout Standard
The first linear principal component of a set of data.
 The line minimizes the total squared distance from each point to its orthogonal
 projection onto the line.
\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename Lyx_Picture/PCA_variance.png
	lyxscale 70
	scale 70

\end_inset


\end_layout

\begin_layout Subsection
Eigen Value 
\begin_inset Formula $\lambda$
\end_inset

 and Eigen vector 
\begin_inset Formula $u$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
Au=\lambda u
\]

\end_inset


\end_layout

\begin_layout Standard
Choose
\begin_inset Formula $u_{1}…u_{k}$
\end_inset

the highest
\begin_inset Formula $k$
\end_inset

eigen-vectors (with highest eigen values).
\end_layout

\begin_layout Standard
Eigen vectors can rotate freely, thus we should not try to explain the real
 meaning of single eigen vectors.
\end_layout

\begin_layout Standard
But the subspace made by those
\begin_inset Formula $k$
\end_inset

 eigen vectors is fixed.
 This subpace is the hyperplane on which all data project)
\end_layout

\begin_layout Standard
Other dimentions did not contribute any varanation of the data.(like if all
 data are in a board, there is no need for the 3rd dimentionwhich orthogonal
 to the board.)
\end_layout

\begin_layout Standard
Thus we have a
\begin_inset Formula $k$
\end_inset

 dimentional projection.
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
y^{i}=(u_{1}^{T}x_{1},..u_{k}^{T}x_{k})
\]

\end_inset


\end_layout

\begin_layout Subsection
Use Singular value decomposition to solve eigen value
\end_layout

\begin_layout Standard
See math.lyx
\end_layout

\begin_layout Standard
Write 
\begin_inset Formula $X=USV^{T}$
\end_inset

 and thus the covarance matrix is 
\begin_inset Formula 
\[
\Sigma=X^{T}X/(n-1)=VSU^{T}USV^{T}/(n-1)=V\frac{S^{2}}{n-1}V
\]

\end_inset

 
\end_layout

\begin_layout Subsection
Application and R code
\end_layout

\begin_layout Standard
reference:http://www.r-bloggers.com/computing-and-visualizing-pca-in-r/
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout

<<>>=
\end_layout

\begin_layout Plain Layout

data(iris) 
\end_layout

\begin_layout Plain Layout

PCA = prcomp(iris[,1:4],center = T,scale.= T)
\end_layout

\begin_layout Plain Layout

# it shows the variance of each PCA.As X is standardized, so this plot is
 also the plot of variance of loadingvectors.
\end_layout

\begin_layout Plain Layout

plot(PCA)
\end_layout

\begin_layout Plain Layout

summary(PCA)
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

# to get real PCA
\end_layout

\begin_layout Plain Layout

predict(PCA, iris[,1:4]) %>% head
\end_layout

\begin_layout Plain Layout

@
\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection
Number of PCAs we need
\end_layout

\begin_layout Standard
Normally we only choose the first 
\begin_inset Formula $N$
\end_inset

 PCAs that can explain 90% or 95% ofvariance.See R code
\family typewriter
summary(PCA)
\family default
.
\end_layout

\begin_layout Subsection

\series bold
Ecnomic explanations of PCA component / Loading vector.
\end_layout

\begin_layout Standard
not that meaningful
\end_layout

\begin_layout Itemize

\series bold
ISL 390, ecnomic explanations of each PCA component.
\end_layout

\begin_layout Section
Robust PCA
\end_layout

\begin_layout Standard
Robust Principal Component Analysis? by Emmanuel J.
 Candes, Xiaodong Li, Yi Ma, and John Wright
\end_layout

\begin_layout Standard
Suppose we are given a large data matrix M, and know that it may be decomposed
 as
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
M=L_{0}+S_{0}
\]

\end_inset


\end_layout

\begin_layout Standard
where L0 has low-rank and S0 is sparse; here, both components are of arbitrary
 magnitude.
 We do not know the low-dimensional column and row space of L0, not even
 their dimension.
 Similarly, we do not know the locations of the nonzero entries of S0, not
 even how many there are.
 Can we hope to recover the low-rank and sparse components both accurately
 (perhaps even exactly) and efficiently?
\end_layout

\begin_layout Standard
Classical Principal Component Analysis (PCA) seeks the best (in an 
\begin_inset Formula $l^{2}$
\end_inset

 sense) rank-k estimate of 
\begin_inset Formula $L_{0}$
\end_inset

 by solving
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
min||M-L||\\
subject & to & rank(L)\le k
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
Traditional PCA: its 
\series bold
brittleness
\series default
 with respect to grossly corrupted observations often puts its validity
 in jeopardy { a single grossly corrupted entry in M could render the estimated
 ^L arbitrarily far from the true L0.
 Unfortunately.
\end_layout

\begin_layout Standard
The new problem we consider here can be considered as an idealized version
 of Robust PCA, in which we aim to recover a low-rank matrix 
\begin_inset Formula $L_{0}$
\end_inset

 from highly corrupted measurements M = L0+S0.
 Unlike the small noise term N0 in classical PCA, the entries in S0 can
 have arbitrarily large magnitude, and their support is assumed to be sparse
 but unknown.
\end_layout

\begin_layout Subsection
Math: Matrix Completion
\end_layout

\begin_layout Subsection
Why we care low-rank representation
\end_layout

\begin_layout Standard
image, video, multimedia processing, web relevancy data analysis, search,
 biomedical imaging and bioinformatics.
 In such application domains, data now routinely lie in thousands or even
 billions of dimensions, with a number of samples sometimes of the same
 order of magnitude.
\end_layout

\begin_layout Standard
To alleviate the curse of dimensionality and scale,2 we must leverage on
 the fact that such data have low intrinsic dimensionality, e.g.
 that they lie on some low-dimensional subspace [15], are sparse in some
 basis [13], or lie on some low-dimensional manifold [4,46].
\end_layout

\begin_layout Subsection
Application
\end_layout

\begin_layout Itemize
Video Surveillance.
 Given a sequence of surveillance video frames, we often need to identify
 activities that stand out from the background.
 If we stack the video frames as columns of a matrix M, 
\end_layout

\begin_deeper
\begin_layout Itemize
then the low-rank component L0 naturally corresponds to the stationary backgroun
d: as the backgorund is almost never change, so requires few features to
 represent it.
\end_layout

\begin_layout Itemize
the
\series bold
 sparse component 
\begin_inset Formula $S_{0}$
\end_inset


\series default
 
\series bold
captures the moving objects in the foreground.

\series default
 Most time they are 0, except when they appear.
 However, each image frame has thousands or tens of thousands of pixels,
 and each video fragment contains hundreds or thousands of frames.
 It would be impossible to decompose M in such a way unless we have a truly
 scalable solution to this problem.
 In Section 4, we will show the results of our algorithm on video decomposition.
\end_layout

\end_deeper
\begin_layout Itemize
Face detection: to remove shadows, specularities, and saturations
\end_layout

\begin_deeper
\begin_layout Itemize
a total of 58 dierent illuminations were used for each person.
\end_layout

\begin_layout Itemize
(b) Low-rank approximation ^L recovered by convex programming.
 
\end_layout

\begin_layout Itemize
(c) Sparse error ^ S corresponding to specularities in the eyes, shadows
 around the nose region, or brightness saturations on the face.
\end_layout

\end_deeper
\begin_layout Section
Principle Curves
\end_layout

\begin_layout Standard
\begin_inset Formula $X$
\end_inset

 is in dimention 
\begin_inset Formula $R^{d}$
\end_inset

, then we can define a principle curve as a mapping from 
\begin_inset Formula $\mbox{\lambda\rightarrow f:}R^{1}\rightarrow R^{D}$
\end_inset

 that satistifies 
\series bold
self-consistency
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
f(λ)=E(X|λ_{f}(X)=λ)
\]

\end_inset


\series bold
self-consistency
\series default
 means that each point of 
\begin_inset Formula $f$
\end_inset

 is the 
\series bold
average
\series default
 (under the distribution of X) of all points that 
\series bold
project
\series default
 there.
\end_layout

\begin_layout Standard
where
\end_layout

\begin_layout Itemize
\begin_inset Formula $λ_{f}(x)$
\end_inset

 define the closest point on the curve to x
\end_layout

\begin_layout Itemize
The parameter λ can be chosen, for example,
\end_layout

\begin_deeper
\begin_layout Itemize
to be arc-length along the curve from some fixed origin.
\end_layout

\begin_layout Itemize
to be simple distance between points on the curve and origin.
\end_layout

\end_deeper
\begin_layout Itemize

\series bold
Note that the inputs are scaler 
\begin_inset Formula $\lambda$
\end_inset


\end_layout

\begin_layout Subsection
Estimate
\end_layout

\begin_layout Standard
To find a principal curve f(λ) of a distribution, we consider its coordinate
 functions 
\begin_inset Formula $f(λ)=[f_{1}(λ),f_{2}(λ),...,f_{p}(λ)]$
\end_inset

 and let 
\begin_inset Formula $X_{T}=(X_{1},X_{2},...,X_{p})$
\end_inset

.
 Consider the following alternating steps: (assuming there are 2 dimentions)
\end_layout

\begin_layout Itemize
initialize: find the PCA (a plane) of the data.
\end_layout

\begin_deeper
\begin_layout Itemize
If 
\begin_inset Formula $X$
\end_inset

 contains 
\begin_inset Formula $N$
\end_inset

 observations, then you will find 
\begin_inset Formula $N$
\end_inset

 points on the PCA projected by 
\begin_inset Formula $X$
\end_inset

, call them 
\begin_inset Formula $Y$
\end_inset

.
\end_layout

\begin_layout Itemize
Distance between 
\begin_inset Formula $Y_{i}$
\end_inset

 to origin is 
\begin_inset Formula $\lambda_{i}$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Enumerate
\begin_inset Formula $\hat{f_{j}}(λ)=E(X_{j}|λ_{f}(X)=λ)$
\end_inset

, where 
\begin_inset Formula $j$
\end_inset

 means each dimention.
\end_layout

\begin_deeper
\begin_layout Itemize
Normlly you don't have a full distribution simulated points, you only get
 limited data.
 So each projected point normmly only one 
\begin_inset Formula $x$
\end_inset

 that projects there.
 So NO WAY to calculate expectation!
\end_layout

\begin_layout Itemize
So we just use a smooth line a proxy the average of distribution/population.
\end_layout

\begin_layout Itemize
In coding, using the data 
\begin_inset Formula $\lambda_{i}$
\end_inset

 (
\begin_inset Formula $R^{1}$
\end_inset

) and 
\begin_inset Formula $x_{i}$
\end_inset

 (
\begin_inset Formula $R^{N}$
\end_inset

), we estimate smooth splines for each dimention 
\begin_inset Formula $s_{j}:\lambda\rightarrow x_{j}$
\end_inset


\end_layout

\end_deeper
\begin_layout Enumerate
\begin_inset Formula $\hat{\lambda}_{f}(x)=argmin_{\lambda'}||x-\hat{f}(\lambda')||^{2}$
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Find the updated 
\begin_inset Formula $Y$
\end_inset

, those 
\begin_inset Formula $N$
\end_inset

 points on the curve that are closet to 
\begin_inset Formula $X$
\end_inset

.
\end_layout

\begin_layout Itemize
Update 
\begin_inset Formula $\lambda_{i}^{'}$
\end_inset

 using 
\begin_inset Formula $Y_{i}^{'}$
\end_inset

.
\end_layout

\begin_layout Itemize
Go back to step 1 to re-estimate the spline.
 Stop with 
\begin_inset Formula $\sum||\lambda_{i}^{'}-\lambda_{i}^{''}||^{2}$
\end_inset

 is smaller than certain criteria.
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset listings
inline false
status open

\begin_layout Plain Layout

## generate some bivariate data
\end_layout

\begin_layout Plain Layout

set.seed(42)
\end_layout

\begin_layout Plain Layout

x1 <- seq(1,10,0.3)
\end_layout

\begin_layout Plain Layout

w = .6067;
\end_layout

\begin_layout Plain Layout

a0 = 1.6345;
\end_layout

\begin_layout Plain Layout

a1 = -.6235;
\end_layout

\begin_layout Plain Layout

b1 = -1.3501;
\end_layout

\begin_layout Plain Layout

a2 = -1.1622;
\end_layout

\begin_layout Plain Layout

b2 = -.9443;
\end_layout

\begin_layout Plain Layout

x2 = a0 + a1*cos(x1*w) + b1*sin(x1*w) + a2*cos(2*x1*w) +
\end_layout

\begin_layout Plain Layout

  b2*sin(2*x1*w) + rnorm(length(x1),0,3/4)
\end_layout

\begin_layout Plain Layout

x <- scale(cbind(x1,x2))
\end_layout

\begin_layout Plain Layout

alim <- extendrange(x, f=0.1)
\end_layout

\begin_layout Plain Layout

alim_ <- range(x)
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

## plot centered data
\end_layout

\begin_layout Plain Layout

plot(x[,1], x[,2], bty='n',
\end_layout

\begin_layout Plain Layout

     xlab=expression(x[1]),
\end_layout

\begin_layout Plain Layout

     ylab=expression(x[2]),
\end_layout

\begin_layout Plain Layout

     xlim=alim, ylim=alim)
\end_layout

\begin_layout Plain Layout

legend("topleft", legend=c("Initialize"), bty="n")
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

## plot first principal component line
\end_layout

\begin_layout Plain Layout

svdx <- svd(x)
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

svdx$v[,1]
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

clip(alim_[1],alim_[2],alim_[1],alim_[2])
\end_layout

\begin_layout Plain Layout

with(svdx, abline(a=0, b=v[2,1]/v[1,1]))
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

## plot projections of each point onto line
\end_layout

\begin_layout Plain Layout

z1 <- with(svdx, x%*%v[,1]%*%t(v[,1]))
\end_layout

\begin_layout Plain Layout

segments(x0=x[,1],y0=x[,2],
\end_layout

\begin_layout Plain Layout

         x1=z1[,1],y1=z1[,2])
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

## compute initial lambda (arc-lengths associated with
\end_layout

\begin_layout Plain Layout

## orthogonal projections of data onto curve)
\end_layout

\begin_layout Plain Layout

lam <- with(svdx, as.numeric(u[,1]*d[1])) 
\end_layout

\begin_layout Plain Layout

# which is the distance between origin in q dimention to the projected point
 in q dimention.
\end_layout

\begin_layout Plain Layout

# It is the same as the distance from origin in p dimention to the point
 of the PCA plane in P dimention
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

sum(lam - with(svdx, x%*%v[,1])) # SAME
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

for(itr in 1:3) {
\end_layout

\begin_layout Plain Layout

  #
\end_layout

\begin_layout Plain Layout

  #  i = 0; itr = i + 1
\end_layout

\begin_layout Plain Layout

  
\end_layout

\begin_layout Plain Layout

  #### step (a) of iterative algorithm ####
\end_layout

\begin_layout Plain Layout

  
\end_layout

\begin_layout Plain Layout

  ## compute scatterplot smoother in either dimension
\end_layout

\begin_layout Plain Layout

  ## increase 'df' to make the curve more flexible
\end_layout

\begin_layout Plain Layout

  fit1 <- smooth.spline(x=lam, y=x[,1], df=4)
\end_layout

\begin_layout Plain Layout

  plot(x)
\end_layout

\begin_layout Plain Layout

  plot(fit1)
\end_layout

\begin_layout Plain Layout

  plot(fit2)
\end_layout

\begin_layout Plain Layout

  
\end_layout

\begin_layout Plain Layout

  
\end_layout

\begin_layout Plain Layout

  fit2 <- smooth.spline(x=lam, y=x[,2], df=4)
\end_layout

\begin_layout Plain Layout

  
\end_layout

\begin_layout Plain Layout

  ## plot data and the principal curve for a sequence of lambdas
\end_layout

\begin_layout Plain Layout

  plot(x[,1], x[,2], bty='n',
\end_layout

\begin_layout Plain Layout

       xlab=expression(x[1]),
\end_layout

\begin_layout Plain Layout

       ylab=expression(x[2]),
\end_layout

\begin_layout Plain Layout

       xlim=alim, ylim=alim)
\end_layout

\begin_layout Plain Layout

  
\end_layout

\begin_layout Plain Layout

  
\end_layout

\begin_layout Plain Layout

  legend("topleft", legend=c("Step (a)"), bty="n")
\end_layout

\begin_layout Plain Layout

  
\end_layout

\begin_layout Plain Layout

  seq_lam <- seq(min(lam),max(lam),length.out=100)
\end_layout

\begin_layout Plain Layout

  lines(predict(fit1, seq_lam)$y, predict(fit2, seq_lam)$y)
\end_layout

\begin_layout Plain Layout

  
\end_layout

\begin_layout Plain Layout

  ## show points along curve corresponding
\end_layout

\begin_layout Plain Layout

  ## to original lambdas
\end_layout

\begin_layout Plain Layout

  z1 <- cbind(predict(fit1, lam)$y, predict(fit2, lam)$y)
\end_layout

\begin_layout Plain Layout

  segments(x0=x[,1],y0=x[,2],
\end_layout

\begin_layout Plain Layout

           x1=z1[,1],y1=z1[,2])
\end_layout

\begin_layout Plain Layout

  
\end_layout

\begin_layout Plain Layout

  
\end_layout

\begin_layout Plain Layout

  #### step (b) of iterative algorithm ####
\end_layout

\begin_layout Plain Layout

  
\end_layout

\begin_layout Plain Layout

  ## recompute lambdas 
\end_layout

\begin_layout Plain Layout

  euc_dist <- function(l, x, f1, f2)
\end_layout

\begin_layout Plain Layout

    sum((c(predict(f1, l)$y, predict(f2, l)$y) - x)^2)
\end_layout

\begin_layout Plain Layout

  
\end_layout

\begin_layout Plain Layout

  
\end_layout

\begin_layout Plain Layout

  # provide f1,f2,x, search l to make the squared sum minimized.
\end_layout

\begin_layout Plain Layout

  optimize(euc_dist,
\end_layout

\begin_layout Plain Layout

           interval=extendrange(lam, f=0.50),
\end_layout

\begin_layout Plain Layout

           x=x[1,], f1=fit1, f2=fit2)
\end_layout

\begin_layout Plain Layout

  
\end_layout

\begin_layout Plain Layout

  
\end_layout

\begin_layout Plain Layout

  lam <- apply(x,1,function(x0) optimize(euc_dist,
\end_layout

\begin_layout Plain Layout

                                         interval=extendrange(lam, f=0.50),
\end_layout

\begin_layout Plain Layout

                                         x=x0, f1=fit1, f2=fit2)$minimum)
\end_layout

\begin_layout Plain Layout

  
\end_layout

\begin_layout Plain Layout

  
\end_layout

\begin_layout Plain Layout

  ## show projections associated with recomputed lambdas
\end_layout

\begin_layout Plain Layout

  plot(x[,1], x[,2], bty='n',
\end_layout

\begin_layout Plain Layout

       xlab=expression(x[1]),
\end_layout

\begin_layout Plain Layout

       ylab=expression(x[2]),
\end_layout

\begin_layout Plain Layout

       xlim=alim, ylim=alim)
\end_layout

\begin_layout Plain Layout

  
\end_layout

\begin_layout Plain Layout

  
\end_layout

\begin_layout Plain Layout

  legend("topleft", legend=c("Step (b)"), bty="n")
\end_layout

\begin_layout Plain Layout

  seq_lam <- seq(min(lam),max(lam),length.out=100)
\end_layout

\begin_layout Plain Layout

  lines(predict(fit1, seq_lam)$y, predict(fit2, seq_lam)$y)
\end_layout

\begin_layout Plain Layout

  z1 <- cbind(predict(fit1, lam)$y, predict(fit2, lam)$y)
\end_layout

\begin_layout Plain Layout

  segments(x0=x[,1],y0=x[,2],
\end_layout

\begin_layout Plain Layout

           x1=z1[,1],y1=z1[,2])
\end_layout

\begin_layout Plain Layout

}
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Section
Partial Least Squares (PLS)
\end_layout

\begin_layout Standard
P251ISL
\end_layout

\begin_layout Standard
PCA is unsupervised method.
 If your goal is to predict
\begin_inset Formula $y$
\end_inset

and you do have
\begin_inset Formula $y$
\end_inset

 data, then PLS would be better.
\end_layout

\begin_layout Enumerate
After standardizing the 
\begin_inset Formula $p$
\end_inset

 predictors, PLS computes the first direction Z1 by setting each 
\begin_inset Formula $φ_{j1}$
\end_inset

 in (6.16) equal to the coefficient from the simple linear regression of
 
\begin_inset Formula $Y$
\end_inset

 onto 
\begin_inset Formula $X_{j}$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Standard
\begin_inset Formula 
\[
Z_{1}=\sum^{p}\phi_{j1}X_{j}
\]

\end_inset


\end_layout

\end_deeper
\begin_layout Enumerate
To identify the second PLS direction we first adjust each of the variablesfor
 
\begin_inset Formula $Z_{1}$
\end_inset

, by regressing each variable on Z1 and taking residuals.These residuals
 can be interpreted as the remaining information that hasnot been explained
 by the first PLS direction.We then compute
\begin_inset Formula $Z_{1}$
\end_inset

….
\begin_inset Formula $Z_{m}$
\end_inset

using this orthogonalized data in exactly the same fashion as Z1 was computedbas
ed on the original data.
\end_layout

\begin_layout Standard
PLS is popular in the field of chemometrics, where many variables arisefrom
 digitized spectrometry signals.
\end_layout

\begin_layout Standard
In practice it often performs no better than ridge regression or PCR.While
 the supervised dimension reduction of PLS can reduce bias, it alsohas the
 potential to increase variance, so that the overall benefit ofPLS relative
 to PCR is a wash.
\end_layout

\begin_layout Section
Application
\end_layout

\begin_layout Standard
Used in :
\end_layout

\begin_layout Itemize
Impage compression 
\end_layout

\begin_layout Itemize
One of the very first techniques for face recognition was based on PCA (Eigenfac
e).
 The idea was that if you find the principal components of face images and
 project the face images onto these principal components, then things like
 facial expressions and random pixel noise will be removed and it will be
 easier to identify the face.
 This technique, however, performs quite poorly, and has been largely superseded
 by better techniques.
\end_layout

\begin_layout Subsubsection
Uniqueness of the Principal Components
\end_layout

\begin_layout Standard
Each principal component loading vector is unique, up to a sign flip.This
 means that two different software packages will yield the same principal
 component loading vectors, although the signs of those loading vectorsmay
 differ.
\end_layout

\begin_layout Subsubsection
The Proportion of Variance Explained
\end_layout

\begin_layout Standard
ISL P396
\end_layout

\begin_layout Subsection
\begin_inset Index idx
status open

\begin_layout Plain Layout
Latent Senmatic Indexing (LSI)
\end_layout

\end_inset

Application: Latent Senmatic Indexing (LSI) ???
\end_layout

\begin_layout Standard
Text data looks like
\begin_inset Formula $x^{i}={1,0,1,1,0,0..}$
\end_inset

(first have a dictionary, then see whether each word in the dictionary appears
 in the document).
\end_layout

\begin_layout Standard
Usually skip the preprocessing step in LSI
\end_layout

\begin_layout Subsubsection
LSI
\end_layout

\begin_layout Itemize
If one has 
\begin_inset Quotes eld
\end_inset

learning
\begin_inset Quotes erd
\end_inset

, the other one has 
\begin_inset Quotes eld
\end_inset

study
\begin_inset Quotes erd
\end_inset

, then no similarity (vectors are orthogonal).
\end_layout

\begin_layout Itemize
PCA will project thw two vectors into one direction, thus they are the same
 for certain versions
\end_layout

\begin_layout Subsection
Use PCA
\end_layout

\begin_layout Standard
Before use PCA, see wether we can train the original data directly, rather
 than use PCA to compress it.
\end_layout

\begin_layout Enumerate

\series bold
Principal Components Regression (PCR)
\begin_inset Index idx
status open

\begin_layout Plain Layout
Principal Components Regression (PCR)
\end_layout

\end_inset

: Dimention reduction in supervised learning: for example, regression on
 each PCA, rather than the original features.
\end_layout

\begin_layout Enumerate
Visualization.
\end_layout

\begin_layout Enumerate
Compression data size.Then quicken the learning algorithm much faster.
\end_layout

\begin_layout Enumerate
Fewer features, thus less prone to overfitting.
\end_layout

\begin_layout Enumerate
Anomaly Detection (a point far from the subspace than other point)
\end_layout

\begin_layout Enumerate
Matching/ distance calculation
\end_layout

\begin_deeper
\begin_layout Enumerate
Face detaction, 1000..faces, each face has 1000..pixels (dimentions).But you
 know only some key dimentions correspond to the key features offaces.
\end_layout

\end_deeper
\begin_layout Subsection
Mahalanobis distance
\end_layout

\begin_layout Standard
The Mahalanobis distance is a measure of the distance between a point Pand
 a distribution D, introduced by P.C.Mahalanobis in 1936.[1]
\end_layout

\begin_layout Standard
It is a multi-dimensional generalization of the idea of measuring how manystanda
rd deviations away P is from the mean of D.
\end_layout

\begin_layout Standard
This distance is zero if P is at the mean of D, and grows as P moves awayfrom
 the mean: along each principal component axis, it measures the numberof
 standard deviations from P to the mean of D.
\end_layout

\begin_layout Standard
If each of these axes is rescaled to have unit variance, then Mahalanobisdistanc
e corresponds to standard Euclidean distance in the transformedspace.Mahalanobis
 distance is thus unitless and scale-invariant, and takes intoaccount the
 correlations of the data set.
\end_layout

\begin_layout Subsubsection
Defintion
\end_layout

\begin_layout Standard
The Mahalanobis distance of an observation
\begin_inset Formula $x=(x_{1},x_{2},x_{3},\dots,x_{N})^{T}$
\end_inset

from a set of observations with mean
\begin_inset Formula $\mu=(\mu_{1},\mu_{2},\mu_{3},\dots,\mu_{N})^{T}$
\end_inset

and covariance matrix S is defined as:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
D_{M}(x)=\sqrt{(x-\mu)^{T}S^{-1}(x-\mu)}
\]

\end_inset


\end_layout

\begin_layout Section
Kernel Principal Components
\end_layout

\begin_layout Itemize
wiki
\end_layout

\begin_layout Itemize
http://www.ics.uci.edu/~welling/classnotes/papers_class/Kernel-PCA.pdf
\end_layout

\begin_layout Subsection
Spectral Clustering and Kernel Principal Components
\end_layout

\begin_layout Standard
P560 ESL.Traditional clustering methods like K-means use a spherical or elliptica
l metric to group data points.
 Hence they will NOT work well when the clusters are non-convex, such asthe
 concentric circles in the top left panel of
\series bold
 Figure 14.29
\series default
.
\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename Lyx_Picture/Spectral Clustering.png

\end_inset


\end_layout

\begin_layout Subsection
kernel principal components,
\end_layout

\begin_layout Standard
Spectral clustering is related to kernel principal components, a non-linear
 version of linear principal components.
 Standard linear principal components (PCA) are obtained from the eigenv
 ectorsof the covariance matrix, and give directions in which the data have
 maxima lvariance.
 Kernel PCA (Sch¨olkopf et al., 1999) expand the scope of PCA, mimicking
 what we would obtain if we were to expand the features by non-linear transforma
tions, and then apply PCA in this transformed feature space.
\end_layout

\begin_layout Subsection
Initiative: non-linear-separatable data
\end_layout

\begin_layout Standard
To understand the utility of kernel PCA, particularly for clustering, observetha
t, while N points cannot in general be linearly separated in d<N dimensions,they
 can almost always be linearly separated in
\begin_inset Formula $d\geq N$
\end_inset

 dimensions.
\end_layout

\begin_layout Subsection
The feature space
\begin_inset Formula $\Phi$
\end_inset

(
\begin_inset Formula $N-dimention$
\end_inset

) to Linear Separate Data
\end_layout

\begin_layout Standard
That is, given N points, 
\begin_inset Formula $f{x}_{i}$
\end_inset

, if we map them to an N-dimensional space with 
\begin_inset Formula $\Phi(\mathbf{x}_{i})$
\end_inset

 where
\begin_inset Formula 
\[
\Phi(x):\mathbb{\mbox{\,}R}^{d}\to\mathbb{R}^{N}
\]

\end_inset

, where it is easy to construct a hyperplane that divides the points into
 arbitrary clusters.Of course, this
\begin_inset Formula $\Phi$
\end_inset

creates linearly independent vectors, so there is no covariance on which
 to perform eigen-decomposition explicitly as we would in linear PCA.
\end_layout

\begin_layout Subsection
Kernal: Avoid to work on feature space 
\begin_inset Formula $\Phi$
\end_inset


\end_layout

\begin_layout Standard
Instead, in kernel PCA, a non-trivial, arbitrary 
\begin_inset Formula $\Phi$
\end_inset

 function is ‘chosen’ that is never calculated explicitly, allowing the
 possibility to use very-high-dimensional 
\begin_inset Formula $\Phi$
\end_inset

‘s if we never have to actually evaluate the data in that space.
 Since we generally try to avoid working in the
\begin_inset Formula $\Phi$
\end_inset

-space, which we will call the ‘feature space’, we can create the N-by-N
 kernel
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
K=k(\mathbf{x},\mathbf{y})=(\Phi(\mathbf{x}),\Phi(\mathbf{y}))=\Phi(\mathbf{x})^{T}\Phi(\mathbf{y})
\]

\end_inset

which represents the inner product space (see Gramian matrix) of the otherwise
 intractable (hard to control or deal with.
 "intractable economic problems") feature space.
\end_layout

\begin_layout Standard
(Note that for linear PCA, eventially we need to solve
\begin_inset Formula $\sum=\frac{1}{m}\sum_{i=1}^{m}x^{i}(x^{i})^{T}$
\end_inset

, so in feature space 
\begin_inset Formula $\Phi$
\end_inset

 we don't need to care any calculation in feature space 
\begin_inset Formula $\Phi$
\end_inset

, we just need to create a way to easily calculate 
\begin_inset Formula $\Phi(\mathbf{x})^{T}\Phi(\mathbf{y})$
\end_inset

, that is why we invented kernal_
\end_layout

\begin_layout Standard
The dual form that arises in the creation of a kernel allows us to mathematicall
y formulate a version of PCA in which we never actually solve the eigenvectors
 and eigenvalues of the covariance matrix in the
\begin_inset Formula $\Phi(\mathbf{x}$
\end_inset

)-space (see Kernel trick).
\end_layout

\begin_layout Standard
The N-elements in each column of K represent the dot product of one point
 of the transformed data with respect to all the transformed points (N points).So
me well-known kernels are shown in the example below.
\end_layout

\begin_layout Subsection
Write 
\begin_inset Formula $\Sigma\mu_{a}=\lambda\mu_{a}$
\end_inset

 with Kernal
\end_layout

\begin_layout Standard
Rewrite 
\begin_inset Formula $\lambda\mu_{a}=\Sigma\mu_{a}$
\end_inset

 with 
\begin_inset Formula $\mu_{a}=\sum\alpha_{i}^{a}x_{i}$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\frac{1}{N}X^{T}X\sum_{i}^{N}\alpha_{i}^{a}x_{i}=\lambda\sum\alpha_{i}^{a}x_{i}
\]

\end_inset


\end_layout

\begin_layout Itemize
Define kernal as 
\begin_inset Formula $K=N\times N=\Phi(X)\times\Phi^{T}(X)$
\end_inset

, here in the simple cast 
\begin_inset Formula $K=XX^{T}$
\end_inset


\end_layout

\begin_layout Itemize
To write eveything in terms of 
\begin_inset Formula $K$
\end_inset

, 
\end_layout

\begin_deeper
\begin_layout Itemize
left multiply 
\begin_inset Formula $x_{i}^{T}$
\end_inset

 in both sides (起到过河拆桥作用，到最后会消掉).
 The left side becomes 
\begin_inset Formula 
\begin{eqnarray*}
\frac{1}{N}XX^{T}X\sum_{i}^{N}\alpha_{i}^{a}x_{i} & = & \frac{1}{N}KX(X^{T}\mathbf{a})\\
 & = & \frac{1}{N}KK\mathbf{a}\\
 & = & \frac{1}{N}K^{2}\mathbf{a}\mbox{ (K is a sysmetrical metrics, so \ensuremath{K=K^{T},}and \ensuremath{KK=K^{2}}}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Itemize
The right side becomes
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
X\lambda\sum_{j}^{N}\alpha_{j}^{a}x_{j} & = & \lambda XX^{T}\mathbf{a}\\
 & = & \lambda K
\end{eqnarray*}

\end_inset


\end_layout

\end_deeper
\begin_layout Enumerate
Combine the right and left side and we have 
\begin_inset Formula 
\[
Ka=N\lambda a
\]

\end_inset


\end_layout

\begin_layout Enumerate
The normality condition 
\begin_inset Formula $\mu^{T}\mu=1$
\end_inset

 thus becomes (writing with Kernal and 
\begin_inset Formula $a$
\end_inset

)
\end_layout

\begin_deeper
\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
1 & = & \sum_{i}a_{i}\Phi^{T}(x_{i})\sum_{j}a_{j}\Phi(x_{j})\\
 & = & (\Phi(X)^{T}\mathbf{a})^{T}\Phi(X)^{T}\mathbf{a}\\
 & = & \mathbf{a}^{T}K\mathbf{a}
\end{eqnarray*}

\end_inset


\end_layout

\end_deeper
\begin_layout Enumerate
Solve the true loading vector 
\begin_inset Formula $\mu$
\end_inset

 from 
\begin_inset Formula $a$
\end_inset

: So we can use the formula above to first get 
\begin_inset Formula $a$
\end_inset

, then according to 
\begin_inset Formula $a_{i}^{a}=\sum\frac{\Phi(x_{i})\mu_{a}}{N\lambda_{a}}$
\end_inset

 and, we can solve the loading vector 
\begin_inset Formula $\mu$
\end_inset

 from 
\begin_inset Formula $a$
\end_inset

.
\end_layout

\begin_layout Subsection

\series bold
Projections with Kernal 
\end_layout

\begin_layout Enumerate

\series bold
For a new point 
\begin_inset Formula $x_{i}$
\end_inset

, its projection onto the principal component is
\end_layout

\begin_deeper
\begin_layout Enumerate
\begin_inset Formula 
\begin{eqnarray*}
\phi(x_{i})^{T}v_{j} & = & \phi(x_{i})^{T}\sum_{j}^{N}a_{j}\phi(x_{i})\\
 & = & \phi(x_{i})^{T}\Phi(X)^{T}\mathbf{a}\\
 & = & K_{i}\mbox{\textbf{a} (\ensuremath{K_{i}=1\times N})}
\end{eqnarray*}

\end_inset


\end_layout

\end_deeper
\begin_layout Subsection
Formulas of Kernal
\end_layout

\begin_layout Labeling
\labelwidthstring 00.00.0000
\begin_inset Formula $K=\Phi(X)\Phi(X)^{T}=N\times N$
\end_inset


\end_layout

\begin_layout Labeling
\labelwidthstring 00.00.0000
\begin_inset Formula $K_{ij}=\Phi(x_{i})^{T}\Phi(x_{j})=scaler$
\end_inset


\end_layout

\begin_layout Labeling
\labelwidthstring 00.00.0000
\begin_inset Formula $K\mathbf{1_{n\times1}}=\sum_{j}K[\mbox{ },\mbox{ }j]=N\times1$
\end_inset

 (sum up all the columns for each row/ rowsum())
\end_layout

\begin_deeper
\begin_layout Standard
\begin_inset Formula $K\mathbf{1_{N\times1}}\mathbf{1_{1\times N}}=\sum_{i}\sum_{j}K[i\mbox{ },\mbox{ }j]=scaler$
\end_inset

 
\end_layout

\end_deeper
\begin_layout Labeling
\labelwidthstring 00.00.0000
\begin_inset Formula $K_{i}=\Phi(x_{i})^{T}\Phi(X)^{T}=1\times N$
\end_inset

, this is one new observation multiply with existing data 
\begin_inset Formula $X$
\end_inset

 .
\end_layout

\begin_layout Subsection
Popular Kernals
\end_layout

\begin_layout Standard
Radia/Gaussianl Kernal to deal with Figure 14.29 data (see ESL)
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
K(x,x\text{′})=exp(\text{−∥}x\text{−}x\text{'∥}2/c)
\]

\end_inset


\end_layout

\begin_layout Standard
then the kernel matrix 
\begin_inset Formula $K$
\end_inset

 has the same form as the similarity matrix 
\begin_inset Formula $S$
\end_inset

 in spectral clustering.
\end_layout

\begin_layout Subsection
Centered data in Kernal PCA
\end_layout

\begin_layout Standard
See Kernel Principal Components Analysis: MaxWelling
\end_layout

\begin_layout Standard
It is in fact very difficult to explicitly center the data in feature space.
 But, we know that the final algorithm only depends on the kernel matrix,
 so if we can center the kernel matrix, we are done as well.
\end_layout

\begin_layout Standard
Centered data in feature space: 
\begin_inset Formula $\tilde{\Phi}_{i}=\Phi_{i}-\frac{1}{N}\sum_{k}\Phi_{k}$
\end_inset


\end_layout

\begin_layout Standard
Centered Kernal:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
\tilde{K}_{ij} & = & (\Phi_{i}-\frac{1}{N}\sum_{k}\Phi_{k})^{T}(\Phi_{j}-\frac{1}{N}\sum_{k}\Phi_{k})\\
 & = & K_{ij}-\frac{1}{N}K_{i}\mathbf{1}_{N\times1}-\frac{1}{N}\mathbf{1_{1\times N}}K_{j}+\frac{1}{N^{2}}K\mathbf{1}_{N\times1}\mathbf{1_{1\times N}}
\end{eqnarray*}

\end_inset

where
\begin_inset Formula $\mathbf{1_{N}}$
\end_inset

 denotes a N-by-N matrix for which each element takes value 1/N.
 We use 
\begin_inset Formula $\tilde{K_{ij}}$
\end_inset

 to perform the kernel PCA algorithm described above.
\end_layout

\begin_layout Section
Sparsed PCA
\end_layout

\begin_layout Standard
However, PCA also has an obvious drawback, that is, each PC is a linear
 combination of all p variables and the loadings are typically nonzero.
 This makes it often difficult to interpret the derived PCs
\end_layout

\begin_layout Itemize
lasso (L1) penalties: (The SCoTLASS procedure of Joliffe et al.)
\end_layout

\begin_deeper
\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
max(XV)^{T} & subject\\
 &  & \sum_{j}^{P}|v_{j}|\le t\\
 &  & V^{T}V=1
\end{eqnarray*}

\end_inset


\end_layout

\end_deeper
\begin_layout Subsection
Applications
\end_layout

\begin_layout Standard
Financial Data Analysis: Suppose ordinary PCA is applied to a dataset where
 each input variable represents a different asset, it may generate principal
 components that are weighted combination of all the assets.
 In contrast, sparse PCA would produce principal components that are weighted
 combination of only a few input assets, so one can easily interpret its
 meaning.
 Furthermore, if one uses a trading strategy based on these principal components
, fewer assets imply less transaction costs.
\end_layout

\begin_layout Standard
Biology: Consider a dataset where each input variable corresponds to a specific
 gene.
 Sparse PCA can produce a principal component that involves only a few genes,
 so researchers can focus on these specific genes for further analysis.
\end_layout

\begin_layout Section
Independent Component Analysis (ICA)
\end_layout

\begin_layout Standard
Factor analysis models are typically wed to Gaussian distributions, which
 has to some extent hindered their usefulness.
 More recently, independent component analysis has emerged as a strong competito
r to factor analysis, and as we will see, relies on the non-Gaussian nature
 of the underlying sources for its success.
\end_layout

\begin_layout Standard
Not popular anymore.
\end_layout

\begin_layout Standard
Lecture 15.40m
\end_layout

\begin_layout Standard
ESL P573
\end_layout

\begin_layout Itemize
In PCA, you try to find the component that can explain the variance themost.
\end_layout

\begin_layout Itemize
ICA trys to find the independent component variation of data.
\end_layout

\begin_layout Itemize
http://cis.legacy.ics.tkk.fi/aapo/papers/IJCNN99_tutorialweb/IJCNN99_tutorial3.html
\end_layout

\begin_layout Subsection
Applications
\end_layout

\begin_layout Standard
Financial Data Analysis: Suppose ordinary PCA is applied to a dataset where
 each input variable represents a different asset, it may generate principal
 components that are weighted combination of all the assets.
 In contrast, sparse PCA would produce principal components that are weighted
 combination of only a few input assets, so one can easily interpret its
 meaning.
 Furthermore, if one uses a trading strategy based on these principal components
, fewer assets imply less transaction costs.
\end_layout

\begin_layout Standard
Biology: Consider a dataset where each input variable corresponds to a specific
 gene.
 Sparse PCA can produce a principal component that involves only a few genes,
 so researchers can focus on these specific genes for further analysis.
\end_layout

\begin_layout Subsection
Setting
\end_layout

\begin_layout Standard
Different microphones recored diferent speakers together, you want to retractthe
 clear vocie of each speaker.
\end_layout

\begin_layout Itemize
\begin_inset Formula $S_{j}^{i}$
\end_inset

 is the 
\begin_inset Formula $j^{th}$
\end_inset

 source/
\begin_inset Formula $j^{th}$
\end_inset

 speaker: it can be a sound wave, or a time serires.
\end_layout

\begin_deeper
\begin_layout Itemize
Assume there are 
\begin_inset Formula $N$
\end_inset

 speakers.
\end_layout

\begin_layout Itemize
Original source
\begin_inset Formula $S\in R^{n}$
\end_inset

.(there are
\begin_inset Formula $n$
\end_inset

speakers).
 These are the un-observed data.
\end_layout

\end_deeper
\begin_layout Itemize
\begin_inset Formula $i$
\end_inset

 is the time point of that wave.
\end_layout

\begin_layout Itemize
Mirco
\begin_inset Formula $x^{i}$
\end_inset

:
\begin_inset Formula $x^{i}\in R^{n}$
\end_inset

.
 Each microphone recoreds a linear combination of
\begin_inset Formula $S_{j}$
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $X=T\times N$
\end_inset

 is the observed data.
 There are 
\begin_inset Formula $N$
\end_inset

 microphones
\end_layout

\end_deeper
\begin_layout Itemize
\begin_inset Formula $A$
\end_inset

 is an unknown square matrix called the mixing matrix: 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
x_{j}^{i}=\sum_{k}A_{jk}S_{k}^{i}
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
X=AS
\]

\end_inset


\end_layout

\begin_layout Itemize
Training set is 
\begin_inset Formula ${x^{1}…x^{n}}$
\end_inset


\end_layout

\begin_layout Itemize
Mircophones number must be larger or equal to speakers number!
\end_layout

\begin_layout Paragraph*
Goal
\end_layout

\begin_layout Itemize
Find 
\begin_inset Formula $W=A^{-1}$
\end_inset

 
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $w_{i}$
\end_inset

 is the 
\begin_inset Formula $i^{th}$
\end_inset

 row ow 
\begin_inset Formula $W$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Itemize
so that the 
\begin_inset Formula $j^{th}$
\end_inset

 source can be recovered by computing 
\begin_inset Formula $s_{j}^{i}=w_{j}^{T}x^{i}$
\end_inset


\end_layout

\begin_layout Paragraph*
Limitation
\end_layout

\begin_layout Itemize
You can find the index of speaker
\end_layout

\begin_layout Itemize
You can find the sign (+, -) of 
\begin_inset Formula $x_{j}^{i}$
\end_inset

, but you can play the extracted sound
\begin_inset Formula $x_{j}^{i}$
\end_inset

to change the sign
\end_layout

\begin_layout Itemize
These two ambiguities are because
\begin_inset Formula $S_{j}^{i}$
\end_inset

is non-Gaussin
\end_layout

\begin_layout Subsection
Why Gaussina Variables are Not Allowed 
\end_layout

\begin_layout Standard
To conduct ICA, at least one indepdent component cannot be Gaussian.
\end_layout

\begin_layout Standard
Assume there are two microphones, 
\begin_inset Formula $s_{1},s_{2}$
\end_inset

, both follow standardized Gaussian, then their joint distribution countor
 will be symmetrical circles.
\end_layout

\begin_layout Itemize
ref_ICA_tutorial.pdf:
\end_layout

\begin_deeper
\begin_layout Standard
This distribution is illustrated in Fig.
 7.
 The Figure shows that the density is completely symmetric.
 Therefore, it does not contain any information on the directions of the
 columns of the mixing matrix 
\begin_inset Formula ${\bf A}$
\end_inset

.
 This is why 
\begin_inset Formula ${\bf A}$
\end_inset

 cannot be estimated.
 
\end_layout

\begin_layout Standard
More rigorously, one can prove that the distribution of any orthogonal transform
ation of the gaussian (
\begin_inset Formula $x_{1},x_{2}$
\end_inset

) has exactly the same distribution as (x1,x2), and that x1 and x2 are independe
nt.
 Thus, in the case of gaussian variables, we can only estimate the ICA model
 up to an orthogonal transformation.
 In other words, the matrix 
\begin_inset Formula ${\bf A}$
\end_inset

 is not identifiable for gaussian independent components.
 (Actually, if just one of the independent components is gaussian, the ICA
 model can still be estimated.) 
\end_layout

\end_deeper
\begin_layout Itemize
Andrew Ng notes (Notes 11)
\end_layout

\begin_deeper
\begin_layout Itemize
Now, suppose we observe some x = As, where A is our mixing matrix.
 The distribution of x will also be Gaussian, with zero mean and covariance
 
\begin_inset Formula $E[xx^{T}]=E[Ass^{T}A{}^{T}]=AA^{T}$
\end_inset


\end_layout

\begin_layout Itemize
Now, let R be an arbitrary orthogonal (less formally, a rotation/reflection)
 matrix, so that 
\begin_inset Formula $RR^{T}=R^{T}R=I$
\end_inset

, and let 
\begin_inset Formula $A′=AR$
\end_inset

.
 Then if the data had been mixed according to 
\begin_inset Formula $A′$
\end_inset

 instead of A, we would have instead observed 
\begin_inset Formula $x′=A′s$
\end_inset

.
 The distribution of 
\begin_inset Formula $x′$
\end_inset

 is also Gaussian, with zero mean and covariance 
\begin_inset Formula $E[x′(x′)^{T}]=E[A′ss^{T}(A′)^{T}]=E[ARss^{T}(AR)^{T}]=AA^{T}$
\end_inset

 .
\end_layout

\begin_layout Itemize
Hence, whether the mixing matrix is A or A′, we would observe data from
 a N(0,
\begin_inset Formula $AA^{T}$
\end_inset

 ) distribution.
 Thus, there is no way to tell if the sources were mixed using A and A′.
 So, there is an arbitrary rotational component in the mixing matrix that
 cannot be determined from the data, and we cannot recover the original
 sources.
 Our argument above was based on the fact that the multivariate standard
 normal distribution is rotationally symmetric.
 Despite the bleak picture that this paints for ICA on Gaussian data, it
 turns out that, so long as the data is not Gaussian, it is possible, given
 enough data, to recover the n independent sources.
\end_layout

\end_deeper
\begin_layout Subsection
Dimention of Matrix
\end_layout

\begin_layout Itemize
\begin_inset Formula $w^{i}=n\times1$
\end_inset

 is the ith row of matrix 
\begin_inset Formula $W=N\times M$
\end_inset

 (
\begin_inset Formula $WX=S$
\end_inset

)
\end_layout

\begin_deeper
\begin_layout Standard
\begin_inset Formula 
\[
W=A^{-1}=\left[--\begin{array}[t]{c}
w_{1}^{T}\\
w_{2}^{T}\\
…\\
w_{n}^{T}
\end{array}--\right]
\]

\end_inset


\end_layout

\end_deeper
\begin_layout Itemize
\begin_inset Formula $x^{i}=n\times1$
\end_inset

, where 
\begin_inset Formula $i=\{1....M\}$
\end_inset

(
\begin_inset Formula $M$
\end_inset

 is the sample size) (
\begin_inset Formula $N$
\end_inset

 is the number of microphones.)
\end_layout

\begin_layout Itemize
\begin_inset Formula $S=N\times M=WX=(N\times N)\times(N\times M)$
\end_inset

 
\end_layout

\begin_layout Itemize
At time 
\begin_inset Formula $i$
\end_inset

, the 
\begin_inset Formula $j^{th}$
\end_inset

 source can be recovered by computing: 
\begin_inset Formula $s_{j}^{i}=1\times1=w_{j}^{T}x^{i}=(1\times n)\times(n\times1)$
\end_inset

.
 To recover all sources 
\begin_inset Formula $s^{i}=n\times1=Wx^{i}$
\end_inset

.
\end_layout

\begin_layout Subsection
Algorithm: MLE
\end_layout

\begin_layout Itemize
For one training sample: 
\begin_inset Formula $P(x^{i})=\left[\prod_{j}^{n}P_{s}(w_{j}^{T}x^{i})\right]\cdot|W|$
\end_inset

, then we need to choose a density function for 
\begin_inset Formula $P_{s}$
\end_inset


\end_layout

\begin_layout Standard
Just be convinient choose signoid function as CDF of
\begin_inset Formula $s$
\end_inset

:
\begin_inset Formula $g(s)=\frac{1}{1+e^{-s}}$
\end_inset

(Signoid function has property from 0 to 1.) Thus 
\begin_inset Formula $pdf$
\end_inset

 of 
\begin_inset Formula $s$
\end_inset

 is 
\begin_inset Formula $P(s)=g'(s)=(1+e^{-s})^{-2}e^{-s}$
\end_inset

.
 
\end_layout

\begin_layout Standard
The log likelihood for training set {
\begin_inset Formula $x^{i};i=1,...,m$
\end_inset

}:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
l(w) & = & log\left(\prod_{i}^{m}[\prod_{j}^{n}g'(w_{j}^{T}x^{i})\cdot|W|]\right)\\
 & = & \sum_{i=1}^{m}\sum_{j=1}^{n}\mbox{log}g'(w_{j}^{T}x^{i})+m\times log|W|
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
Use Stochastic Gradient Assent (assuming 
\begin_inset Formula $g(s)=\frac{1}{1+e^{-s}}$
\end_inset

):
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
W: & = & W+\alpha\left(\left[\begin{array}{c}
1-2g(w_{1}^{T}x^{i})\\
1-2g(w_{2}^{T}x^{i})\\
...\\
1-2g(w_{n}^{T}x^{i})
\end{array}\right](x^{i})^{T}+(W^{T})^{-1}\right)\\
 & = & W+\alpha\left[I-2g(Wx^{i})\right](x^{i})^{T}+(W^{T})^{-1}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Subsection
Math Techniques
\end_layout

\begin_layout Standard
Andrew Ng notes (Notes 11)
\end_layout

\begin_layout Itemize
\begin_inset Formula $\nabla_{W}|W|=|W|(W^{-1})^{T}$
\end_inset


\end_layout

\begin_layout Itemize
Given 
\begin_inset Formula $x=As$
\end_inset

 (
\begin_inset Formula $A^{-1}=W$
\end_inset

) and 
\begin_inset Formula $p_{s}(s)$
\end_inset

, we can derive: 
\begin_inset Formula $p_{x}(x)=p_{s}(Wx)·|W|=p_{s}(s)\cdot|W|$
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
For example, let s ∼ Uniform[0, 1], so that s’s density is ps(s) = 1{0 ≤
 s ≤ 1}.
 Now, let A = 2, so that x = 2s.
 Clearly, x is distributed uniformly in the interval [0, 2].
 Thus, its density is given by px(x) = (0.5)1{0 ≤ x ≤ 2}.
 This does not equal ps(Wx), where W = 0.5 = A−1.
 Instead, the correct formula is px(x) = ps(Wx)|W|.
\end_layout

\end_deeper
\begin_layout Subsection
Algoorithm: Information Theory
\end_layout

\begin_layout Standard
See the Information Theory Section.
\end_layout

\begin_layout Subsection
Preprocessing Data for ICA
\end_layout

\begin_layout Standard
ref_ICA_tutorial.pdf:
\end_layout

\begin_layout Enumerate
Center the data: 
\begin_inset Formula $X-E(X)$
\end_inset


\end_layout

\begin_layout Enumerate
Do the whiltening trasnformation: 
\begin_inset Formula $\tilde{x}=ED^{-1/2}E^{T}x=\tilde{A}x$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Enumerate

\series bold
Thus 
\begin_inset Formula $A$
\end_inset

 where 
\begin_inset Formula $s=Ax$
\end_inset

 will become 
\begin_inset Formula $s=A^{*}\tilde{A}x=A^{*}\tilde{x}$
\end_inset

, so you just need to use ICA to look for 
\begin_inset Formula $A^{*}$
\end_inset


\end_layout

\end_deeper
\begin_layout Enumerate
It may also be quite useful to reduce the dimension of the data at the same
 time as we do the whitening.
 Then we look at the eigenvalues 
\begin_inset Formula $d_{j}$
\end_inset

 of 
\begin_inset Formula $Ef(xx^{T})$
\end_inset

 and discard those that are too small as is often done in the statistical
 technique of principal component analysis.
 This has often the eect of reducing noise.
 Moreover dimension reduction prevents overlearning which can sometimes
 be observed in ICA.
\end_layout

\begin_layout Subsection
Whiltening trasnformation data
\end_layout

\begin_layout Standard
ref_ICA_tutorial.pdf:
\end_layout

\begin_layout Itemize
Goal: we transform the observed vector 
\begin_inset Formula $x$
\end_inset

 linearly so that we obtain a new vector 
\begin_inset Formula $\tilde{x}=\tilde{A}x$
\end_inset

 which is 
\series bold
white
\series default
 i.e, its components are uncorrelated and their variances equal unity: 
\begin_inset Formula $E(\tilde{x}\tilde{x}^{T})=I$
\end_inset


\end_layout

\begin_layout Itemize
Process: Whitening can now be done by
\end_layout

\begin_deeper
\begin_layout Standard
\begin_inset Formula 
\[
\tilde{x}=ED^{-1/2}E^{T}x
\]

\end_inset

where 
\begin_inset Formula $E$
\end_inset

 and 
\begin_inset Formula $D$
\end_inset

 are comming from the eigen value decomposition of the covariance matrix
 
\begin_inset Formula $E\{xx^{T}\}=EDE^{T}$
\end_inset

, where E is the orthogonal matrix of eigenvectors of 
\begin_inset Formula $E\{xx^{T}\}$
\end_inset

 and D is the diagonal matrix of its eigenvalues
\end_layout

\end_deeper
\begin_layout Itemize
Result: see figure 6 and figure 10 of ref_ICA_tutorial.pdf.
\end_layout

\begin_layout Subsection
Use of ICA
\end_layout

\begin_layout Itemize
Once you find ICs of data, you can subtract the certain IC from the data
 to get a much cleaner data or can help you see the data from a different
 view.
\end_layout

\begin_layout Part
Semi-supervised Learning
\end_layout

\begin_layout Standard
A lot of unlabeled data is plentiful and cheap, eg.
 documents off the web speech samples images and video But labeling can
 be expensive.
\end_layout

\begin_layout Standard
Semisupervised and active learning is to use both unsupervised data and
 supervised data together.
\end_layout

\begin_layout Standard
Semi-supervised learning is a class of supervised learning tasks and techniques
 that also make use of unlabeled data for training – typically a small amount
 of labeled data with a large amount of unlabeled data.
\end_layout

\begin_layout Subsection
Goal
\end_layout

\begin_layout Standard
Use both Labeled data and Unlabled data to predict the labels 
\end_layout

\begin_layout Standard
In a more mathematical formulation, one could say that the knowledge on
 p(x) that one gains through the unlabeled data has to carry information
 that is useful in the inference of p(y|x).
 
\end_layout

\begin_layout Subsection
Assumptions used in Semi-Supervised learning
\end_layout

\begin_layout Standard
In order to make any use of unlabeled data, we must assume some structure
 to the underlying distribution of data.
 Semi-supervised learning algorithms make use of at least one of the following
 assumptions.
\end_layout

\begin_layout Enumerate
Smoothness assumption: Points which are close to each other are more likely
 to share a label.
 
\end_layout

\begin_deeper
\begin_layout Enumerate
This is also generally assumed in supervised learning and yields a preference
 for geometrically simple decision boundaries.
\end_layout

\begin_layout Enumerate
In the case of semi-supervised learning, the smoothness assumption additionally
 yields a preference for decision boundaries in low-density regions, so
 that there are fewer points close to each other but in different classes.
\end_layout

\end_deeper
\begin_layout Enumerate
Cluster assumption: The data tend to form discrete clusters, and points
 in the same cluster are more likely to share a label (although data sharing
 a label may be spread across multiple clusters).
 
\end_layout

\begin_deeper
\begin_layout Enumerate
This is a special case of the smoothness assumption and gives rise to feature
 learning with clustering algorithms.
\end_layout

\end_deeper
\begin_layout Enumerate
Manifold assumption: The data lie approximately on a manifold of much lower
 dimension than the input space.
 In this case we can attempt to learn the manifold using both the labeled
 and unlabeled data to avoid the curse of dimensionality.
 Then learning can proceed using distances and densities defined on the
 manifold.
 
\end_layout

\begin_deeper
\begin_layout Enumerate
The manifold assumption is practical when high-dimensional data are being
 generated by some process that may be hard to model directly, but which
 only has a few degrees of freedom.
 For instance, human voice is controlled by a few vocal folds,[2] and images
 of various facial expressions are controlled by a few muscles.
 We would like in these cases to use distances and smoothness in the natural
 space of the generating problem, rather than in the space of all possible
 acoustic waves or images respectively.
\end_layout

\end_deeper
\begin_layout Section
Mixture Model
\end_layout

\begin_layout Standard
Semi-supervised learning in mixture models amounts to finding the MLE of
 (3.13).
 The only difference between supervised and semi-supervised learning (for
 mixture models) is the objective function being maximized.
\end_layout

\begin_layout Subsection
Likelihood
\end_layout

\begin_layout Standard
For semi-supervised learning: their likelihood is:
\end_layout

\begin_layout Standard
P25 text_Intro_SSL.pdf
\end_layout

\begin_layout Standard
Based on data 
\begin_inset Formula $D=\{(x_{1},y_{1})....,(x_{l},y_{l}),x_{l+1}....,x_{l+u}\}$
\end_inset

 where 
\begin_inset Formula $l+1$
\end_inset

 to 
\begin_inset Formula $l+u$
\end_inset

 observations didn't have data.
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray}
\mbox{log}p(D|\theta) & = & \mbox{log}\left(\prod_{i=1}^{l}p(x_{i},y_{i}|\theta)\prod_{i=l+1}^{l+\mu}p(x_{i}|\theta)\right)\nonumber \\
 & = & \sum_{i=1}^{l}\mbox{log}p(y_{i}|\theta)p(x_{i}|y_{i},\theta)+\sum_{i=l+1}^{l+u}p(x_{i}|\theta)\label{eq:MLE_SSL}
\end{eqnarray}

\end_inset


\end_layout

\begin_layout Standard
The essential difference between this semi-supervised log likelihood (3.13)
 and the previous supervised log likelihood (3.6) is the second term for
 unlabeled instances.We call 
\begin_inset Formula $P(x|θ)$
\end_inset

 the marginal probability, which is defined as marginalizing out 
\begin_inset Formula $y$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
P(x|\theta)=\sum_{y=1}^{C}P(x,y|\theta)P(x|y,\theta)
\]

\end_inset


\end_layout

\begin_layout Standard
where 
\begin_inset Formula $C$
\end_inset

 is the number of possible classes of label 
\begin_inset Formula $y$
\end_inset

.
\end_layout

\begin_layout Subsection
Example: a 2-class GMM with Hidden Variables
\end_layout

\begin_layout Standard
P26 Intro_SSL
\end_layout

\begin_layout Standard
E-step:
\end_layout

\begin_layout Standard
Calculate the soft label: It can be thought of as assigning “soft labels”
 to the unlabeled data according to the current model 
\begin_inset Formula $θ^{(t)}$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\gamma_{ij}=P(y_{j}|x_{i},\theta^{t})=\frac{\pi_{j}^{t}N(x_{i}|\mu_{j}^{t},\Sigma_{j}^{t})}{\sum_{k=1}^{2}\pi_{k}^{t}N(x_{i}|\mu_{k}^{t},\Sigma_{k}^{t})}
\]

\end_inset


\end_layout

\begin_layout Standard
M-step: Note that the 
\begin_inset Formula $M$
\end_inset

-step is the max the overall likelihood 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:MLE_SSL"

\end_inset

, rather than just the second term of 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:MLE_SSL"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
l_{j} & = & \sum_{i=1}^{l+u}\gamma_{ij}\\
\mu_{j}^{t+1} & = & \frac{1}{l_{j}}\sum_{i=1}^{l+u}\gamma_{ij}x_{i}\\
\Sigma_{j}^{t+1} & = & \frac{1}{l_{j}}\sum_{i=1}^{l+u}\gamma_{ij}(x_{i}-\mu_{j}^{t+1})(x_{i}-\mu_{j}^{t+1})^{T}\\
\pi_{j}^{t+1} & = & \frac{l_{j}}{l+u}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Subsection
Assumptions of Mixture Models in SSL
\end_layout

\begin_layout Standard
EM in SSL assumes each class of data only comes from one single gaussian
 cluster.
\end_layout

\begin_layout Standard
A case that EM does not work: a dataset contains four clusters of data,
 two of each class, The correct decision boundary is a horizontal line along
 the x-axis.
 Clearly, the data is not generated from two Gaussians.
 If we insist that each class is modeled by a single Gaussian, the results
 may be poor.
\end_layout

\begin_layout Itemize
As the case mentioned above, we may be better off using only labeled data
 and supervised learning in this case.
\end_layout

\begin_layout Itemize
In the above example, one might model each class itself as a GMM with two
 components, instead of a single Gaussian.
\end_layout

\begin_deeper
\begin_layout Itemize
That means you need to first cluster the data using un-supervised method.
\end_layout

\end_deeper
\begin_layout Itemize
Another way is to de-emphasize the unlabeled data, in case the model correctness
 is uncertain.
 Specifically, we scale the contribution from unlabeled data in the semi-supervi
sed log likelihood (3.13) by a small positive weight λ < 1 (bascially put
 
\begin_inset Formula $\lambda$
\end_inset

 on the second term of Equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:MLE_SSL"

\end_inset

)
\end_layout

\begin_layout Subsection
Identification Issues
\end_layout

\begin_layout Part
Spam and Fraud
\end_layout

\begin_layout Subsection
Typical Types of Spam
\end_layout

\begin_layout Enumerate
Users creation:
\end_layout

\begin_deeper
\begin_layout Enumerate
whether log in/ sign up different accounts with same IP at the same time
\end_layout

\begin_layout Enumerate
Techniques: 
\series bold
enforcing phone/email verification
\end_layout

\end_deeper
\begin_layout Enumerate
Users Hacked:
\end_layout

\begin_deeper
\begin_layout Enumerate
whether log in at different IP / geography,
\end_layout

\end_deeper
\begin_layout Enumerate
Contents Creation:
\end_layout

\begin_deeper
\begin_layout Enumerate
robots -- same user keeps sending same content, Different user keeps sending
 same content
\end_layout

\begin_layout Itemize
natural lanague processing skills to detect same format of the contents
 
\end_layout

\end_deeper
\begin_layout Enumerate
Contents Interactions
\end_layout

\begin_deeper
\begin_layout Enumerate
Sundenlly a lot of likes and shares from the same IP.
\end_layout

\begin_layout Enumerate
or totally different users -- they are disconnected + no common interest
 --- suddenly like or follow the same person
\end_layout

\end_deeper
\begin_layout Subsection
Techs against Spam
\end_layout

\begin_layout Standard
• Techniques fight against API abuse (i.e., read-only apps, enforcing phone
 verification on app creation, leaked API keys, etc) -
\end_layout

\begin_layout Subsection
Goals, challenges and BotMaker overview
\end_layout

\begin_layout Standard
The goal of any anti-spam system is to reduce spam that the user sees while
 having nearly zero false positives.
 Three key principles guided our design of Botmaker:
\end_layout

\begin_layout Enumerate
Prevent spam content from being created.
 By making it as hard as possible to create spam, we reduce the amount of
 spam the user sees.
\end_layout

\begin_layout Enumerate
Reduce the amount of time spam is visible on Twitter.
 For the spam content that does get through, we try to clean it up as soon
 as possible.
\end_layout

\begin_layout Enumerate
Reduce the reaction time to new spam attacks.
 Spam evolves constantly.
 Spammers respond to the system defenses and the cycle never stops.
 In order to be effective, we have to be able to collect data, and evaluate
 and deploy rules and models quickly
\end_layout

\begin_layout Part
Application in Classification
\end_layout

\begin_layout Subsection

\series bold
Questions First to Ask: --- classify basic types of problems
\end_layout

\begin_layout Itemize
How 
\series bold
expensive
\series default
 to collect the label? OR Whether there is enough 
\series bold
labeled data 
\series default
to build supervised model?
\end_layout

\begin_layout Standard
Structure
\end_layout

\begin_layout Subsection
with known labels: fraud in consumer banking
\end_layout

\begin_layout Standard
Depends on whether you care about the accuracy of forecasting, or the easy
 interpretation of the model.
\end_layout

\begin_layout Itemize
If you care about 
\series bold
accuracy
\series default
: 
\series bold
in my experience,
\series default
 Random Forest is the best -- it gives you accuracy, and also give you classific
ation and probability.
\end_layout

\begin_layout Itemize
If you care about the 
\series bold
interpretation
\series default
 of the model, Logistic is the best.
\end_layout

\begin_deeper
\begin_layout Itemize
because you can literally see the the direction, significance, magnitude
 of each coefficient for each variable.
 
\end_layout

\end_deeper
\begin_layout Subsection
without labels:
\end_layout

\begin_layout Standard

\series bold
Reason
\series default
: too expensive to collect the label, classify business trips and personal
 trips / Classify SMB account and personal account
\end_layout

\begin_layout Standard

\series bold
There are two type of questions:
\end_layout

\begin_layout Enumerate
Anomaly Detection
\end_layout

\begin_layout Enumerate
Clustering
\end_layout

\begin_layout Enumerate
a lot of times this is a problem of anomaly detection, where anomaly is
 just a small percentage of normal cases.
 
\end_layout

\begin_deeper
\begin_layout Enumerate
K-means or Mixture-Gaussian suffice.
\end_layout

\end_deeper
\begin_layout Itemize
with limited/manual collecting labels.
\end_layout

\begin_deeper
\begin_layout Itemize
Semi-Supervised -- Mixture Gaussian.
\end_layout

\end_deeper
\begin_layout Itemize
Purpose is to collect more data
\end_layout

\begin_deeper
\begin_layout Itemize
Active Learning: 
\end_layout

\begin_deeper
\begin_layout Itemize
use unsupervised/supervised learning first, then collect the more uncertain
 point near the boundary.
\end_layout

\end_deeper
\end_deeper
\begin_layout Subsection
How to Measure the goodness of Unsupervised Learning
\end_layout

\begin_layout Subsection
Classification of Geography data
\end_layout

\begin_layout Standard
Unsupervised Learning
\end_layout

\begin_layout Standard
Semi-Supervised Leaning
\end_layout

\begin_layout Standard
Active Learning
\end_layout

\begin_layout Part
Anomaly Detection
\end_layout

\begin_layout Standard
In data mining, anomaly detection (also outlier detection) is the identification
 of items, events or observations which do not conform to
\series bold
 an expected pattern or other items in a dataset.
\series default
[1] Typically the anomalous items will translate to some kind of problem
 such as bank fraud, a structural defect, medical problems or errors in
 a text.
 
\end_layout

\begin_layout Standard

\series bold
Anomalies are also referred to as outliers, novelties, noise, deviations
 and exceptions.[2]
\end_layout

\begin_layout Standard
In particular in the context of abuse and network intrusion detection, 
\series bold
the interesting objects are often not rare objects, but unexpected bursts
 in activit
\series default
y.
 This pattern does not adhere to the common statistical definition of an
 outlier as a rare object, and many outlier detection methods (in particular
 unsupervised methods) will fail on such data, unless it has been aggregated
 appropriately.
 Instead, a cluster analysis algorithm may be able to detect the micro clusters
 formed by these patterns.[3]
\end_layout

\begin_layout Standard
Three broad categories of anomaly detection techniques exist.[1] 
\end_layout

\begin_layout Standard
Unsupervised anomaly detection techniques detect anomalies in an unlabeled
 test data set under the assumption that the majority of the instances in
 the data set are normal by looking for instances that seem to fit least
 to the remainder of the data set.
 
\end_layout

\begin_layout Standard
Supervised anomaly detection techniques require a data set that has been
 labeled as "normal" and "abnormal" and involves training a classifier (the
 key difference to many other statistical classification problems is the
 inherent unbalanced nature of outlier detection).
 
\end_layout

\begin_layout Standard
Semi-supervised anomaly detection techniques construct a model representing
 normal behavior from a given normal training data set, and then testing
 the likelihood of a test instance to be generated by the learnt model.
\end_layout

\begin_layout Part
Fraud
\end_layout

\begin_layout Subsection
How to Fight Fraud in Square
\end_layout

\begin_layout Standard

\series bold
Machine learning for fraud detection
\end_layout

\begin_layout Itemize
We have 50+ machine learning 
\series bold
models
\series default
 and 
\series bold
heuristics
\series default
 that use historical transactions and payment activity to predict whether
 or not a future payment is likely to result in loss to Square.
 We use machine learning techniques like random forests and boosting trees
 to classify merchants as fraudulent, and occasionally other regression
 techniques to estimate the potential loss to Square.
 These models target different types of fraud; for example 
\series bold
product-specific fraud, or buyer vs.
 seller fraud.
\end_layout

\begin_layout Standard

\series bold
Caseload Optimization: Quantify the Type 1 and Type 2 Error
\end_layout

\begin_layout Itemize
For each model, we need to
\series bold
 determine at what threshold the case
\series default
 is worth reviewing, given the potential loss to Square and cost of reviewing
 the case.
 Ideally you want to catch the most bad dollars while reviewing the fewest
 cases as possible.
 Having many models means you need to have some standardized way to evaluate
 the models and allocate resources to reviewing the cases from each model.
 Some common metrics are:
\end_layout

\begin_layout Itemize
Precision: % payments suspected that were bad Recall: % total bad payments/dolla
rs that your models suspected Yield: $ bad payments caught per suspicion
\end_layout

\begin_layout Standard

\series bold
Operations Decisioning
\end_layout

\begin_layout Itemize
The models can't do all the work - if the models were to take automated
 actions on all merchants we'd risk insulting our strong merchants.
 The operations team is then responsible for reviewing those cases.
 So the problem here then is presenting the data to the operations team
 in a way that leads them to making the best decisions, and determining
 and reporting on the metrics that motivate those decisions.
\end_layout

\begin_layout Standard

\series bold
Loss forecasting
\end_layout

\begin_layout Itemize
Loss can take months to realize, but at any given month we want to know
 as soon as possible how loss is tracking overall, and by product, country,
 etc.
 These forecasts should be stable and accurate, as they become the top level
 metrics for the team and are important for accounting as well.
\end_layout

\begin_layout Section
Fraud Knowledge
\end_layout

\begin_layout Subsection
Fraud Type
\end_layout

\begin_layout Enumerate
Lost or stolen
\end_layout

\begin_layout Enumerate
Card not present (online fraud)
\end_layout

\begin_layout Enumerate
Counterfeit (magnet /'mægnət/ strip)
\end_layout

\begin_layout Enumerate
Bust out (the applicant is him/her self, he just apply the card, and maximize
 the balance, and then not paying us at all)
\end_layout

\begin_layout Subsection
Fraud Knowledge
\end_layout

\begin_layout Itemize

\series bold
POS Entry Mode: A literal translation of the value which indicates whether
 the card number was entered via the mag-stripe reader or key pad.
 
\end_layout

\begin_layout Subsection
Customer-Transaction level Model
\end_layout

\begin_layout Itemize
Individual Features
\end_layout

\begin_deeper
\begin_layout Itemize
travel indicator: buy a airplane ticket, call in to mention to travel.
\end_layout

\begin_layout Itemize
whether make purchase before in this zip code
\end_layout

\begin_layout Itemize
whether being watched
\end_layout

\begin_layout Itemize
Refer frequency
\end_layout

\begin_deeper
\begin_layout Itemize
You may miss the first time, but you may identify it second time.
\end_layout

\end_deeper
\begin_layout Itemize
month on book: a long time uber user tent to do not do this thing.
\end_layout

\end_deeper
\begin_layout Itemize
Transaction Feature
\end_layout

\begin_deeper
\begin_layout Itemize
Number of approvals or transactions within several hours ago
\end_layout

\begin_layout Itemize
Home to merchant distance 
\end_layout

\begin_layout Itemize
Merchant categories.
\end_layout

\begin_layout Itemize
Local hours / Local weekday: whether shop in weekdays in work hours
\end_layout

\begin_layout Itemize
POS, Card-not-present
\end_layout

\begin_layout Itemize
Chip
\end_layout

\begin_layout Itemize
POS Entry Mode
\end_layout

\begin_layout Itemize
transaction amount / personal average transaction amount
\end_layout

\begin_layout Itemize
transaction amount / average transaction amount for this merchant category
\end_layout

\begin_layout Itemize
input: billing address, CVV, home zip code, pin
\end_layout

\end_deeper
\begin_layout Standard
Online specific features
\end_layout

\begin_layout Subsection
Merchant-level Model
\end_layout

\begin_layout Itemize
imminent risk when merchant data system is attacked and compromised.
\end_layout

\begin_layout Itemize
So all people who have used cards there will suffer fraud risk.
\end_layout

\begin_layout Itemize
Can apply the control zone for a merchant or geographically, and used in
 decision
\end_layout

\begin_layout Standard
How to detect that merchant,
\end_layout

\begin_layout Enumerate
check what is the common merchant for the customers who are subject to fraud
 went before.
\end_layout

\begin_layout Standard
Fraud after rate: % of customers going to that merchant then report fraud.
\end_layout

\begin_layout Standard
The result of the merchant level fraud model will be used as an input of
 the customer - transaction level model
\end_layout

\begin_layout Subsection
Action Plan
\end_layout

\begin_layout Standard
Decide whether to do any action 
\end_layout

\begin_layout Enumerate
block
\end_layout

\begin_layout Enumerate
approval, but call / message / email --- WATCH
\end_layout

\begin_layout Enumerate
Hold the payment to Merchant
\end_layout

\begin_layout Enumerate
Special rules:
\end_layout

\begin_deeper
\begin_layout Enumerate
Natural Disasters
\end_layout

\end_deeper
\begin_layout Section
How to Model Fraud
\end_layout

\begin_layout Standard
fraud strategy contains 2 parts.
\end_layout

\begin_layout Enumerate
Build a score model:
\end_layout

\begin_deeper
\begin_layout Enumerate
We really just use logistics model, 
\end_layout

\begin_layout Enumerate
We though about Support Vector Machine but it only gave us 1 or negative
 1.
 What we really need is a probability score to used in the action plan.
 Similarly a tree sufffers the same problem.
 Similarly tree model suffers the same problem
\end_layout

\begin_layout Enumerate
There are certainly fancier model, like random forest, which can also give
 you a probability, like 20% tree give you 1 and 80% of tree give you 0,
 But implementing it in our IT system is Formidable task, because we got
 XX Million active customers and our system is definitely capable to do
 random forest in normal time, but During Thanksgiving, it will definitely
 crash.
\end_layout

\begin_layout Enumerate
We really just use logistics model, but we do build models for different
 groups of customers.
 
\end_layout

\begin_deeper
\begin_layout Enumerate
For our X card, which give you 1.5% cash reward, 
\end_layout

\begin_layout Enumerate
For our XX card, which give no reward at all but just lower APR to do balance
 transfer, we get another model
\end_layout

\begin_layout Enumerate
For Business card, we get one model, 
\end_layout

\end_deeper
\end_deeper
\begin_layout Enumerate
Use the score and other characteristic as input for a decision tree to get
 an action plan
\end_layout

\begin_deeper
\begin_layout Enumerate
We really just develop the decision tree heuristically, rather than using
 any tree algorithm.
\end_layout

\begin_layout Enumerate
We use variables like whether the transaction is larger than 1000, whether
 the customer is an VIP, whether the customer is in Travel
\end_layout

\begin_layout Enumerate
For each leave, we measure the True Positive and False Negative, 
\end_layout

\begin_layout Enumerate
Action Plan
\end_layout

\begin_deeper
\begin_layout Enumerate
When in one leaf, the TP is high and FN the want to block the transaction
\end_layout

\begin_layout Enumerate
If you are unsure, then just approve it, but call or message or email the
 customer to remind him or her there is a suspicion.
\end_layout

\begin_deeper
\begin_layout Enumerate
Obviously Making a Phone Call is more expensive and we only use it for relativel
y large transactions
\end_layout

\end_deeper
\begin_layout Enumerate
The third way is just approve the transaction even if it is suspicious,
 and not reminding the customer at all.
 Just let it go.
 
\end_layout

\begin_layout Enumerate
For all approved suspicious accounts, normally we will watch this account
 in our system to see whether there are more suspicion coming in.
\end_layout

\end_deeper
\end_deeper
\begin_layout Subsection
Unsupervised Learning: K-means or PCA
\end_layout

\begin_layout Enumerate
As fraulders keep trying to innovate new ways to do frauld, Supervised model
 will certainly fail in that situation.
 That is where you need Unsupervised Learning.
\end_layout

\begin_layout Enumerate
Even if you didn't want to use Unsupervised model in formal frauld strategy,
 it can still give you a lot of business intuition.
 At least it can tell which particular cluster of customers you want to
 do more research on.
 So I regard it as a giuld of your next step of work.
\end_layout

\begin_layout Enumerate
But in practice, we really use those clustering techniques in the frauld
 strategy, we really use them as a kind of pre-step to do supervised model.
 For example, to lower the dimension.
\end_layout

\begin_layout Subsection
correctly labeling your data.
\end_layout

\begin_layout Standard
A second issue is correctly labeling your data.
 A lot of fraudulent transactions are not detected and subsequently get
 wrongly labeled as non-fraudulent.
 Not only does this compound the class imbalance issue, but the model will
 also have a harder time identifying fraud-specific patterns.
\end_layout

\begin_layout Subsection
Precision Recall on Machine Learning
\end_layout

\begin_layout Standard
The inherent compromise in fraud detection is balancing the trade-off between
 catching fraudsters and increasing friction for good users.
 I spend time with customers to learn which they prioritize, then tune their
 models to catch as many fraudsters as possible without too many false positives.
 In our reporting to customers, one of the most important data points is
 the Confusion Matrix, showing number of fraudsters caught and number missed
 vs.
 number of good users incorrectly caught vs.
 number ignored
\end_layout

\begin_layout Part
Web Search
\end_layout

\begin_layout Section
Latent Senantic (LSI)
\end_layout

\begin_layout Standard
Intro to Information Retrieval p 432
\end_layout

\begin_layout Standard
There are two methods to do a query on groups of documents:
\end_layout

\begin_layout Itemize
Measure similarity by cos
\end_layout

\begin_layout Itemize
LSI
\end_layout

\begin_layout Standard
Called Latent Semantic Indexing because of its ability to correlate semantically
 related terms that are latent in a collection of text, it was first applied
 to text at Bellcore in the late 1980s.
 The method, also called latent semantic analysis (LSA), uncovers the underlying
 latent semantic structure in the usage of words in a body of text and how
 it can be used to extract the meaning of the text in response to user queries,
 commonly referred to as concept searches.
 
\end_layout

\begin_layout Standard
Queries, or concept searches, against a set of documents that have undergone
 LSI will return results that are conceptually similar in meaning to the
 search criteria even if the results don’t share a specific word or words
 with the search criteria.
\end_layout

\begin_layout Subsection
Motivation
\end_layout

\begin_layout Standard
A simple query method to match the exact same word (based on cosine similarity)
 in query with document suffers problem of 
\end_layout

\begin_layout Itemize

\series bold
Synonymy:
\series default
 refers to the fact that the same underlying concept can be described using
 different terms.
\end_layout

\begin_layout Itemize

\series bold
Polysemy
\series default
: describes words that have more than one meaning, which is common property
 of language.
\end_layout

\begin_layout Standard
So want to represent the group of documents 
\begin_inset Formula $C$
\end_inset

 as 
\begin_inset Formula $C_{k}$
\end_inset

, and apply the query on 
\begin_inset Formula $C_{k}$
\end_inset

 instead of 
\begin_inset Formula $C$
\end_inset

.
\end_layout

\begin_layout Subsection
Term-document matrix 
\begin_inset Formula $C$
\end_inset


\end_layout

\begin_layout Standard
LSI begins by constructing a 
\begin_inset Formula $m\times n$
\end_inset

 term-document matrix, 
\begin_inset Formula $C$
\end_inset

, to identify the 
\series bold
occurrences
\series default
 of the 
\begin_inset Formula $m$
\end_inset

 unique terms within a collection of 
\begin_inset Formula $n$
\end_inset

 documents.
 
\end_layout

\begin_layout Standard
In a term-document matrix, 
\end_layout

\begin_layout Itemize
each term is represented by a row, and 
\end_layout

\begin_layout Itemize
each document is represented by a column, 
\end_layout

\begin_layout Itemize
with each matrix cell, 
\begin_inset Formula $a_{ij}$
\end_inset

, initially representing the number of times the associated term appears
 in the indicated document, 
\begin_inset Formula $\mathrm{tf_{ij}}$
\end_inset

.
\end_layout

\begin_layout Description
term 
\begin_inset Formula $m$
\end_inset

: unique words / dictionary of a groups of documents
\end_layout

\begin_layout Description
document 
\begin_inset Formula $n$
\end_inset

: a dataset normally contains a lot of documents.
 Like Shakespeare's writings contains a lot of plays.
\end_layout

\begin_layout Description
topics 
\begin_inset Formula $k$
\end_inset

: linear combination of terms, getting
\end_layout

\begin_layout Standard
This matrix is usually very large and very sparse.
\end_layout

\begin_layout Subsection
Get 
\begin_inset Formula $k$
\end_inset

 topics / Low-rank approximations : SVD
\end_layout

\begin_layout Standard
Intro to Information Retrieval p 434
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
C\approx TSD^{T}
\]

\end_inset


\end_layout

\begin_layout Standard
A rank-reduced, singular value decomposition is performed on the matrix
 to determine patterns in the relationships between the terms and concepts
 contained in the text.
 The SVD forms the foundation for LSI.
 It computes the term and document vector spaces by approximating the single
 term-frequency matrix, A, into three other matrices— 
\end_layout

\begin_layout Enumerate
an 
\begin_inset Formula $m$
\end_inset

 by 
\begin_inset Formula $r$
\end_inset

 
\series bold
term-concept vector
\series default
 matrix T, (
\begin_inset Formula $r$
\end_inset

 is the rank of 
\begin_inset Formula $C$
\end_inset

, or ) 
\end_layout

\begin_layout Enumerate
an 
\begin_inset Formula $r$
\end_inset

 by 
\begin_inset Formula $r$
\end_inset

 singular values matrix 
\begin_inset Formula $S$
\end_inset

, 
\end_layout

\begin_deeper
\begin_layout Enumerate
each singular value represent the weight of topics/concepts
\end_layout

\end_deeper
\begin_layout Enumerate
and a 
\begin_inset Formula $n$
\end_inset

 by 
\begin_inset Formula $r$
\end_inset

 
\series bold
concept-document vector 
\series default
matrix, 
\begin_inset Formula $D$
\end_inset

, which satisfy the following relations:
\end_layout

\begin_layout Itemize
\begin_inset Formula $T^{T}T=I_{r}$
\end_inset

, 
\begin_inset Formula $D^{T}D=I_{r}$
\end_inset


\end_layout

\begin_layout Itemize
\begin_inset Formula $S_{1,1}\geq S_{2,2}\geq\ldots\geq S_{r,r}>0\quad S_{i,j}=0\;\text{where}\;i\neq j$
\end_inset


\end_layout

\begin_layout Standard
The SVD is then truncated to reduce the rank by keeping only the largest
 
\begin_inset Formula $k\le r$
\end_inset

 diagonal entries in the singular value matrix 
\begin_inset Formula $S$
\end_inset

, where 
\begin_inset Formula $k$
\end_inset

 is typically on the order 100 to 300 dimensions.
 This effectively reduces the term and document vector matrix sizes to 
\begin_inset Formula $m\times k$
\end_inset

 and 
\begin_inset Formula $n\times k$
\end_inset

 respectively.
 The SVD operation, along with this reduction, has the effect of preserving
 the most important semantic information in the text while reducing noise
 and other undesirable artifacts of the original space of 
\begin_inset Formula $A$
\end_inset

.
 This reduced set of matrices is often denoted with a modified formula such
 as:
\end_layout

\begin_layout Subsection
35.4 Explanation
\end_layout

\begin_layout Subsection
• Weight of term i
\end_layout

\begin_layout Subsection
in doc j
\end_layout

\begin_layout Subsection
is expressed as a “linear combination of term-concept and doc-concept weights”
\end_layout

\begin_layout Subsection
C_{ij}=
\backslash
sum_{c}^{r}t_{i,c}
\backslash
lambda_{c}d_{j,c}
\end_layout

\begin_layout Subsection
• CC^{T}=TS^{2}T^{2}
\end_layout

\begin_layout Subsection
is the term-term similarity matrix, and the columns of T are the eigenvectors
 of such matrix.
\end_layout

\begin_layout Subsection
• DD^{T}=TS^{2}T^{2}
\end_layout

\begin_layout Subsection
document-document similarity matrix, and the columns of D are the eigenvectors
 of such matrix.
\end_layout

\begin_layout Subsection
35.5 Low Dimention Approximation
\end_layout

\begin_layout Subsection
It turns out that when you select the k
\end_layout

\begin_layout Subsection
largest singular values, and their corresponding singular vectors fromU
\end_layout

\begin_layout Subsection
and V, you get the rank k
\end_layout

\begin_layout Subsection
approximation to {
\backslash
displaystyle X} X with the smallest error (Frobenius norm).
 This approximation has a minimal error.
 But more importantly we can now treat the term and document vectors as
 a "semantic space".
 The row "term" vector 
\backslash
hat{
\backslash
textbf{t}}_{i}^{T}
\end_layout

\begin_layout Subsection
then has k
\end_layout

\begin_layout Subsection
entries mapping it to a lower-dimensional space dimensions.
 These new dimensions do not relate to any comprehensible concepts.
 They are a lower-dimensional approximation of the higher-dimensional space.
 Likewise, the "document" vector {
\backslash
displaystyle 
\backslash
hat{
\backslash
textbf{d}}_{j}}
\end_layout

\begin_layout Subsection
is an approximation in this lower-dimensional space.
 We write this approximation as
\end_layout

\begin_layout Subsection
C_{k}=T_{k}S_{k}D_{k}^{T}
\end_layout

\begin_layout Subsection
Accordingly, it is conventional to omit the rightmost M − r columns of T
\end_layout

\begin_layout Subsection
corresponding to these omitted rows of S; likewise the rightmost N − r columns
 of D
\end_layout

\begin_layout Subsection
are omitted since they correspond in D^{T}
\end_layout

\begin_layout Subsection
to the rows that will be multiplied by the N − r columns of zeros in S.
 This written form of the SVD is sometimes known as the reduced SVD or truncated
 SVD
\end_layout

\begin_layout Subsection
C_{k}=T_{k}S_{k}D_{k}^{T}
\end_layout

\begin_layout Subsection
Efficient LSI algorithms only compute the first k
\end_layout

\begin_layout Subsection
singular values and term and document vectors as opposed to computing a
 full SVD and then truncating it.
\end_layout

\begin_layout Subsection
Namely, the terms are represented by the row vectors of the m × k matrix
\end_layout

\begin_layout Subsection
S_{k}
\backslash
Sigma_{k}
\end_layout

\begin_layout Subsection
, Note that S_{k}
\end_layout

\begin_layout Subsection
is the real mapping from t
\end_layout

\begin_layout Subsection
to k
\end_layout

\begin_layout Subsection
, 
\backslash
Sigma_{k}
\end_layout

\begin_layout Subsection
is just a scalling factors.
\end_layout

\begin_layout Subsection
whereas the documents by the column vectors the k × n matrix
\end_layout

\begin_layout Subsection

\backslash
Sigma_{k}U_{k}^{T}
\end_layout

\begin_layout Subsection
Then the query is represented by the centroid of the vectors for its terms.
\end_layout

\begin_layout Subsection
35.6 Queries
\end_layout

\begin_layout Subsection
the user query q
\end_layout

\begin_layout Subsection
can be represented by at the k
\end_layout

\begin_layout Subsection
dimention world:
\end_layout

\begin_layout Subsection

\backslash
hat{q}^{T}=q^{T}T_{t
\backslash
times k}S_{k
\backslash
times k}^{-1}
\end_layout

\begin_layout Subsection
where q
\end_layout

\begin_layout Subsection
is simply the vector of words in the users query, t
\end_layout

\begin_layout Subsection
is the length of this query..
\end_layout

\begin_layout Subsection
• q^{T}=1
\backslash
times t
\end_layout

\begin_layout Subsection
• 
\backslash
hat{q}^{T}=1
\backslash
times k
\end_layout

\begin_layout Subsection
• T_{t
\backslash
times k}
\end_layout

\begin_layout Subsection
term weights./ or mapping to concepts with terms weights
\end_layout

\begin_layout Subsection
• S_{k
\backslash
times k}^{-1}
\end_layout

\begin_layout Subsection
concept weights.
 Use S_{k
\backslash
times k}^{-1}
\end_layout

\begin_layout Subsection
to scale the query q
\end_layout

\begin_layout Subsection
as the same scale as the unitary matrix T_{t
\backslash
times k}
\end_layout

\begin_layout Subsection
.
 But first you need to insure query q
\end_layout

\begin_layout Subsection
and term-document C
\end_layout

\begin_layout Subsection
both use the standard tf-idf weighting schedual, so that they can use the
 same scaling factor S_{k
\backslash
times k}
\end_layout

\begin_layout Subsection
The query vector can then be compared to all existing document vectors,
 and the documents ranked by their similarity (nearness) to the query.
 One common measure of similarity is the cosine between the query vector
 and document vector.
 Typically, the z closest documents or all documents exceeding some cosine
 threshold are returned to the user
\end_layout

\begin_layout Subsection
35.7 Difference between PCA and LSI
\end_layout

\begin_layout Subsection
One difference I noted was that PCA can only give you either the term-term
 or Document-Document similarity (depending on how you multiplied the coreferenc
e matrix AA∗AA∗ or A∗AA∗A) but SVD/LSA can deliver both since you have eigenvect
ors of both AA∗AA∗ and A∗AA∗A.
 Actually I don't see a reason to use PCA ever over SVD.
\end_layout

\begin_layout Subsection
One difference I noted was that PCA can only give you either the term-term
 or Document-Document similarity (depending on how you multiplied the coreferenc
e matrix AA∗AA∗ or A∗AA∗A) but SVD/LSA can deliver both since you have eigenvect
ors of both AA∗AA∗ and A∗AA∗A.
 Actually I don't see a reason to use PCA ever over SVD.
\end_layout

\begin_layout Subsection
36 Non-Negative Matrix Factorization in Clustering News
\end_layout

\begin_layout Section
Scaling Up
\end_layout

\begin_layout Standard
So far, we have only considered "toy grammars," small grammars that illustrate
 the key aspects of parsing.
 But there is an obvious question as to whether the approach can be scaled
 up to cover large corpora of natural languages.
 How hard would it be to construct such a set of productions by hand? In
 general, the answer is: very hard.
 
\end_layout

\begin_layout Standard
Despite these problems, some large collaborative projects have achieved
 interesting and impressive results in developing rule-based grammars for
 several languages.
 
\end_layout

\begin_layout Standard
Examples are the
\end_layout

\begin_layout Itemize
Lexical Functional Grammar (LFG) Pargram project, t
\end_layout

\begin_layout Itemize
he Head-Driven Phrase Structure Grammar (HPSG) LinGO Matrix framework, 
\end_layout

\begin_layout Itemize
the Lexicalized Tree Adjoining Grammar XTAG Project.
\end_layout

\end_body
\end_document
