#LyX 2.1 created this file. For more info see http://www.lyx.org/
\lyxformat 474
\begin_document
\begin_header
\textclass article
\begin_preamble
\usepackage[BoldFont,SlantFont,CJKnumber,fallback]{xeCJK}%使用TexLive自带的xeCJK宏包，并启用加粗、斜体、CJK数字和备用字体选项
\setCJKmainfont{Songti SC}%设置中文衬线字体,若没有该字体,请替换该字符串为系统已有的中文字体,下同
\setCJKsansfont{STXihei}%中文无衬线字体
\setCJKmonofont{SimHei}%中文等宽字体
%中文断行和弹性间距在XeCJK中自动处理了
%\XeTeXlinebreaklocale “zh”%中文断行
%\XeTeXlinebreakskip = 0pt plus 1pt minus 0.1pt%左右弹性间距
\usepackage{indentfirst}%段落首行缩进
\setlength{\parindent}{2em}%缩进两个字符
\end_preamble
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package auto
\inputencoding utf8-plain
\fontencoding global
\font_roman default
\font_sans default
\font_typewriter default
\font_math auto
\font_default_family default
\use_non_tex_fonts true
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100
\graphics default
\default_output_format pdf4
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref true
\pdf_bookmarks true
\pdf_bookmarksnumbered true
\pdf_bookmarksopen true
\pdf_bookmarksopenlevel 3
\pdf_breaklinks false
\pdf_pdfborder false
\pdf_colorlinks true
\pdf_backref section
\pdf_pdfusetitle true
\pdf_quoted_options "unicode=false"
\papersize default
\use_geometry true
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 0
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 0
\use_package mhchem 1
\use_package stackrel 0
\use_package stmaryrd 0
\use_package undertilde 0
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 2.5cm
\topmargin 2.5cm
\rightmargin 2.5cm
\bottommargin 2.5cm
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes true
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\author -445235034 "yangguodaxia" 
\author 16419249 "v660271" 
\end_header

\begin_body

\begin_layout Title
Machine Learning_Learning Theory and GLM: July 6 2016
\end_layout

\begin_layout Author
Fan Yang
\begin_inset Foot
status open

\begin_layout Plain Layout
First version: Feb 
\begin_inset Formula $4{}^{th}$
\end_inset

, 2013
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset CommandInset toc
LatexCommand tableofcontents

\end_inset


\end_layout

\begin_layout Part*
Book Reference
\end_layout

\begin_layout Itemize
Logit, GDA ,Naive Bayes etc: http://www.stat.washington.edu/courses/stat527/s14/sli
des/LDA-QDA-KDE-NaiveBayes.pdf
\end_layout

\begin_layout Itemize
http://www.cs.cmu.edu/~avrim/ML14/
\end_layout

\begin_layout Itemize
http://www.cs.cornell.edu/Courses/cs6783/2014fa/
\end_layout

\begin_layout Itemize
ISL 
\begin_inset CommandInset label
LatexCommand label
name "ISL"

\end_inset

: text_classical_An Introduction to Statistical Learning with Applications
 in R.
\end_layout

\begin_layout Itemize
ITF: Introduction to Time Series and Forecasting
\end_layout

\begin_layout Itemize
TSAA-R: Time Series analysis and its application with R examples.
\end_layout

\begin_layout Itemize
http://en.wikipedia.org/wiki/Predictive_modelling
\end_layout

\begin_layout Itemize
http://en.wikipedia.org/wiki/Kaggle
\end_layout

\begin_layout Itemize
Probability and statistics EBook :
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Flex URL
status open

\begin_layout Plain Layout

http://wiki.stat.ucla.edu/socr/index.php/Probability_and_statistics_EBook
\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Part
Information Theory
\end_layout

\begin_layout Section
Entropy
\end_layout

\begin_layout Standard
wiki:
\end_layout

\begin_layout Standard
There are reasons (explained below) to define information as the negativeof
 the logarithm of the probability distribution.
\end_layout

\begin_layout Standard
Units of entropy are the shannon, nat, or hartley, depending on the baseof
 the logarithm used to define it, though the shannon is commonly referredto
 as a bit.The base of the logarithm is not important as long as the same
 one is usedconsistently: change of base merely results in a rescaling of
 the entropy.Information theorists may prefer to use base 2 in order to express
 theentropy in bits; mathematicians and physicists will often prefer the
 naturallogarithm, resulting in a unit of nats for the entropy.
\end_layout

\begin_layout Standard
Generally, entropy refers to disorder or uncertainty: If one of the eventsis
 more probable than others, observation of that event is less informative.Convers
ely, rarer events provide more information when observed.
\end_layout

\begin_layout Standard
Shannon defined the entropy Η (Greek letter Eta) of a discrete random variableX
 with possible values {x1, …, xn} and probability mass function P(X) as:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
H(X)=\mathrm{E}[\mathrm{I}(X)]=\mathrm{E}[-\ln(\mathrm{P}(X))]=-\sum_{k\ge1}p_{k}\log p_{k}\;.
\]

\end_inset


\end_layout

\begin_layout Standard
Here E is the expected value operator, and I is the information contentof
 X.[4][5] I(X) is itself a random variable.
\end_layout

\begin_layout Standard
One may also define the conditional entropy of two events X and Y takingvalues
 xi and yj respectively, as
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
H(X|Y)=\sum_{i,j}p(x_{i},y_{j})\log\frac{p(y_{j})}{p(x_{i},y_{j})}
\]

\end_inset


\end_layout

\begin_layout Subsection
Rationale to use negative log of
\begin_inset Formula $P(x)$
\end_inset

to represent information
\end_layout

\begin_layout Enumerate
I(p) ≥ 0 – information is a non-negative quantity
\end_layout

\begin_layout Enumerate
I(1) = 0 – events that always occur do not communicate information
\end_layout

\begin_layout Enumerate
I(p1 p2) = I(p1) + I(p2) – information due to independent events is additive
\end_layout

\begin_layout Subsection
bit and Entropy
\end_layout

\begin_layout Itemize
\begin_inset Formula $\log_{2}$
\end_inset

下的信息量单位是bit: 每一种消息可以赋予一个概率，所有消息的概率之和为1。设一种消息的概率为p，则
\begin_inset Formula $-\log p$
\end_inset

称为该消息的信息量。log以2为底时，信息量的单位为bit。
\end_layout

\begin_layout Itemize
熵的单位也是bit: 所有消息的信息量的加权平均（以概率为权，下同）称为该消息集合的熵 Entropy，其单位也是bit。
\end_layout

\begin_layout Itemize
二进制的消息单位也是bit: 消息可以通过编码转化成具体的01串，这样的01串可以度量其长度，单位还是bit。
\end_layout

\begin_deeper
\begin_layout Itemize
In information theory, one bit is typically defined as the uncertainty of
 a binary random variable that is 0 or 1 with equal probability, [2] or
 the information that is gained when the value of such a variable becomes
 known.[3]
\end_layout

\begin_layout Itemize
The length of a binary number may be referred to as its bit-length.
\end_layout

\begin_layout Itemize
对于一种特定的编码方案，我们可以计算消息集合中所有消息的编码长度的加权平均值。信息论告诉我们，平均编码长度不会小于熵。
\end_layout

\end_deeper
\begin_layout Subsection
byte
\end_layout

\begin_layout Standard
Multiple bits may be expressed and represented in several ways.
 For convenience of representing commonly reoccurring groups of bits in
 information technology, several units of information have traditionally
 been used.
 The most common is the unit byte, coined by Werner Buchholz in July 1956,
 which historically was used to represent the number of bits used to encode
 a single character of text (until UTF-8 multibyte encoding took over) in
 a computer[8][9] and for this reason it was used as the basic addressable
 element in many computer architectures.
 The trend in hardware design converged on the most common implementation
 of using eight bits per byte, as it is widely used today.
 However, because of the ambiguity of relying on the underlying hardware
 design, the unit octet was defined to explicitly denote a sequence of eight
 bits.
\end_layout

\begin_layout Standard
64位CPU是指CPU内部的通用寄存器的宽度为64比特，支持整数的64比特宽度的算术与逻辑运算。
\end_layout

\begin_layout Subsection
Information capacity and information compression
\end_layout

\begin_layout Standard
Information capacity of a storage system is only an upper bound to the actual
 quantity of information stored therein.
 If the two possible values of one bit of storage are not equally likely,
 that bit of storage will contain less than one bit of information.
 Indeed, if the value is completely predictable, then the reading of that
 value will provide no information at all (zero entropic bits, because no
 resolution of uncertainty and therefore no information).
\end_layout

\begin_layout Standard
If a computer file that uses n bits of storage contains only m < n bits
 of information, then that information can in principle be encoded in about
 m bits, at least on the average.
\end_layout

\begin_layout Section
Principle of Maximum Entropy
\end_layout

\begin_layout Standard
The principle of maximum entropy, as a method of statistical inference,is
 due to Jaynes [6, 7, 8].His idea is that this principle leads to the selection
 of a probabilitydensity function that is consistent with our knowledge
 and introduces nounwarranted information.
 Any probability density function satisfying the constraints that has smallerent
ropy will contain more information (less uncertainty), and thus sayssomething
 stronger than what we are assuming.(http://www.math.uconn.edu/~kconrad/blurbs/analy
sis/entropypost.pdf)
\end_layout

\begin_layout Standard
The Principle of Maximum Entropy is based on the premise that when estimatingthe
 probability distribution, you should select that distribution whichleaves
 you the largest remaining uncertainty (i.e., the maximum entropy)consistent
 with your constraints.That way you have not introduced any additional assumption
s or biases intoyour calculations
\end_layout

\begin_layout Standard
See example below:
\end_layout

\begin_layout Itemize
when the avaiable information (constraints) are mean and std, then the bestdistr
ibution (the distribution with max entropy with these constraints)to describe
 the data are Normal.
\end_layout

\begin_layout Itemize
If we know more information (more contraints), we can derive other distributions
using the same Principle of Maximum Entropy and the analytical method below.
\end_layout

\begin_layout Subsection
Maximum Entropy and Normal Distribution
\end_layout

\begin_layout Standard
See: https://en.wikipedia.org/wiki/Normal_distribution
\end_layout

\begin_layout Standard
Of all probability distributions over the reals with a specified mean μand
 variance σ2, the normal distribution N(μ, σ2) is the one with maximumentropy.[22
] If X is a continuous random variable with probability densityf(x), then
 the entropy of X is defined as
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
H(X)=-\int_{-\infty}^{\infty}f(x)\log f(x)dx
\]

\end_inset


\end_layout

\begin_layout Standard
where f(x) log f(x) is understood to be zero whenever f(x) = 0.This functional
 can be maximized, subject to the constraints that the distribution is properly
 normalized and has a specified variance, by using variationalcalculus.A
 function with two Lagrangian multipliers is defined:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
L=\int_{-\infty}^{\infty}f(x)\ln(f(x))\,dx-\lambda_{0}\left(1-\int_{-\infty}^{\infty}f(x)\,dx\right)-\lambda\left(\sigma^{2}-\int_{-\infty}^{\infty}f(x)(x-\mu)^{2}\,dx\right)
\]

\end_inset


\end_layout

\begin_layout Standard
where f(x) is, for now, regarded as some function with mean μ and standarddeviat
ion
\backslash
sigma.At maximum entropy, a small variation δf(x) about f(x) will produce
 a variationδL about L which is equal to zero:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
0=\delta L=\int_{-\infty}^{\infty}\delta f(x)\left(\ln(f(x))+1+\lambda_{0}+\lambda(x-\mu)^{2}\right)\,dx
\]

\end_inset


\end_layout

\begin_layout Standard
Since this must hold for any small δf(x), the term in brackets must be zero,and
 solving for f(x) yields:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
f(x)=e^{-\lambda_{0}-1-\lambda(x-\mu)^{2}}
\]

\end_inset


\end_layout

\begin_layout Standard
Using the constraint equations to solve for λ0 and λ yields the normal distribut
ion:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
f(x,\mu,\sigma)=\frac{1}{\sqrt{2\pi\sigma^{2}}}e^{-\frac{(x-\mu)^{2}}{2\sigma^{2}}}
\]

\end_inset


\end_layout

\begin_layout Section
Cross Entropy
\end_layout

\begin_layout Standard
In information theory, the cross entropy between two probability distributions
 over the same underlying set of events measures the average number of bits
 needed to identify an event drawn from the set, if a coding scheme is used
 that is optimized for an "unnatural" probability distribution q, rather
 than the "true" distribution p.
\end_layout

\begin_layout Standard
For discrete p and q this means
\end_layout

\begin_layout Standard
\begin_inset Formula $H(p,q)=-\sum_{x}p(x)\,\log q(x)$
\end_inset


\end_layout

\begin_layout Standard
The situation for continuous distributions is analogous:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
-\int_{X}p(x)\,\log q(x)\,dx.
\]

\end_inset


\end_layout

\begin_layout Standard
As an example, consider alphabet of four letters (A, B, C, D), but with
 A and B having the same frequency and C and D not appearing at all.
 So the probability is
\begin_inset Formula $P=(1/2,1/2,0,0)$
\end_inset

.
\end_layout

\begin_layout Standard
Then if we want to encode it optimally, we encode A as 0 and B as 1, so
 we get 1 bit of encoded message per one letter.
 (And it is exactly Shannon entropy of our probability distribution.)
\end_layout

\begin_layout Standard
But if we have the same probability P, but we encode it according to distributio
n where all letters are equally probably 
\begin_inset Formula $Q=(1/4,1/4,1/4,1/4)$
\end_inset

, then we get 2 bits per letter (for example, we encode A as 00, B as 01,
 C as 10 and D as 11).
\end_layout

\begin_layout Section
Mutual Information
\end_layout

\begin_layout Itemize
more robust option for correlation estimation is mutual information, defined
 as 
\begin_inset Formula 
\[
I(X;Y)=\int_{Y}\int_{X}p(x,y)\log{\left(\frac{p(x,y)}{p(x)\,p(y)}\right)}\;dx\,dy,
\]

\end_inset

 which measures mutual dependence between variables, typically in bits.
 It can be inconvenient to use directly for feature ranking for two reasons
 though.
\end_layout

\begin_deeper
\begin_layout Itemize
Firstly, it is not a metric and not normalized (i.e.
 doesn’t lie in a fixed range), so the MI values can be incomparable between
 two datasets.
 
\end_layout

\begin_layout Itemize
Secondly, it can be inconvenient to compute for continuous variables: in
 general the variables need to be discretized by binning, but the mutual
 information score can be quite sensitive to bin selection.
\end_layout

\end_deeper
\begin_layout Subsection
Maximal Information Coefficient, MIC
\end_layout

\begin_layout Standard
MIC can solve the disadvatanges for mutual information
\end_layout

\begin_layout Standard
Let 
\begin_inset Formula $I((X,Y)|_{G}$
\end_inset

 be the mutual information between 
\begin_inset Formula $X$
\end_inset

 and 
\begin_inset Formula $Y$
\end_inset

 under grid schedual 
\begin_inset Formula $G$
\end_inset

: 
\begin_inset Formula $k$
\end_inset

 bins for 
\begin_inset Formula $X$
\end_inset

 and 
\begin_inset Formula $l$
\end_inset

 bins for 
\begin_inset Formula $Y$
\end_inset

.
\end_layout

\begin_layout Itemize
First to find 
\begin_inset Formula $G$
\end_inset

 or 
\begin_inset Formula $k$
\end_inset

 and 
\begin_inset Formula $l$
\end_inset

 to max 
\begin_inset Formula $I$
\end_inset

: 
\begin_inset Formula $I^{*}=max_{G}I((X,Y)|_{G}$
\end_inset


\end_layout

\begin_layout Itemize
Then normaliz it to get MIC 
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula 
\[
MIC(X,Y)=\frac{I^{*}(I(X,Y)|G)}{\mathbf{lo\mathbf{g\mbox{min}}}(k,l)}
\]

\end_inset


\end_layout

\end_deeper
\begin_layout Part
General Modelling Ideas
\end_layout

\begin_layout Section
Supervised vs Unsupervised Learning
\end_layout

\begin_layout Subsection
Supervised
\end_layout

\begin_layout Standard
Give the machine the correct answer, and train the machine the learn the
 algorithm.
\end_layout

\begin_layout Standard
\begin_inset Formula $X$
\end_inset

 is the design matrix that contains all the training samples 
\begin_inset Formula $x_{i}$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula $(x^{i},y^{i})$
\end_inset

 is 
\begin_inset Formula $i^{th}$
\end_inset

 traning example (row) of a data with size 
\begin_inset Formula $m$
\end_inset


\end_layout

\begin_layout Standard
Learning Algorithm (Hypothesis) 
\begin_inset Formula $h$
\end_inset

, which maps 
\begin_inset Formula $x$
\end_inset

 to 
\begin_inset Formula $y$
\end_inset


\end_layout

\begin_layout Standard
Choose the repsentation of 
\begin_inset Formula $h$
\end_inset

 as Linear 
\begin_inset Formula $\theta$
\end_inset

, thus we have 
\begin_inset Formula 
\[
y=h_{\theta}(x)=\theta^{T}x
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula $\epsilon$
\end_inset

 is the error term, it contains
\end_layout

\begin_layout Enumerate
Unmordeled part
\end_layout

\begin_layout Enumerate
Noise.
\end_layout

\begin_layout Standard
Also assume 
\begin_inset Formula $\epsilon^{i}$
\end_inset

 is 
\begin_inset Formula $iid.$
\end_inset

 (independent across different 
\begin_inset Formula $x^{i},y^{i}$
\end_inset

)
\end_layout

\begin_layout Standard
If it is only contains random independent noise, then it should follow Gaussin
 with mean 
\begin_inset Formula $0$
\end_inset

.
\end_layout

\begin_layout Standard
To minimize Loss function
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\min_{\theta}J(\theta)=min_{\theta}\left\{ h_{\theta}(x)-x\right\} ^{T}\left\{ h_{\theta}(x)-x\right\} 
\]

\end_inset


\end_layout

\begin_layout Standard
In supervised learning Pr(X) is typically of no direct concern.
 One is interested mainly in the properties of the conditional density Pr(Y
 |X).
\end_layout

\begin_layout Subsection
Unsupervised
\end_layout

\begin_layout Standard
Factor analysis: like PCA
\end_layout

\begin_layout Standard
In this case one has a set of N observations (x1, x2, .
 .
 .
 , xN) of a random p-vector X having joint density Pr(X).
 The goal is to directly infer the properties of this probability density
 without the help of a supervisor or teacher providing correct answers or
 degree-of-error for each observation.
\end_layout

\begin_layout Standard
In the context of unsupervised learning, there is no such direct measure
 of success.
 It is difficult to ascertain the validity of inferences drawn from the
 output of most unsupervised learning algorithms.
 One must resort to heuristic arguments not only for motivating the algorithms,
 as is often the case in supervised learning as well, but also for judgments
 as to the quality of the results.
 This uncomfortable situation has led to heavy proliferation of proposed
 methods, since effectiveness is a matter of opinion and cannot be verified
 directly.
 
\end_layout

\begin_layout Subsection
Label the unlabled training data, with hand-written rules
\end_layout

\begin_layout Standard
If you have no labeled training data, and especially if there are existing
 staff knowledgeable about the domain of the data, then you should never
 forget the solution of using hand-written rules
\end_layout

\begin_layout Standard
For example: 
\series bold
IF (wheat OR grain) AND NOT (whole OR bread) THEN c = grain
\end_layout

\begin_layout Standard
In practice, rules get a lot bigger than this, and can be phrased using
 more sophisticated query languages than just Boolean expressions, including
 the use of numeric scores.
 With careful crafting (that is, by humans tuning the rules on development
 data), the accuracy of such rules can become very high.
 Jacobs and Rau (1990) report identifying articles about takeovers with
 92% precision and 88.5% recall, and Hayes and Weinstein (1990) report 94%
 recall and 84% precision over 675 categories on Reuters newswire documents.
 
\end_layout

\begin_layout Subsection
Active Learning System: Automatic Labeliing
\end_layout

\begin_layout Standard
The best way to do this is to insert yourself into a process where humans
 will be willing to label data for you as part of their natural tasks.
 For example, in many cases humans will sort or route email for their own
 purposes, and these actions give information about classes.
\end_layout

\begin_layout Section
Regression vs Classification
\end_layout

\begin_layout Standard
Forecasting error in regression = 
\begin_inset Formula $E(y_{0}-\hat{f}(x_{o}))^{2}$
\end_inset


\end_layout

\begin_layout Standard
Forecasting error in regression = 
\begin_inset Formula $\frac{1}{n}\sum I(y_{i}\ne\hat{y_{i}})$
\end_inset


\end_layout

\begin_layout Section
Interpretability vs.
 Flexibility 
\end_layout

\begin_layout Standard
See ISL
\end_layout

\begin_layout Itemize
Flexibility means more degree of freedoms how the model.
 Normally more complex models, or non-parametric models consums more dfs,
 thus have more flexibility.
\end_layout

\begin_layout Itemize
But more Flexibility often means less Interpretability.
 For example, OLS has a high Interpretability, but less Flexibility.
\end_layout

\begin_layout Subsection
Inference vs.
 Forecasting
\end_layout

\begin_layout Standard
Inference is to measure the effect of certain predictors on dependent.
 
\end_layout

\begin_layout Standard
When you want to focus on Inference instead of Forecasting, then your model
 shall clear Interpretability
\end_layout

\begin_layout Section
Supervised Versus Unsupervised Learning
\end_layout

\begin_layout Itemize
Supervised Learning: You know in the data which one is labeled as 
\begin_inset Formula $1$
\end_inset

 or 
\begin_inset Formula $0$
\end_inset

.
\end_layout

\begin_layout Itemize
Unsupervised Learning: tou don't know the label of data but you need to
 know the correct classification.
\end_layout

\begin_deeper
\begin_layout Itemize
Marketing data (consumers' characters): can separate them into different
 market segments.
\end_layout

\begin_layout Itemize
Google news, classify different news into category
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Index idx
status open

\begin_layout Plain Layout
semi- supervised learning 
\end_layout

\end_inset

semi- supervised learning ISL P42
\end_layout

\begin_layout Section
Ensemble methods
\end_layout

\begin_layout Standard
The goal of ensemble methods is to combine the predictions of several base
 estimators built with a given learning algorithm in order to improve generaliza
bility / robustness over a single estimator.
\end_layout

\begin_layout Standard
Two families of ensemble methods are usually distinguished:
\end_layout

\begin_layout Itemize
In averaging methods, the driving principle is to build several estimators
 independently and then to average their predictions.
 On average, the combined estimator is usually better than any of the single
 base estimator because its variance is reduced.
\end_layout

\begin_deeper
\begin_layout Itemize
Examples: Bagging methods, Forests of randomized trees, ...
\end_layout

\end_deeper
\begin_layout Itemize
By contrast, in boosting methods, base estimators are built sequentially
 and one tries to reduce the bias of the combined estimator.
 The motivation is to combine several weak models to produce a powerful
 ensemble.
\end_layout

\begin_deeper
\begin_layout Itemize
Examples: AdaBoost, Gradient Tree Boosting, ...
\end_layout

\end_deeper
\begin_layout Section
Loss Function Discusssions
\end_layout

\begin_layout Subsection
Least Absolute Deviation LAD
\end_layout

\begin_layout Standard
For each 
\begin_inset Formula $i$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
L=|y_{i}-f(x_{i})|
\]

\end_inset


\end_layout

\begin_layout Itemize
So its negative deviation is 
\begin_inset Formula $-\frac{\partial L}{\partial f}=sign(y_{i}-f_{i})$
\end_inset


\end_layout

\begin_layout Standard
Loss for data:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
L=\sum_{i}^{N}|y-f|
\]

\end_inset


\end_layout

\begin_layout Standard
If 
\begin_inset Formula $f$
\end_inset

 can only be taken as a single value, the one minimize 
\begin_inset Formula $L$
\end_inset

 is median of 
\begin_inset Formula $y$
\end_inset

.
\end_layout

\begin_layout Subsection
Binomial Deviance loss / Logit Losss
\end_layout

\begin_layout Standard
\begin_inset Formula $y\in\{1,-1\}$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
log(1+exp(−2yf(X))).
\]

\end_inset


\end_layout

\begin_layout Subsection
K-class (multinomial) Deviance loss function
\end_layout

\begin_layout Itemize
Note that Probabilities of Logit of 
\begin_inset Formula $K$
\end_inset

 class
\end_layout

\begin_deeper
\begin_layout Standard
\begin_inset Formula 
\[
P_{k}(x)=\frac{e^{f_{k}(x)}}{\sum_{l=1}^{K}e^{f_{l}(x)}}
\]

\end_inset


\end_layout

\begin_layout Itemize
So Binomial deviance extends to K-class multinomial deviance loss function
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
L(y,p(x)) & = & -\sum_{i=1}^{K}I(y=G_{k})\mbox{log}p_{k}(x)\\
 & = & -\sum_{i=1}^{K}I(y=G_{k})f_{k}(x)+\mbox{log}\sum_{i=1}^{K}e^{f_{l}(x)}
\end{eqnarray*}

\end_inset


\end_layout

\end_deeper
\begin_layout Subsection
Exponential Loss
\end_layout

\begin_layout Itemize
\begin_inset Formula $L(y,f(x))=exp(-yf(x))$
\end_inset

 for Classification
\end_layout

\begin_deeper
\begin_layout Itemize
The principal attraction of exponential loss in the context of additive
 modeling is computational; it leads to the simple modular reweighting AdaBoost
 algorithm.
 
\end_layout

\end_deeper
\begin_layout Subsection
Squared-error loss 
\end_layout

\begin_layout Itemize
for Classification: is not a good surrogate for misclassification error.
 As seen in Figure 10.4, it is not a monotone decreasing function of increasing
 margin yf(x).Thus, if class assignment is the goal, a monotone decreasing
 criterion serves as a better surrogate loss function.
\end_layout

\begin_deeper
\begin_layout Itemize
Easy to deal with mathematically Not robust to outliers.
\end_layout

\begin_layout Itemize
Not robust to outliers (result in large values for outlier 
\begin_inset Formula $x$
\end_inset

)
\end_layout

\end_deeper
\begin_layout Itemize
Absolute loss for regression (more robust to outliers than Squared-error
 loss)
\end_layout

\begin_layout Subsection
Huber loss (more robust to outliers)
\end_layout

\begin_layout Standard
Huber loss is a loss function used in robust regression, that is less sensitive
 to outliers in data than the squared error loss.
 Huber loss (below) deals well with outliers and nearly as efficient as
 least squares for Gaussian errors.
 
\end_layout

\begin_layout Itemize
\begin_inset Formula 
\[
L(y,F)=\begin{cases}
\frac{1}{2}(y-F)^{2} & |y-F|\le\delta\\
\delta(|y-F|-\delta/2) & |y-F|>\delta
\end{cases}
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Any 
\begin_inset Formula $y$
\end_inset

 that has 
\begin_inset Formula $|y-F|>\delta$
\end_inset

 is believed to be 
\series bold
outliers.
 To not let outliers affect the model, we only use absolute loss for them,
 not squared loss.
\end_layout

\begin_layout Itemize
\begin_inset Formula $\delta$
\end_inset

 is often choosen as 
\begin_inset Formula $\alpha$
\end_inset

-quatitle of distribution 
\begin_inset Formula $|y-F|$
\end_inset


\end_layout

\begin_layout Itemize
Construct in this way such that its derivative has a nice form, and also
 continuous at points 
\begin_inset Formula $|y-F|=\delta$
\end_inset


\end_layout

\end_deeper
\begin_layout Subsection
Compare Loss Functions
\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename H:/jpmDesk/Desktop/Personal/Research_personal/Lyx_Picture/Loss_Functions_Classification.png

\end_inset


\end_layout

\begin_layout Standard
Figure 10.4 shows both the exponential (10.8) and binomial deviance criteria
 as a function of the margin y · f(x).
 
\end_layout

\begin_layout Standard
Misclassification loss 
\begin_inset Formula $L(y,f(x))=I(y·f(x)<0)$
\end_inset

, which gives unit penalty for negative margin values, and no penalty at
 all for positive ones.
\end_layout

\begin_layout Subsection
Negative Likelihood
\end_layout

\begin_layout Itemize
probabilities 
\begin_inset Formula $p_{k}(X)=Pr(G=k|X)$
\end_inset

 and thus 
\begin_inset Formula $\hat{G}(x)=argmax_{k}\hat{p}_{k}(X)$
\end_inset

.
 Then 
\begin_inset Formula $L(G,\hat{G}(x))=-2log\hat{p}_{k}(X)$
\end_inset

.
 
\end_layout

\begin_deeper
\begin_layout Itemize
The quantity −2 × the log-likelihood is sometimes referred to as the deviance.
 The log-likelihood can be used as a loss-function for general response
 densities, such as the Poisson, gamma, exponential, log-normal and others.
 
\end_layout

\begin_layout Itemize
The “−2” in the definition makes the log-likelihood loss for the Gaussian
 distribution match squared-error loss
\end_layout

\end_deeper
\begin_layout Part
Model Assessment and Selection
\end_layout

\begin_layout Section
Variance of model vs.
 Bias: Source of Forecasting Error
\end_layout

\begin_layout Subsection
Variance of model vs.
 Bias: Source of Forecasting Error
\end_layout

\begin_layout Standard
We do not really care how well the method works training on the training
 data.
 Rather, we are interested in the accuracy of the predictions that we obtain
 when we apply our method to previously unseen test data.
 
\end_layout

\begin_layout Standard
Here we use Mean Squared Error as example.
\end_layout

\begin_layout Standard
The predictions error can always be decomposed into the sum of three fundamental
 quantities: 
\end_layout

\begin_layout Enumerate
the variance of the error terms ε (natural randomness of the fact)
\end_layout

\begin_layout Enumerate
the squared bias of f(x0) 
\end_layout

\begin_layout Enumerate
the variance of f(x0)
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
E(y_{0}-\hat{f}(x_{o}))^{2}=var\left[\hat{f}(x_{0})\right]+Bias\left[\hat{f}(x_{0})\right]^{2}+var(\epsilon)
\]

\end_inset


\end_layout

\begin_layout Standard

\series bold
In general, more flexible statistical methods have higher variance.
 
\end_layout

\begin_layout Standard
see 
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout

<<>>=
\end_layout

\begin_layout Plain Layout

set.seed(413)
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

n = 1000
\end_layout

\begin_layout Plain Layout

sample_size = 100
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

# error = rnorm(2*n,0,1)
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

error = rnorm(n,0,1)
\end_layout

\begin_layout Plain Layout

x = c(1:n)
\end_layout

\begin_layout Plain Layout

y = 1+ x + error
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

data = data.frame(y,x,error)
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

fit = ldply(1:100,function(i){
\end_layout

\begin_layout Plain Layout

  #i = 1
\end_layout

\begin_layout Plain Layout

  train_id = sample(1:n,sample_size)
\end_layout

\begin_layout Plain Layout

  train = data.frame(y,x,error)[train_id,]
\end_layout

\begin_layout Plain Layout

  
\end_layout

\begin_layout Plain Layout

  # train$error %>% sd
\end_layout

\begin_layout Plain Layout

  # test = data.frame(y,x,error)[((n+1:n) %in% (train_id+n)),]
\end_layout

\begin_layout Plain Layout

  
\end_layout

\begin_layout Plain Layout

  # nrow(test)
\end_layout

\begin_layout Plain Layout

  # nrow(train)
\end_layout

\begin_layout Plain Layout

  rg = lm(y ~ x, train)
\end_layout

\begin_layout Plain Layout

  
\end_layout

\begin_layout Plain Layout

  # summary(rg)
\end_layout

\begin_layout Plain Layout

  # sd(rg$resi)
\end_layout

\begin_layout Plain Layout

  # rg$residual %>% mean
\end_layout

\begin_layout Plain Layout

  train_result = data.frame(i, 
\end_layout

\begin_layout Plain Layout

                            x= train$x, 
\end_layout

\begin_layout Plain Layout

                            fit = rg$fit, 
\end_layout

\begin_layout Plain Layout

                            residual = rg$resi)
\end_layout

\begin_layout Plain Layout

  
\end_layout

\begin_layout Plain Layout

# randomly generate new y, outside the train data
\end_layout

\begin_layout Plain Layout

  train_result$new_y = 1+ train_result$x + rnorm(sample_size,0,1)
\end_layout

\begin_layout Plain Layout

  train_result
\end_layout

\begin_layout Plain Layout

})
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

fit_sd = fit  %>% 
\end_layout

\begin_layout Plain Layout

  group_by(x) %>% 
\end_layout

\begin_layout Plain Layout

  summarise(n = n (),
\end_layout

\begin_layout Plain Layout

            mean_fit = mean(fit),
\end_layout

\begin_layout Plain Layout

            mean_new_y = mean(new_y),
\end_layout

\begin_layout Plain Layout

            sd_residual = sd(residual),
\end_layout

\begin_layout Plain Layout

            sd_new_y = sd(new_y),
\end_layout

\begin_layout Plain Layout

            sd_fit = sd(fit)
\end_layout

\begin_layout Plain Layout

  )%>% 
\end_layout

\begin_layout Plain Layout

  data.frame
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

fit_with_mean = join(fit, fit_sd, by = 'x')
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

# var for fit: here we minus its mean only to better plot
\end_layout

\begin_layout Plain Layout

ggplot(fit_with_mean) + 
\end_layout

\begin_layout Plain Layout

  geom_point(aes(x = x, y = fit - mean_fit, sda = i),
\end_layout

\begin_layout Plain Layout

             color = 'blue',alpha = 0.2) + labs(title = 'var of fit')
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

# var for new y: much larger than fit
\end_layout

\begin_layout Plain Layout

ggplot(fit_with_mean) + 
\end_layout

\begin_layout Plain Layout

  geom_point(aes(x = x, y = new_y - mean_new_y, sda = i),
\end_layout

\begin_layout Plain Layout

             color = 'red',alpha = 0.2) + labs(title = 'var of real y')
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

@
\end_layout

\end_inset


\end_layout

\begin_layout Subsection
AIC
\end_layout

\begin_layout Standard
ESL P249 : drivation is not that clear,
\end_layout

\begin_layout Standard
AIC is just the log loss function of prediction error = test error + optimism
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
AIC=-\frac{2}{N}loglik+2\frac{d}{N}
\]

\end_inset


\end_layout

\begin_layout Subsection
The Bayesian Approach and BIC
\end_layout

\begin_layout Standard
ESL P252 : drivation is clear!
\end_layout

\begin_layout Subsection
The Effective Number of Parameters
\end_layout

\begin_layout Standard
The concept of “number of parameters” can be generalized, especially to
 models where regularization is used in the fitting.
 (regularization causes lost of degree of freedom).
\end_layout

\begin_layout Standard
a linear fitting method is one for which we can write
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\hat{y}=Sy
\]

\end_inset


\end_layout

\begin_layout Standard
Then the effective number of parameters is defined as 
\begin_inset Formula 
\[
df(S)=trace(S),
\]

\end_inset


\end_layout

\begin_layout Standard
if y arises from an additive-error model Y = f(X) + ε with Var(ε) = 
\begin_inset Formula $σ_{\epsilon}^{2}$
\end_inset

 , then 
\begin_inset Formula $df(\hat{y})=\frac{\sum Cov(\hat{y_{i},y_{i})}}{\sigma_{\epsilon}^{2}}$
\end_inset


\end_layout

\begin_layout Subsection
Learning Curve
\end_layout

\begin_layout Standard
See http://www.astroml.org/sklearn_tutorial/practical.html
\end_layout

\begin_layout Standard
Train the model with different sample sizes from small to high, and plot
 the traning errors and testing errors at 
\begin_inset Formula $y-axis$
\end_inset

 and corresponding sample sizes at 
\begin_inset Formula $x-axis$
\end_inset

 .
\end_layout

\begin_layout Itemize
If raning errors and testing errors are staying high even if the training
 sample size increases, then that means you have a high BIAS in model
\end_layout

\begin_layout Itemize
If there are huge difference in training errors and testing errors, that
 means you have a high Variance model
\end_layout

\begin_layout Subsection
Application: how to deal with Bias and Variance
\end_layout

\begin_layout Standard

\series bold
High Bias
\end_layout

\begin_layout Standard
If our algorithm shows high bias, the following actions might help:
\end_layout

\begin_layout Standard
Add more features.
 In our example of predicting home prices, it may be helpful to make use
 of information such as the neighborhood the house is in, the year the house
 was built, the size of the lot, etc.
 Adding these features to the training and test sets can improve a high-bias
 estimator
\end_layout

\begin_layout Itemize
Use a more sophisticated model.
 Adding complexity to the model can help improve on bias.
 For a polynomial fit, this can be accomplished by increasing the degree
 d.
 Each learning technique has its own methods of adding complexity.
\end_layout

\begin_layout Itemize
Use fewer samples.
 Though this will not improve the classification, a high-bias algorithm
 can attain nearly the same error with a smaller training sample.
 For algorithms which are computationally expensive, reducing the training
 sample size can lead to very large improvements in speed.
\end_layout

\begin_layout Itemize
Decrease regularization.
 Regularization is a technique used to impose simplicity in some machine
 learning models, by adding a penalty term that depends on the characteristics
 of the parameters.
 If a model has high bias, decreasing the effect of regularization can lead
 to better results.
\end_layout

\begin_layout Standard

\series bold
High Variance
\end_layout

\begin_layout Standard
If our algorithm shows high variance, the following actions might help:
\end_layout

\begin_layout Itemize
Use fewer features.
 Using a feature selection technique may be useful, and decrease the over-fittin
g of the estimator.
\end_layout

\begin_layout Itemize
Use more training samples.
 Adding training samples can reduce the effect of over-fitting, and lead
 to improvements in a high variance estimator.
\end_layout

\begin_layout Itemize
Increase Regularization.
 Regularization is designed to prevent over-fitting.
 In a high-variance model, increasing regularization can lead to better
 results.
\end_layout

\begin_layout Section
Curse of Dimention and Dimention Reduction Methods
\end_layout

\begin_layout Standard
High dimentions means there are lot of features / independent variables.
 
\end_layout

\begin_layout Standard
\begin_inset Index idx
status open

\begin_layout Plain Layout
Curse of Dimention
\end_layout

\end_inset

Curse of Dimention: 
\end_layout

\begin_layout Enumerate
noise features increase with the dimensionality, exacerbating the risk of
 overfitting.
\end_layout

\begin_layout Enumerate
In the high-dimensional setting, the multicollinearity problem is extreme:
 any variable in the model can be written as a linear combination of all
 of the other variables in the model.
\end_layout

\begin_layout Standard
PCA, ridge/lasso can be used in dimention reduction.
\end_layout

\begin_layout Section
Cross Validation
\end_layout

\begin_layout Standard
Separate the data into Train and Test, to avoid overfitting.
\end_layout

\begin_layout Subsection
Wrong way to do CV 
\end_layout

\begin_layout Standard
ESL p266
\end_layout

\begin_layout Standard
In general, with a multistep modeling procedure, cross-validation must be
 applied to the entire sequence of modeling steps.
 In particular, samples must be “left out” before any supervised variable
 selection or filtering steps are applied.
 
\end_layout

\begin_layout Standard
There is one qualification: initial unsupervised screening steps can be
 done before samples are left out.
 For example, we could select the 1000 predictors with highest variance
 across all 50 samples, before starting cross-validation.
 Since this filtering does not involve the class labels, it does not give
 the predictors an unfair advantage.
\end_layout

\begin_layout Subsection
K-Ford
\end_layout

\begin_layout Standard
Each time randomly choose 1/K data and Test data, the left ones are Train
 data.
\end_layout

\begin_layout Standard
The purposes are
\end_layout

\begin_layout Itemize
Then we train K models, select the best according to Forecasting errors
 in their corresponding 1/K data.
\end_layout

\begin_deeper
\begin_layout Itemize
Estimate the testing error 
\begin_inset Formula $TestingError=\frac{1}{K}\sum_{j=1}^{K}CV_{k}$
\end_inset

 where 
\begin_inset Formula $CV_{j}$
\end_inset

 is the testing error for the jth model (
\begin_inset Formula $CV_{j}=\frac{1}{n}\sum_{i}(\hat{y}_{i}-y_{i})^{2}$
\end_inset

.
\end_layout

\begin_layout Itemize
Similarly, estimation of testing error is the simple average of prediction
 error for each fold (ESL p242) 
\end_layout

\end_deeper
\begin_layout Standard
ESL P275: Empirical observation:
\end_layout

\begin_layout Standard
We conclude that estimation of test error for a particular training set
 is not easy in general, given just the data from that same training set.
 Instead, cross-validation and related methods may provide reasonable estimates
 of the expected error Err.
\end_layout

\begin_layout Subsection
Training, Validation and Test data
\end_layout

\begin_layout Standard
ESL P241
\end_layout

\begin_layout Standard
If we are in a data-rich situation, the best approach for both problems
 is to randomly divide the dataset into three parts: 
\end_layout

\begin_layout Enumerate

\series bold
a training set: to biuld models
\end_layout

\begin_layout Enumerate

\series bold
a validation set: to select models
\end_layout

\begin_layout Enumerate

\series bold
a test set: to assess the selected model.
\end_layout

\begin_deeper
\begin_layout Enumerate
Ideally, the test set should be kept in a “vault,” and be brought out only
 at the end of the data analysis.
\end_layout

\end_deeper
\begin_layout Standard
ESL P241.
 It is difficult to give a general rule on how to choose the number of observati
ons in each of the three parts, as this depends on the signal-to- noise
 ratio in the data and the training sample size.
 A typical split might be 50% for training, and 25% each for validation
 and testing.
\end_layout

\begin_layout Standard
ISL P44
\end_layout

\begin_layout Subsection
How to Measure Error
\end_layout

\begin_layout Standard
ESL 238
\end_layout

\begin_layout Standard
For data 
\begin_inset Formula $X$
\end_inset

, the loss function could be 
\end_layout

\begin_layout Itemize
\begin_inset Formula $L(Y,\hat{f}(x))=\left(Y-\hat{f}(x)\right)^{2}$
\end_inset

 (squared error)
\end_layout

\begin_layout Itemize
\begin_inset Formula $L(Y,\hat{f}(x))=|Y-\hat{f}(x)|$
\end_inset

 (absolute messure).
\end_layout

\begin_layout Standard
For categorical 
\begin_inset Formula $G=\{1..K\}$
\end_inset

, 
\end_layout

\begin_layout Itemize
if we estimate 
\begin_inset Formula $G$
\end_inset

 directly, 
\begin_inset Formula $L(G,\hat{G}(x))=I(G\ne\hat{G}(x))$
\end_inset

.
 (0-1 loss)
\end_layout

\begin_layout Itemize
If we esitmate the probabilities 
\begin_inset Formula $p_{k}(X)=Pr(G=k|X)$
\end_inset

 and thus 
\begin_inset Formula $\hat{G}(x)=argmax_{k}\hat{p}_{k}(X)$
\end_inset

.
 Then 
\begin_inset Formula $L(G,\hat{G}(x))=-2log\hat{p}_{k}(X)$
\end_inset

.
 
\end_layout

\begin_deeper
\begin_layout Itemize
The quantity −2 × the log-likelihood is sometimes referred to as the deviance.
 The log-likelihood can be used as a loss-function for general response
 densities, such as the Poisson, gamma, exponential, log-normal and others.
 
\end_layout

\begin_layout Itemize
The “−2” in the definition makes the log-likelihood loss for the Gaussian
 distribution match squared-error loss
\end_layout

\end_deeper
\begin_layout Subsection
Type of Errors
\end_layout

\begin_layout Subsubsection
Test Error / Generalization Error / Prediction Error
\end_layout

\begin_layout Standard
where both X and Y are drawn randomly from their joint distribution (population).
 
\end_layout

\begin_layout Standard
\begin_inset Formula $X$
\end_inset

 could be a new observation.
\end_layout

\begin_layout Standard

\series bold
Here the training set T is fixed, and test error refers to the error forthe
 model based this specific training set.
 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
Err_{\tau}=E\left[L(Y,\hat{f}(X)|\tau\right]
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula $X$
\end_inset

 could be a new observation.
\end_layout

\begin_layout Standard
\begin_inset Formula $Err_{\tau}$
\end_inset

 can be thought of as extra-sample error.
\end_layout

\begin_layout Subsubsection
Expected Test Error
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
Err=E\left[Err_{\tau}\right]
\]

\end_inset


\end_layout

\begin_layout Standard
Note that this expectation averages over everything that is random, including
 the randomness in the training set that produced 
\begin_inset Formula $\hat{f}$
\end_inset

.
\end_layout

\begin_layout Itemize

\series bold
Example
\series default
: See example ESL 238: Figure 7.1 shows the prediction error (light red curves)
 ErrT for 100 simulated training sets each of size 50.
 The lasso (Section 3.4.2) was used to produce the sequence of fits.
 The solid red curve is the average, and hence an estimate of Err.
\end_layout

\begin_layout Standard
That mean it assumes the the average-out training set can fully represent
 the population.
 
\series bold
Thus 
\begin_inset Quotes eld
\end_inset

Variance of the model (error of overfitting)
\begin_inset Quotes erd
\end_inset

 is eliminated.
\end_layout

\begin_layout Itemize

\series bold
Usage
\series default
 & 
\series bold
How to get it
\series default
: 
\begin_inset Formula $Err$
\end_inset

 is the eventual goal, though it is hard to get, especially when we only
 have one set of training set.
 In that case, you can only get 
\begin_inset Formula $Err_{r}$
\end_inset

.
 OR you can do bootstrap: each time only choose partial training sets and
 biuld a model and do these 100 times, to get the 
\begin_inset Formula $Err$
\end_inset

.
\end_layout

\begin_layout Subsubsection
Training Error
\end_layout

\begin_layout Standard
Training error is the average loss over the training sample:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\bar{err}=\frac{1}{N}\sum^{N}L(y_{i},\hat{f}(x_{i}))
\]

\end_inset


\end_layout

\begin_layout Subsubsection
In-Sample (test/prediction) Error
\end_layout

\begin_layout Standard
For the same training data 
\begin_inset Formula $X$
\end_inset

, if we observe different set of 
\begin_inset Formula $Y$
\end_inset

 as 
\begin_inset Formula $Y_{0}$
\end_inset

, then we get the in-sample error (prediction error).
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
Err_{in}=\frac{1}{N}\sum E_{Y^{0}}\left[L(Y_{i}^{0},\hat{f}(x_{i}))|\tau\right]
\]

\end_inset


\end_layout

\begin_layout Subsubsection
Optimism = Prediction Error - Training Error
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
op=Err_{in}-\bar{err}
\]

\end_inset


\end_layout

\begin_layout Standard
??? We can usually estimate only the expected error 
\begin_inset Formula $ω=E_{y}(op)$
\end_inset

 rather than 
\begin_inset Formula $op$
\end_inset

, in the same way that we can estimate the expected error 
\begin_inset Formula $Err$
\end_inset

 rather than the conditional error 
\begin_inset Formula $Err_{\tau}$
\end_inset

 .
\end_layout

\begin_layout Subsubsection
Estimates of Optimsm
\end_layout

\begin_layout Standard
Once we know the optimism, we know the in-sample prediction error 
\begin_inset Formula $Err_{in}=op+\bar{err}$
\end_inset

.
\end_layout

\begin_layout Standard
ESL p248
\end_layout

\begin_layout Standard
For classification where 
\begin_inset Formula $y=\{1,0\}$
\end_inset

 and (
\begin_inset Formula $\hat{y}$
\end_inset

 could be {0,1} or a probability)
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
op=2/N\sum Cov(\hat{y}_{i},y_{i})
\]

\end_inset


\end_layout

\begin_layout Standard
The idea is, the higher the correlation between 
\begin_inset Formula $y_{i}$
\end_inset

 and 
\begin_inset Formula $y$
\end_inset

 (the better your model fit the sample), then the higher the optimisim.
\end_layout

\begin_layout Standard
For squared loss and addictive error: 
\begin_inset Formula $Y=f(X)+\epsilon$
\end_inset

, 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
op=2\frac{d}{N}\sigma_{\epsilon}^{2}
\]

\end_inset


\end_layout

\begin_layout Standard
Effective Number of Parameters 
\end_layout

\begin_layout Section
AIC and BIC
\end_layout

\begin_layout Subsection
Effective number of Parameters
\end_layout

\begin_layout Standard
P249 
\end_layout

\begin_layout Subsection
AIC and BIC
\end_layout

\begin_layout Standard
P252 推倒
\end_layout

\begin_layout Section
Model Selection
\end_layout

\begin_layout Itemize
Trade off between bias and error
\end_layout

\begin_layout Itemize
Overfitting and underfitting both give you high Generalization Error.
\end_layout

\begin_layout Itemize
If there are 
\begin_inset Formula $n$
\end_inset

 features, then there are 
\begin_inset Formula $2^{n}$
\end_inset

 subset of possible subsets of all possible features.
\end_layout

\begin_layout Subsection
Model overall Evaluation
\end_layout

\begin_layout Enumerate
Valiation error
\end_layout

\begin_layout Enumerate
Mallows's 
\begin_inset Formula $C_{p}$
\end_inset

: an estimate of forecasting MSE
\end_layout

\begin_deeper
\begin_layout Standard
\begin_inset Formula 
\[
C_{p}=\frac{1}{n}(RSS+2p\hat{\sigma}^{2})
\]

\end_inset


\end_layout

\begin_layout Standard
where 
\begin_inset Formula $\hat{\sigma}^{2}$
\end_inset

 is the error variance, and 
\begin_inset Formula $p$
\end_inset

 is the number of features.
\end_layout

\end_deeper
\begin_layout Enumerate
AIC: for least squares models, Cp and AIC are proportional to each other.
\end_layout

\begin_layout Enumerate
BIC: BIC just replace the 
\begin_inset Formula $k$
\end_inset

 in AIC by 
\begin_inset Formula $log(N)$
\end_inset

, where 
\begin_inset Formula $N$
\end_inset

 is number of oberservations.
 Since logn > 2 for any n > 7, the BIC statistic generally places a heavier
 penalty on models with many variables, and hence results in the selection
 of smaller models than Cp.
\end_layout

\begin_layout Subsection
Hold-out cross validation
\end_layout

\begin_layout Enumerate
Randomly split the data into training set (70%) and Hold-out cross validation
 set (30%).
\end_layout

\begin_layout Enumerate
Pick up the 
\begin_inset Quotes eld
\end_inset

best
\begin_inset Quotes erd
\end_inset

 model with lowest generation error in Hold-out cross validation set.
\end_layout

\begin_layout Enumerate
Re-train the 
\begin_inset Quotes eld
\end_inset

best
\begin_inset Quotes erd
\end_inset

 model with the whole data (this step is optional).
\end_layout

\begin_layout Standard
Not a effienct way to use the data (as sometimes the whole data is so small)
\end_layout

\begin_layout Subsection
K-Ford Cross Validation
\end_layout

\begin_layout Enumerate
Randomly divide the data into K pieces.
\end_layout

\begin_layout Enumerate
Repeatly train the K-1 pieces, and test on last piece
\end_layout

\begin_layout Enumerate
Repeate the process for K peices, till all pieces are under test once.
\end_layout

\begin_layout Enumerate
Choose the one with lowest Average generation error.
\end_layout

\begin_layout Standard
K normally is 10.
 This method is computationally expensive.
\end_layout

\begin_layout Subsection
Stepwise 
\end_layout

\begin_layout Subsubsection
Forward
\end_layout

\begin_layout Enumerate
Start with 
\begin_inset Formula $F=\emptyset$
\end_inset

, create 
\begin_inset Formula $p$
\end_inset

 models with each just getting one feature from features 
\begin_inset Formula $i=1....p$
\end_inset

 .
 Evaluate the models according to testing error (in Cross Validation), or
 AIC, or BIC, etic.
 Select the best model 
\begin_inset Formula $j$
\end_inset

.
 Then the feature 
\begin_inset Formula $j$
\end_inset

 will be chosen in this step.
\end_layout

\begin_layout Enumerate
Based on model 
\begin_inset Formula $j$
\end_inset

, create 
\begin_inset Formula $p$
\end_inset

 models with one additional feature from {1...p, without j}.
 Then choose the best model, thus we add another feature.
\end_layout

\begin_layout Enumerate
Do this till adding another feature cannot improve the model
\end_layout

\begin_layout Standard
Forward stepwise can be used when 
\begin_inset Formula $p>N$
\end_inset

 (features are more than oberservations)
\end_layout

\begin_layout Subsubsection
Backward Selection
\end_layout

\begin_layout Standard
Start from the full model, deleting one feature each time.
\end_layout

\begin_layout Subsubsection
Both
\end_layout

\begin_layout Standard
Allow a flexible starting model between null model and full model.
 Also allow deleting any variable already chosen in the model.
\end_layout

\begin_layout Subsection
Filter Method
\end_layout

\begin_layout Standard
Compute some measure of how informative 
\begin_inset Formula $x_{i}$
\end_inset

 is about 
\begin_inset Formula $i$
\end_inset

:
\end_layout

\begin_layout Itemize
Can just just compute the correlation between 
\begin_inset Formula $x_{i}$
\end_inset

 and 
\begin_inset Formula $y$
\end_inset

, and choose the top 
\begin_inset Formula $k$
\end_inset

 features.
\end_layout

\begin_layout Itemize
Can also calculate the 
\begin_inset Index idx
status open

\begin_layout Plain Layout
mutual information
\end_layout

\end_inset

mutual informatio, if 
\begin_inset Formula $x_{i}$
\end_inset

 and 
\begin_inset Formula $y$
\end_inset

 are both either 
\begin_inset Formula $0$
\end_inset

 or 
\begin_inset Formula $1$
\end_inset

, choose the top 
\begin_inset Formula $k$
\end_inset

 features.
\end_layout

\begin_layout Subsubsection
Mutual Information
\end_layout

\begin_layout Standard
Measure how non-indepdent between 
\begin_inset Formula $x_{i}$
\end_inset

 and 
\begin_inset Formula $y$
\end_inset

 (KL meature)
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
MI(x_{i},y)=\sum_{x_{i}}\sum_{y}P(x,y)log\frac{P(x,y)}{P(x)P(y)}
\]

\end_inset


\end_layout

\begin_layout Part

\change_deleted 16419249 1468354275
Fraud
\change_inserted 16419249 1468354276
Uber
\change_unchanged

\end_layout

\begin_layout Standard
Data generation
\end_layout

\begin_layout Standard
Before building models we want data that is reliable and covers all aspects
 of a merchant's activity.
 This means we need high quality data logging across our products and a
 system that allows us to retrieve this data quickly when evaluating a new
 payment.
 Our engineering team has built a system of hundreds of signals useful for
 our fraud detection models that runs in real-time and also allows us to
 get historical data for backtesting.
\end_layout

\begin_layout Standard
Machine learning for fraud detection
\end_layout

\begin_layout Standard
We have 50+ machine learning models and heuristics that use historical transacti
ons and payment activity to predict whether or not a future payment is likely
 to result in loss to Square.
 We use machine learning techniques like random forests and boosting trees
 to classify merchants as fraudulent, and occasionally other regression
 techniques to estimate the potential loss to Square.
 These models target different types of fraud; for example product-specific
 fraud, or buyer vs.
 seller fraud.
\end_layout

\begin_layout Standard
Caseload Optimization
\end_layout

\begin_layout Standard
For each model, we need to determine at what threshold the case is worth
 reviewing, given the potential loss to Square and cost of reviewing the
 case.
 Ideally you want to catch the most bad dollars while reviewing the fewest
 cases as possible.
 Having many models means you need to have some standardized way to evaluate
 the models and allocate resources to reviewing the cases from each model.
 Some common metrics are:
\end_layout

\begin_layout Standard
Precision: % payments suspected that were bad Recall: % total bad payments/dolla
rs that your models suspected Yield: $ bad payments caught per suspicion
\end_layout

\begin_layout Standard
Operations decisioning
\end_layout

\begin_layout Standard
The models can't do all the work - if the models were to take automated
 actions on all merchants we'd risk insulting our strong merchants.
 The operations team is then responsible for reviewing those cases.
 So the problem here then is presenting the data to the operations team
 in a way that leads them to making the best decisions, and determining
 and reporting on the metrics that motivate those decisions.
\end_layout

\begin_layout Standard
Loss forecasting
\end_layout

\begin_layout Standard
Loss can take months to realize, but at any given month we want to know
 as soon as possible how loss is tracking overall, and by product, country,
 etc.
 These forecasts should be stable and accurate, as they become the top level
 metrics for the team and are important for accounting as well.
\change_inserted 16419249 1468339276

\end_layout

\begin_layout Subsection

\change_inserted 16419249 1468339304
Summary
\end_layout

\begin_layout Itemize

\change_inserted 16419249 1468339333
Confusion Matrix
\end_layout

\begin_layout Itemize

\change_inserted 16419249 1468339342
Loss Function
\end_layout

\begin_layout Itemize

\change_inserted 16419249 1468354508
PCA and SVD
\end_layout

\begin_layout Itemize

\change_inserted 16419249 1468354511
Version Control
\end_layout

\begin_layout Subsection

\change_inserted 16419249 1468356507
Supply and Demand
\end_layout

\begin_layout Standard

\change_inserted 16419249 1468356516
Build Uber’s dynamic pricing engine; identify and predict city specific
 traffic, travel, and demand patterns; optimize the assignment process of
 matching/dispatching drivers to riders; match multiple riders to a driver
 for our UberPool product.
\end_layout

\begin_layout Standard

\change_inserted 16419249 1468356516
Develop a variety of forecasting models together with engineered implementations
 for use in Uber’s marketplace decision systems.
 Not only will you will be modeling localized demand, hourly & weekly temporal
 demand patterns, and driving behaviors, but also the effects of other external
 factors such as weather, sporting events, and even other transportation
 schedules.
\end_layout

\begin_layout Itemize

\change_inserted 16419249 1468355971
https://newsroom.uber.com/inferring-uber-rider-destinations/
\end_layout

\begin_layout Itemize

\change_inserted 16419249 1468356121
https://newsroom.uber.com/semi-automated-science-using-an-ai-simulation-framework/
\end_layout

\begin_layout Itemize

\change_inserted 16419249 1468356122
https://newsroom.uber.com/making-an-opening-with-uber/
\end_layout

\begin_layout Subsection

\change_inserted 16419249 1468354924
Geosuge
\end_layout

\begin_layout Standard

\change_inserted 16419249 1468354924
building intelligent decision systems and models responsible for demand
 prediction, menu optimization, ranking, batching & scheduling, etc.; mobile
 product analytics, growth hacking, and more.
\end_layout

\begin_layout Standard

\change_inserted 16419249 1468354285
I’ve spent a lot of time working on Geosurge with my team, making the dynamic
 fare model based on both geolocation and demand (for a ride) to help our
 driver partners be more efficient.
 I’ve also looked at the long term effect of surge on customer retention,
 the short term effect of surge on customer demand, and the effect of different
 dispatch methods (greedy vs.
 not greedy) on dispatch times and distances.
\end_layout

\begin_layout Subsection
How do Fraud
\end_layout

\begin_layout Standard
frauld strategy contains 2 parts.
\end_layout

\begin_layout Enumerate
Biuld a score model:
\end_layout

\begin_deeper
\begin_layout Enumerate
We really just use logistics model, 
\end_layout

\begin_layout Enumerate
We though about Support Vector Machine but it only gave us 1 or negative
 1.
 What we really need is a probability score to used in the action plan.
 Similarly a tree sufffers the same problem.
 Similarly tree model suffers the same problem
\end_layout

\begin_layout Enumerate
There are certainly fancier model, like random forest, which can also give
 you a probability, like 20% tree give you 1 and 80% of tree give you 0,
 But implementing it in our IT system is Formidable task, because we got
 XX Milion active customers and our system is definitely capable to do random
 forest in normal time, but During Thanksgiving, it will definitely crash.
\end_layout

\begin_layout Enumerate
We really just use logistics model, but we do biuld models for different
 groups of customers.
 
\end_layout

\begin_deeper
\begin_layout Enumerate
For our X card, which give you 1.5% cash reward, 
\end_layout

\begin_layout Enumerate
For our XX card, which give no reward at all but just lowerer APR to do
 balance transfer, we get another model
\end_layout

\begin_layout Enumerate
For Business card, we get one model, 
\end_layout

\end_deeper
\end_deeper
\begin_layout Enumerate
Use the score and other chrasteristic as input for a decisition tree to
 get an action plan
\change_inserted 16419249 1468345430

\end_layout

\begin_deeper
\begin_layout Enumerate

\change_inserted 16419249 1468345632
We really just develop the decision tree heruristically, rather than using
 any tree algorithm.
\end_layout

\begin_layout Enumerate

\change_inserted 16419249 1468345954
We use variables like whether the transcation is larger than 1000, whether
 the customer is an VIP, whether the customer is in Travel
\end_layout

\begin_layout Enumerate

\change_inserted 16419249 1468345954
For each leave, we measure the True Positive and False Negative, 
\end_layout

\begin_layout Enumerate

\change_inserted 16419249 1468345434
Action Plan
\change_unchanged

\end_layout

\begin_deeper
\begin_layout Enumerate

\change_inserted 16419249 1468346000
When in one leaf, the TP is high and FN 
\change_deleted 16419249 1468346122
whether you
\change_inserted 16419249 1468346123
the
\change_unchanged
 
\change_deleted 16419249 1468346113
waqnt
\change_inserted 16419249 1468346114
want
\change_unchanged
 to block the transcation
\end_layout

\begin_layout Enumerate

\change_inserted 16419249 1468346135
If you are unsure, then 
\change_deleted 16419249 1468346137
whether you want to
\change_inserted 16419249 1468346140
just
\change_unchanged
 approve it, but call or message or email the customer to remind him or
 her there is a suspiction.
\end_layout

\begin_deeper
\begin_layout Enumerate
Obviousely Making a Phone Call is more expensive and we only use it for
 relatively large transcations
\end_layout

\end_deeper
\begin_layout Enumerate
The third way is just approve the transcation even if it is suspicious,
 and not reminding the customer at all.
 Just let it go.
 
\end_layout

\begin_layout Enumerate
For all approved suspicious accounts, normally we will watch this account
 in our system to see whether there are more suspicion coming in.
\change_inserted 16419249 1468337762

\end_layout

\end_deeper
\end_deeper
\begin_layout Subsection
Unsupervised Learning: K-means or PCA
\end_layout

\begin_layout Enumerate
As fraulders keep trying to innovate new ways to do fraudl, Supervised model
 will certainly fail in that situation.
 That is where you need Unsupervisd Learning.
\end_layout

\begin_layout Enumerate
Even if you didn't want to use Unsupervised model in formal frauld strategy,
 it can still give you a lot of business intuition.
 At least it can tell which paticular cluster of customes you want to do
 more research on.
 So I regard it as a giuld of your next step of work.
\end_layout

\begin_layout Enumerate
But in practise, we really use those clustering techniques in the frauld
 strategy, wereally use them as a kind of pre-step to do supervised model.
 For example, to lower the dimention.
\change_inserted 16419249 1468336453

\end_layout

\begin_layout Subsection
correctly labeling your data.
\end_layout

\begin_layout Standard
A second issue is correctly labeling your data.
 A lot of fraudulent transactions are not detected and subsequently get
 wrongly labeled as non-fraudulent.
 Not only does this compound the class imbalance issue, but the model will
 also have a harder time identifying fraud-specific patterns.
\end_layout

\begin_layout Subsection
Precision Recall on Machine Learning
\end_layout

\begin_layout Standard
The inherent compromise in fraud detection is balancing the trade-off between
 catching fraudsters and increasing friction for good users.
 I spend time with customers to learn which they prioritize, then tune their
 models to catch as many fraudsters as possible without too many false positives.
 In our reporting to customers, one of the most important data points is
 the Confusion Matrix, showing number of fraudsters caught and number missed
 vs.
 number of good users incorrectly caught vs.
 number ignored
\end_layout

\begin_layout Part
Application 
\change_inserted 16419249 1468348413

\end_layout

\begin_layout Standard

\change_inserted 16419249 1468348636
What is: lift, KPI, robustness, model fitting, design of experiments, 80/20
 rule?
\end_layout

\begin_layout Standard

\change_inserted 16419249 1468348761
extreme value theory,
\end_layout

\begin_layout Subsubsection

\change_inserted 16419249 1468348762
66-job-interview-questions-for-data-scientists
\change_unchanged

\end_layout

\begin_layout Standard

\change_inserted 16419249 1468348753
http://www.datasciencecentral.com/profiles/blogs/66-job-interview-questions-for-da
ta-scientists
\begin_inset space ~
\end_inset


\end_layout

\begin_layout Section
Steps
\end_layout

\begin_layout Enumerate
Know the data, know the business / domain knowledge.
\end_layout

\begin_layout Enumerate
Formulate the problem: think about the target function you want to max/min:
 design the loss function / likelihood function carefully
\end_layout

\begin_layout Enumerate
Clean data!
\end_layout

\begin_layout Enumerate
Feature Engineering
\end_layout

\begin_layout Enumerate
Choose Algortithm
\end_layout

\begin_deeper
\begin_layout Enumerate
know whethehr you want just a label of classification, or you want a probability
 of classification: like scores
\end_layout

\begin_layout Enumerate
see the business purposes of the mode, see you need tto have highly human
 interpretable model (random forest is definitely not that interpretable),
 or you just want to use for prediction behind scene as a black box.
\end_layout

\end_deeper
\begin_layout Enumerate
Run Algorithm
\end_layout

\begin_deeper
\begin_layout Enumerate
adjust the hyper-parameter
\end_layout

\end_deeper
\begin_layout Enumerate
Decision:
\end_layout

\begin_deeper
\begin_layout Enumerate
A common approach in such situations is to run a classifier first, and to
 accept all its high confidence decisions, but to put low confidence decisions
 in a queue for manual review
\end_layout

\end_deeper
\begin_layout Section
Words
\end_layout

\begin_layout Standard
Describe Bayesian
\end_layout

\begin_layout Enumerate

\series bold
brittleness, brittle
\end_layout

\begin_layout Enumerate
inlier / outlier
\end_layout

\begin_layout Enumerate
Spherical shape -- Gaussian.
\end_layout

\begin_layout Enumerate
literature
\end_layout

\begin_layout Enumerate
Sophisticated
\end_layout

\begin_layout Enumerate
Easiest and Lazest
\end_layout

\begin_layout Enumerate

\series bold
Versatile
\end_layout

\begin_layout Enumerate
Ablative 消融的 Analysis
\end_layout

\begin_deeper
\begin_layout Itemize
Backward of Error Analysis
\end_layout

\begin_layout Itemize
Adding each more feature of your algorithm, see how much accuracy you would
 get.
\end_layout

\end_deeper
\begin_layout Section
Outliers
\end_layout

\begin_layout Subsection
Know what causes outliers
\end_layout

\begin_layout Standard
Outliers are interesting.
 Depending on the context, they either 
\end_layout

\begin_layout Enumerate
Deserve special attention or 
\end_layout

\begin_layout Enumerate
Should be completely ignored.
 
\end_layout

\begin_deeper
\begin_layout Enumerate
If outliers are generated purely due to randomness (if 99% confidence interval
 is an outlier, then every 100 observations will have one outlier in average).
\end_layout

\end_deeper
\begin_layout Standard
Take the example of revenue forecasting.
 If unusual spikes of revenue are observed, it's probably a good idea to
 pay extra attention to them and figure out what caused the spike.
 But if the outliers are due to mechanical error, measurement error or anything
 else that’s not generalizable, it’s a good idea to filter out these outliers
 before feeding the data to the modeling algorithm.
\end_layout

\begin_layout Subsection
Novelty Detection using one-class SVM
\end_layout

\begin_layout Standard
http://scikit-learn.org/stable/modules/outlier_detection.html
\end_layout

\begin_layout Itemize
The training data is not polluted by outliers, and we are interested in
 detecting anomalies in new observations.
\end_layout

\begin_layout Standard
The One-Class SVM has been introduced by Schölkopf et al.
 for that purpose and implemented in the Support Vector Machines module
 in the svm.OneClassSVM object.
 It requires the choice of a kernel and a scalar parameter to define a frontier.
 The RBF kernel is usually chosen although there exists no exact formula
 or algorithm to set its bandwidth parameter.
 This is the default in the scikit-learn implementation.
 The 
\backslash
nu parameter, also known as the margin of the One-Class SVM, corresponds
 to the probability of finding a new, but regular, observation outside the
 frontier.
\end_layout

\begin_layout Itemize
This method works only the training data is NOT polluted by outliers
\end_layout

\begin_layout Subsection
Outlier Detection
\end_layout

\begin_layout Standard
http://scikit-learn.org/stable/modules/outlier_detection.html
\end_layout

\begin_layout Enumerate
Fitting an elliptic envelope: get clusters of 
\series bold
inliers
\series default
 and 
\series bold
outliers.
\end_layout

\begin_deeper
\begin_layout Enumerate
Outlier detection is similar to novelty detection in the sense that the
 goal is to separate a core of regular observations from some polluting
 ones, called “outliers”.
 Yet, in the case of outlier detection, we don’t have a clean data set represent
ing the population of regular observations that can be used to train any
 tool.
\end_layout

\end_deeper
\begin_layout Enumerate
SVM: though Strictly-speaking, the One-class SVM is not an outlier-detection
 method, but a novelty-detection method: its training set should not be
 contaminated by outliers as it may fit them.
 That said, outlier detection in high-dimension, or without any assumptions
 on the distribution of the inlying data is very challenging, and a One-class
 SVM gives useful results in these situations.
\end_layout

\begin_layout Standard
SVM vs.
 Ellotic Envelope: see http://scikit-learn.org/stable/modules/outlier_detection.ht
ml
\end_layout

\begin_layout Itemize
For a inlier mode well-centered and elliptic, the svm.OneClassSVM is not
 able to benefit from the rotational symmetry of the inlier population.
 In addition, it fits a bit the outliers present in the training set.
 On the opposite, the decision rule based on fitting an covariance.EllipticEnvelo
pe learns an ellipse, which fits well the inlier distribution.
\end_layout

\begin_layout Itemize
As the inlier distribution becomes bimodal, the covariance.EllipticEnvelope
 does not fit well the inliers.
 However, we can see that the svm.OneClassSVM tends to overfit: because it
 has not model of inliers, it interprets a region where, by chance some
 outliers are clustered, as inliers.
\end_layout

\begin_layout Itemize
If the inlier distribution is strongly non Gaussian, the svm.OneClassSVM
 is able to recover a reasonable approximation, whereas the covariance.EllipticEn
velope completely fails.
\end_layout

\begin_layout Subsection
Some models are more sensitive to outliers than others.
 
\end_layout

\begin_layout Standard
For instance, AdaBoost might treat those outliers as "hard" cases and put
 tremendous weights on outliers while decision tree might simply count each
 outlier as one false classification.
 If the data set contains a fair amount of outliers, it's important to either
 use modeling algorithm robust against outliers or filter the outliers out.
\end_layout

\begin_layout Section
Missing Values
\end_layout

\begin_layout Standard
Think the missing value problem in the context of
\end_layout

\begin_layout Enumerate
Missing values in the training data OR forecasting data?
\end_layout

\begin_layout Enumerate
Missing values randomly OR Missing values systemetically?
\end_layout

\begin_layout Subsection
Missing Mechamism
\end_layout

\begin_layout Subsection
Missing values in building the tree
\end_layout

\begin_layout Enumerate
If missing value small: ignore
\end_layout

\begin_layout Enumerate
If a lot of missing value, and especially you think same missing amount
 of value will also appear in the new data: biuld a class for it.
\end_layout

\begin_layout Itemize
But if those missing values are not often, simply ignoring the missing values
 (like ID3 and other old algorithms does) or treating the missing values
 as another category (in case of a nominal feature) are not real handling
 missing values.
\end_layout

\begin_layout Itemize

\series bold
A way to really incorporate missing data when building tree is to discount
 the information gain / impurity reduction by the percent of missing.
\end_layout

\begin_deeper
\begin_layout Itemize
Example from see the complete text book: C4.5_Programs for Marching Learning.pdf
\end_layout

\begin_layout Itemize
Same idea can be applied to any other trees, such as CART, or any non-parametric
 models.
\end_layout

\end_deeper
\begin_layout Standard
When an attribute's value is unknown:
\end_layout

\begin_layout Standard
It is evident that a test can provide no information about the class membership
 of cases whose value of the test attribute is unknown.
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
gain(X)=P_{known}(info(T)-info_{x}(T))+P_{Unknown}\times0=F\times(info(T)-info_{x}(T))
\]

\end_inset


\end_layout

\begin_layout Standard
In this way we discount the InforGain with percent of missing data.
\end_layout

\begin_layout Itemize
Similarly, the definition of split info(X) can be altered by regarding the
 cases with unknown values as an additional group.
 If a test has n outcomes, its split information is computed as if the test
 divided the cases into n + 1 subsets.
\end_layout

\begin_layout Itemize
Therefore, attributes with a lot of missing values will have relaatively
 lower 
\series bold
Inoformation Gain.
\end_layout

\begin_layout Subsection
Missing values in building the forecasting:
\end_layout

\begin_layout Itemize
Example from see the complete text book: C4.5_Programs for Marching Learning.pdf
\end_layout

\begin_layout Standard
Same as the play or not play example, 
\end_layout

\begin_layout Standard
\begin_inset listings
inline false
status open

\begin_layout Plain Layout

Outlook Temp (° F) Humidity (%) Windy? Class
\end_layout

\begin_layout Plain Layout

sunny, 75 70 true Play
\end_layout

\begin_layout Plain Layout

sunny 90 true Don't Play
\end_layout

\begin_layout Plain Layout

sunny 1 85 85 false Don't Play
\end_layout

\begin_layout Plain Layout

sunny 72 95 false Don't Play
\end_layout

\begin_layout Plain Layout

sunny 69 70 false Play 
\end_layout

\begin_layout Plain Layout

overcas 72 90 true Play
\end_layout

\begin_layout Plain Layout

overcast 83 78 false Play
\end_layout

\begin_layout Plain Layout

overcast 64 65 true Play
\end_layout

\begin_layout Plain Layout

overcast 81 75 false Play
\end_layout

\begin_layout Plain Layout

rain 71 80 true Don't Play
\end_layout

\begin_layout Plain Layout

rain 65 70 true Don't Play
\end_layout

\begin_layout Plain Layout

rain 75 80 false Play
\end_layout

\begin_layout Plain Layout

rain 68 80 false Play
\end_layout

\begin_layout Plain Layout

rain 70 96 false Play
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

# Corresponding Tree
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

outlook = sunny
\end_layout

\begin_layout Plain Layout

	humidity <= 75: Play
\end_layout

\begin_layout Plain Layout

	humidity > 75: Don't Play
\end_layout

\begin_layout Plain Layout

outlook = overcast: Play
\end_layout

\begin_layout Plain Layout

outlook = rain:
\end_layout

\begin_layout Plain Layout

	windy = true: Don't Play
\end_layout

\begin_layout Plain Layout

	windy = false: Play
\end_layout

\begin_layout Plain Layout

	
\end_layout

\end_inset


\end_layout

\begin_layout Standard
suppose the 
\end_layout

\begin_layout Enumerate
one original known case beblow became unknow at whether outlook
\end_layout

\begin_layout Standard
\begin_inset listings
inline false
status open

\begin_layout Plain Layout

Outlook Temp Humidity(%) Windy? Decision     
\end_layout

\begin_layout Plain Layout

? 		72   90 	true	  Play 		
\end_layout

\end_inset


\end_layout

\begin_layout Standard
As being 
\series bold
'sunny'
\series default
 has 
\begin_inset Formula $5/13\approx0.4$
\end_inset

 probability (within known data: #sunny/#known = 5/13), the missing obsrvation
 has weight 5/13 when we assume its missing value of 
\series bold
Outlook
\series default
 is 
\series bold
sunny,
\end_layout

\begin_layout Standard
\begin_inset listings
inline false
status open

\begin_layout Plain Layout

Outlook Temp Humidity(%) Windy? Decision     Weight
\end_layout

\begin_layout Plain Layout

sunny 	75   70 	true 	 Play 		1
\end_layout

\begin_layout Plain Layout

sunny 	80   90 	true 	 Don't Play   1
\end_layout

\begin_layout Plain Layout

sunny 	85   85 	false 	Don't Play   1
\end_layout

\begin_layout Plain Layout

sunny 	72   95 	false 	Don't Play   1
\end_layout

\begin_layout Plain Layout

sunny 	69   70 	false 	Play 		1
\end_layout

\begin_layout Plain Layout

? 		72   90 	true	  Play 		5/l3
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Therefore, if we assume the missing observation has outlook = sunny, then
 the sunny branch in the tree has 
\end_layout

\begin_layout Standard
\begin_inset listings
inline false
status open

\begin_layout Plain Layout

humidity::; 75 2 class Play, 0 class Don't Play
\end_layout

\begin_layout Plain Layout

humidity> 75 0.4 class Play, 3 class Don't Play
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Thus the total tree becomes
\end_layout

\begin_layout Standard
\begin_inset listings
inline false
status open

\begin_layout Plain Layout

outlook = sunny:
\end_layout

\begin_layout Plain Layout

	humidity :<; 75: Play (2.0)
\end_layout

\begin_layout Plain Layout

	humidity > 75: Don't Play (3.4/0.4) 
\end_layout

\begin_layout Plain Layout

outlook = overcast: Play (3.2)
\end_layout

\begin_layout Plain Layout

outlook = rain:
\end_layout

\begin_layout Plain Layout

	windy= true: Don't Play (2.4/0.4)
\end_layout

\begin_layout Plain Layout

	windy = false: Play (3.0)
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Summay: 
\end_layout

\begin_layout Itemize
Essentially you are imputing the missing value using simple naive distribution
\end_layout

\begin_layout Section
Unbalanced Data
\end_layout

\begin_layout Standard
http://www.win-vector.com/blog/2015/02/does-balancing-classes-improve-classifier-p
erformance (fraction of datums classified correctly), 
\end_layout

\begin_layout Subsection
Why Models Underperforms in Unbalanced Data
\end_layout

\begin_layout Standard
Because for Unbalanced Data, the final goal normally is not about the accuracy
 of classifier (fraction of datums classified correctly), which all models
 try their best to achieve as high as possible, but it cares about True
 Positive Rate (Sensitivity), which is not the target of any algorithm at
 all.
\end_layout

\begin_layout Itemize
All models have their loss function/likelihood put equal weight on type
 1 and type 2 error, whereas in unbalanced data we really care the rue positive
 rate more than overall accuracy.
\end_layout

\begin_layout Itemize
For example, if there are only 0.1% percent data is 1, the best model for
 overall accuracy is predicts all as 0.
\end_layout

\begin_layout Itemize

\series bold
Sensitivity
\series default
 or true positive rate (TPR) 
\begin_inset Formula $=\frac{TP}{P}=\frac{TP}{TP+FN}$
\end_inset

, where 
\begin_inset Formula $TP$
\end_inset

 as True Positive, and 
\begin_inset Formula $P$
\end_inset

 as Positive.
\end_layout

\begin_deeper
\begin_layout Itemize

\series bold
Sensitivity = Power = 1 - Type II
\end_layout

\end_deeper
\begin_layout Subsection
SVM: Weakness of Soft-Margins in Unbalanced Data
\end_layout

\begin_layout Standard
SVM suffers in unbalanced data first because its loss function assigns equal
 weight on both type 1 and type 2 error, as all the other models do,
\end_layout

\begin_layout Standard
Secondly, it has Weakness of Soft-Margins:
\end_layout

\begin_layout Standard
Mathematically, we can see from eq.
 2 that minimizing the first term on the right hand side ||w||2 /2, is equivalen
t to maximizing the margin γ, while minimizing the second term C Σξ minimizes
 the associated error.
 The constant C specifies what tradeoff we are willing to tolerate between
 maximizing the margin and minimizing the error.
 
\end_layout

\begin_layout Standard
If C is not very large, SVM simply learns to classify everything as negative
 because that makes the “margin” the largest, with zero cumulative error
 on the abundant negative examples.
 The only tradeoff is the small amount of cumulative error on the few positive
 examples, which does not count for much.
 This explains why SVM fails completely in situations with a high degree
 of imbalance.
\end_layout

\begin_layout Subsection
Metrics to measure performance model
\end_layout

\begin_layout Itemize
Regular classification rate (classification accuracy) isn't a good metric,
 because if you correctly classify only the instances of the majority class
 (class with many samples), this metric still gives you a high rate.
 
\end_layout

\begin_layout Itemize
The Area Under the ROC Curve (AUC) is a good metric for evaluation of classifier
s in such datasets.
 
\end_layout

\begin_layout Subsection
How to solve the problem
\end_layout

\begin_layout Enumerate
Undersampling negative data:
\end_layout

\begin_deeper
\begin_layout Enumerate
But resuling in less data used: more variance
\end_layout

\end_deeper
\begin_layout Enumerate
Oversaampling positive data
\end_layout

\begin_deeper
\begin_layout Enumerate
But resulting in repetitive data used: overfitting
\end_layout

\end_deeper
\begin_layout Enumerate
Perfect way
\end_layout

\begin_deeper
\begin_layout Enumerate
Carefully quantify the cost of type 1 and type 2 error
\end_layout

\begin_layout Enumerate
Caaefully quantify the gain of predicting a True Positive and True Negative
\end_layout

\begin_layout Enumerate
Incorporate those adjusted costs and gains into loss function/likelihood
\end_layout

\end_deeper
\begin_layout Enumerate
Ensembling
\end_layout

\begin_deeper
\begin_layout Enumerate
Randomly select 
\begin_inset Formula $k-ford$
\end_inset

 samples, within each time of sampking, undersample the negative parts,
 and then for each sample you biuld model on it.
\end_layout

\begin_layout Enumerate
Then do the prediction on the ensembling of those different models.
\end_layout

\end_deeper
\begin_layout Section
When to use which alogrithm
\end_layout

\begin_layout Itemize
Depend on Data Size!
\end_layout

\begin_deeper
\begin_layout Itemize
If you have fairly little data and you are going to train a supervised classifie
r, then machine learning theory says you should stick to a classifier with
 high bias: like Naive Bayes .
\end_layout

\begin_deeper
\begin_layout Itemize
KNN is a low bias model!
\end_layout

\end_deeper
\begin_layout Itemize
If there is a reasonable amount of labeled data, then you are in the perfect
 position to use everything that we have presented about text classification.
\end_layout

\begin_layout Itemize
If a huge amount of data are available, then the choice of classifier probably
 has little effect on your results and the best choice may be unclear: It
 may be best to choose a classifier based on the scalability of training
 or even runtime efficiency
\end_layout

\end_deeper
\begin_layout Itemize
Depend on Number of Features
\end_layout

\begin_deeper
\begin_layout Itemize
Some algorithm are particular subject to noise features: LDA / Naive Bayes
\end_layout

\end_deeper
\begin_layout Itemize
Depend on whether Linearly separatble
\end_layout

\begin_deeper
\begin_layout Itemize
KNN, SVM with non-linear Knernal, trees is not linear
\end_layout

\begin_layout Itemize
Logit is linear
\end_layout

\end_deeper
\begin_layout Itemize
Depend on the underlying distribution of data.
\end_layout

\begin_layout Itemize
Depend on the purpose of the model: Prediction or Explore/Explanation?
\end_layout

\begin_deeper
\begin_layout Itemize
Random Forest is great in prediction, but bad in iterpretation.
\end_layout

\end_deeper
\begin_layout Subsection
LDA comments
\end_layout

\begin_layout Standard
LDA and QDA generally works well.
\end_layout

\begin_layout Standard
ESL P127.
\end_layout

\begin_layout Standard
The reason is not likely to be that the data are approximately Gaussian,
 and in addition for LDA that the covariances are approximately equal.
 More likely a reason is that the data can only support simple decision
 boundaries such as linear or quadratic, and the estimates provided via
 the Gaussian models are stable.
 This is a bias variance tradeoff—we can put up with the bias of a linear
 decision boundary because it can be estimated with much lower variance
 than more exotic alternatives.
 This argument is less believable for QDA, since it can have many parameters
 itself, although perhaps fewer than the non-parametric alternatives.
\end_layout

\begin_layout Subsection
LDA vs Logit
\end_layout

\begin_layout Standard
P152 ISL
\end_layout

\begin_layout Itemize
I f
\begin_inset Formula $x|y\sim Normal\rightarrow p(y=1|x)\in Logistic$
\end_inset

 but inverse is not
\end_layout

\begin_deeper
\begin_layout Itemize
For example, if 
\begin_inset Formula $x|y\sim Poission$
\end_inset

 we can still get 
\begin_inset Formula $p(y=1|x)\in Logistic$
\end_inset


\end_layout

\end_deeper
\begin_layout Itemize
Assuming 
\begin_inset Formula $x$
\end_inset

 follows a specific distribution is a strong assumption, that means you
 add you own information (belief in Normalality of data) into modeling.
 Thus you need smaller dataset to fit than Logistic regression.
 It works very well when 
\begin_inset Formula $x$
\end_inset

 really follows your assumed distribution.
\end_layout

\begin_layout Itemize

\series bold
Logistic regression makes no assumption on 
\begin_inset Formula $x$
\end_inset

, more loose, more robust, but need more data to fit.
 GDA specifies that 
\begin_inset Formula $x\sim Gaussian$
\end_inset

, so more restrictive.
 
\end_layout

\begin_layout Enumerate
When the classes are well-separated, the parameter estimates for the logisticreg
ression model are surprisingly unstable.Linear discriminant analysis does
 not suffer from this problem.
\end_layout

\begin_layout Enumerate
I f
\begin_inset Formula $n$
\end_inset

 is small and the distribution of the predictors X is approximately normal
 in each of the classes, the linear discriminant model is again more stablethan
 the logistic regression model.
\end_layout

\begin_layout Subsection
KNN vs other Linear Classification methods
\end_layout

\begin_layout Enumerate
KNN is a completely non-parametric approach: no assumptions are made about
 the shape of the decision boundary.
\end_layout

\begin_layout Enumerate
Thus KNN is non-linear also!
\end_layout

\begin_layout Enumerate
LDA, Naive Bayes are all linear and suject certain distribution assumptions.
\end_layout

\begin_layout Standard
Therefore, we can expect this approach to dominate LDA and logistic regressionwh
en the decision boundary is highly non-linear.On the other hand, KNN does
 not tell us which predictors are important;we don’t get a table of coefficients
 as in Table 4.3.
\end_layout

\begin_layout Standard
Nonlinear methods like kNN have high variance.
 It is apparent from Figure 14.6 that kNN can model very complex boundaries
 between two classes.
 It is therefore sensitive to noise documents of the sort depicted in Figure
 14.10.
 As a result the variance term in Equation (14.11) is large for kNN: Test
 documents are sometimes misclassified – if they happen to be close to a
 noise document in the training set – and sometimes correctly classified
 – if there are no noise documents in the training set near them.
 This results in high variation from training set to training set.
\end_layout

\begin_layout Subsection
Tree vs Linear Models
\end_layout

\begin_layout Standard
Unfortunately, trees generally do not have the same level of predictiveaccuracy
 as some of the other regression and classification approachesseen in this
 book.
\end_layout

\begin_layout Standard
If instead there is a highly non-linear and complex relationship betweenthe
 features and the response as indicated by model (8.9), then decisiontrees
 may outperform classical approaches.
\end_layout

\begin_layout Subsection
K-Means vs Gaussin Mixture: Gaussian Mixtures as Soft K-means Clustering
 ?
\end_layout

\begin_layout Standard
ESL P526
\end_layout

\begin_layout Standard
The K-means clustering procedure is closely related to the EM algorithm
 for estimating a certain Gaussian mixture model.(Sections 6.8 and 8.5.1).
\end_layout

\begin_layout Standard
The E-step of the EM algorithm assigns “responsibilities” for each data
 point based in its relative density under each mixture component, while
 the M-step recomputes the component density parameters based on the current
 responsibilities.
\end_layout

\begin_layout Standard
Suppose we specify K mixture components, each with a Gaussian density having
 scalar covariance matrix
\begin_inset Formula $σ^{2}I$
\end_inset

.
 Then the relative density under each mixture component is a monotone function
 of the Euclidean distance between the data point and the mixture center.
 Hence in this setup EM is a “soft” version of K-means clustering, making
 probabilistic (rather than deterministic) assignments of points to cluster
 centers.
 As the variance σ2 → 0, these probabilities become 0 and 1, and the two
 methods coincide
\end_layout

\begin_layout Standard
That means, in Gaussian Mixture, one point can be in one mixture with some
 probability and another mixure with some probability.
 In K-means, one point has to be in one prototype for sure.
\end_layout

\begin_layout Subsection
When to use which
\end_layout

\begin_layout Paragraph
When assume data is actually in lower dimention subpace: can use Factor
 Analysis and PCA
\end_layout

\begin_layout Itemize
Factor Analysis is a density estimation algorithm (need to model the densityof
 training data
\begin_inset Formula $P(x)$
\end_inset

), PCA is not based probabilistic model
\end_layout

\begin_deeper
\begin_layout Itemize
If you want to identify anomalis (very low possibility of happening), thenuse
 FA
\end_layout

\end_deeper
\begin_layout Itemize
If your goal is try to find a lower-dimentional subspace of data, then usePCA,
 as PCA directly do this thing without worrying about density model.
\end_layout

\begin_layout Itemize
Clustering looks to find homogeneous subgroups among the observations.
\end_layout

\begin_layout Paragraph
When Assume data lies in Clumps/Groups
\end_layout

\begin_layout Itemize
Mixture of Gaussian: when assume data follows Gaussian
\end_layout

\begin_layout Itemize
K-Means: when not Gaussian.
\end_layout

\begin_layout Subsection
SVM as Loss Function and Compare with Logistic
\end_layout

\begin_layout Standard
P445 in ESL
\end_layout

\begin_layout Standard
SVM is the same problem as 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
min_{\beta_{0},\beta}\frac{\lambda}{2}||\beta||^{2}+\sum^{N}\left[1-y_{i}f(x_{i})\right]_{+}
\]

\end_inset

where 
\begin_inset Formula $\lambda=1/C$
\end_inset

 and 
\begin_inset Formula $1-y_{i}f(x_{i})$
\end_inset

 represents the error term or Loss Function, and 
\begin_inset Formula $f(x_{i})=\mathbf{sign}\left(P(y=1|x)-\frac{1}{2}\right)$
\end_inset

 and 
\begin_inset Formula $Y\in\{-1,1\}$
\end_inset


\end_layout

\begin_layout Standard
This function has solution same as Primal Problem (P442 ESL)
\end_layout

\begin_layout Standard
\begin_inset Formula $f(){}_{+}$
\end_inset

 means a hinge function, it has value 0 if 
\begin_inset Formula $f()\le0$
\end_inset

, otherwise 
\begin_inset Formula $f()_{+}=f()$
\end_inset

 if 
\begin_inset Formula $f(\ge0)$
\end_inset

.
 It is the loss function of SVM.
\end_layout

\begin_layout Standard
The (negative) log-likelihood or binomial deviance has similar tails as
 the SVM loss, 
\series bold
giving zero penalty to points well inside their margin, and a linear penalty
 to points on the wrong side and far away
\series default
.
 Squared-error, on the other hand gives a quadratic penalty, and points
 well inside their own margin have a strong influence on the model as well.
\end_layout

\begin_layout Subsection
SVM Compare with Logistic
\end_layout

\begin_layout Itemize
SVM can deal with non-linear-separable with Kernals
\end_layout

\begin_layout Itemize
Logistic can give you probability, SVM can only give you pure separation
\end_layout

\begin_layout Itemize
SVM and Logistic just used different loss functions, and also SVM uses regulariz
ation.
\end_layout

\begin_layout Standard
In logistic 
\end_layout

\begin_layout Standard
we min 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
-L(\beta) & = & -\sum_{i}^{N}\mbox{log}P(y_{i}|\beta)\\
 & = & -\sum\mathbf{logit}(y_{i}x'\beta)
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Subsection
Gaussian Mixture compared with Gaussian Linear Discriminative Analysis
\end_layout

\begin_layout Itemize
Similar as Gassian Discriminative analysis, but here we just replace the
 label 
\begin_inset Formula $y$
\end_inset

 by latent random variable 
\begin_inset Formula $z$
\end_inset

).
\end_layout

\begin_layout Itemize
If we know 
\begin_inset Formula $z^{i}$
\end_inset

 (here we do not), we can use the max likelihood directly to get the Gassian
 Discriminative analysis:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
L(\phi,\mu,\epsilon)=\sum logP(x^{i},z^{i},\phi,\mu,\epsilon)
\]

\end_inset


\end_layout

\begin_layout Section
Feature Engineering
\end_layout

\begin_layout Subsection
Why
\end_layout

\begin_layout Itemize
Better features means flexibility.
\end_layout

\begin_deeper
\begin_layout Itemize
You can choose “the wrong models” (less than optimal) and still get good
 results.
\end_layout

\end_deeper
\begin_layout Itemize
Better features means simpler models.
\end_layout

\begin_layout Itemize
Better features means better results
\end_layout

\begin_layout Itemize
Group multiple-class features can make Algorithm Fast
\end_layout

\begin_deeper
\begin_layout Standard
For random forests and other tree-based models, you don't need to apply
 the techniques mentioned in your question, as well as other pre-processing
 techniques like removing multicollinearity and features with little variance.
\end_layout

\begin_layout Standard
However, you do need to pre-process categorical features that have many
 levels if you build decision trees in random forests using CART.
 For a feature with N levels, CART needs to search through 2N−1 combinations
 for a split.
 Even for a feature with not many levels, e.g., 30, it would take CART "forever"
 to search for a split and the huge number of combinations also makes the
 inference (almost) meaningless.
 
\end_layout

\end_deeper
\begin_layout Subsection
How to do Feature Engineering
\end_layout

\begin_layout Itemize
PCA to lower the dimention.
\end_layout

\begin_layout Itemize
use ratios, differences, 
\series bold
interactions
\series default
 to combine two features
\end_layout

\begin_layout Itemize
use log, or use first differetiation (delta) to achive stablebility.
\end_layout

\begin_layout Itemize
Field/Domain Knowlege!
\end_layout

\begin_deeper
\begin_layout Itemize
Returning to fraud detection, high order interaction features like "billing
 address = shipping address and transaction amount < $50" are required for
 good model performance.
 
\end_layout

\end_deeper
\begin_layout Subsection
Feature Engineering in NLP
\end_layout

\begin_layout Itemize
Classification problems will often contain large numbers of terms which
 can be conveniently grouped: specialized tokens like ISBNs or chemical
 formulas.
\end_layout

\begin_deeper
\begin_layout Itemize
Often, using themdirectly in a classifierwould greatly increase the vocabulary
 without providing classificatory power beyond knowing that, say, a chemical
 formula is present.
 In such cases, the number of features and feature sparseness can be reduced
 by matching such items with regular expressions and converting them into
 distinguished tokens.
\end_layout

\begin_layout Itemize
That means, group all the specifial chemical formulas into one tocken.
\end_layout

\end_deeper
\begin_layout Itemize
matching parts of words,
\end_layout

\begin_deeper
\begin_layout Itemize
Parts of words are often matched by character k-gram features.
 Such features can be particularly good at providing classification clues
 for otherwise unknown words when the classifier is deployed.
\end_layout

\begin_layout Itemize
For instance, an unknown word ending in -rase is likely to be an enzyme,
 even if it wasn’t seen in the training data.
\end_layout

\end_deeper
\begin_layout Itemize
matching selected multiword patterns
\end_layout

\begin_deeper
\begin_layout Itemize
For instance, this would be the case if the keyword ethnic was most indicative
 of the categories food and arts, the keyword cleansing was most indicative
 of the category home, but the collocation ethnic cleansing instead indicates
 the category world news.
\end_layout

\end_deeper
\begin_layout Itemize
stemming
\end_layout

\begin_deeper
\begin_layout Itemize
you often need to collapse forms of a word like oxygenate and oxygenation,
\end_layout

\end_deeper
\begin_layout Itemize
Separate feature spaces for document zones (like author, publish year, ISBN
 etc).
 There are two strategies that can be used for document zones.
 
\end_layout

\begin_deeper
\begin_layout Itemize
Above we upweighted words that appear in certain zones.
 This means that we are using the same features (that is, parameters are
 “tied” across different zones), but we pay PARAMETER TYING more attention
 to the occurrence of terms in particular zones.
\end_layout

\begin_layout Itemize
An alternative strategy is to have a completely separate set of features
 and corresponding parameters for words occurring in different zones
\end_layout

\end_deeper
\begin_layout Subsection
How to do Feature Selection
\end_layout

\begin_layout Itemize
Pearson correlation coefficient: just the correlation between 
\begin_inset Formula $x^{j}$
\end_inset

 and 
\begin_inset Formula $y$
\end_inset

.
 OR use Max Mutual Information Critera (see the Information Theory part)
\end_layout

\begin_layout Itemize
Run a quick and dirty model first to see which feature is more important
\end_layout

\begin_deeper
\begin_layout Itemize
训练能够对特征打分的预选模型：RandomForest (OOB variable importance)和Logistic Regression
 (T-statistics)等都能对模型的特征打分，通过打分获得相关性后再训练最终模型；
\end_layout

\end_deeper
\begin_layout Itemize
Regularization methods like LASSO and ridge regression may also be considered
 algorithms with feature selection baked i
\end_layout

\begin_layout Section
Don't take default loss function for granted
\end_layout

\begin_layout Standard
Many practitioners train and pick the best model using the default loss
 function (e.g., squared error).
 In practice, off-the-shelf loss function rarely aligns with the business
 objective.
 Take fraud detection as an example.
 When trying to detect fraudulent transactions, the business objective is
 to minimize the fraud loss.
 The off-the-shelf loss function of binary classifiers weighs false positives
 and false negatives equally.
 To align with the business objective, the loss function should not only
 penalize false negatives more than false positives, but also penalize each
 false negative in proportion to the dollar amount.
 Also, data sets in fraud detection usually contain highly imbalanced labels.
 In these cases, bias the loss function in favor of the rare case (e.g., through
 up/down sampling).
 
\end_layout

\begin_layout Section
Kaggle
\end_layout

\begin_layout Standard
winer's code
\end_layout

\begin_layout Standard
http://www.chioka.in/kaggle-competition-solutions/
\end_layout

\begin_layout Standard
https://github.com/owenzhang
\end_layout

\begin_layout Section
Advice of ML Application
\change_deleted 16419249 1468259994
: MOVE TO TOP
\change_unchanged

\end_layout

\begin_layout Itemize
Try longer iteration for Gradient Asent, or using Newton Method.
\end_layout

\begin_layout Itemize
Compare SVM and Baysian
\end_layout

\begin_deeper
\begin_layout Itemize
let the accuracy 
\begin_inset Formula $\alpha=\sum1\{\hat{y}^{i}\ne y^{i}\}$
\end_inset

 an 
\begin_inset Formula $J$
\end_inset

 is the maximum likehood
\end_layout

\begin_layout Itemize
When 
\begin_inset Formula $ $
\end_inset


\begin_inset Formula $\alpha_{SVM}>\alpha_{Bay}$
\end_inset

 but 
\begin_inset Formula $J_{SVM}>J_{bay}$
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
As Bayeisan is designed to max the likelihood, that means your baysiean
 
\series bold
algorithm did not converge!
\end_layout

\end_deeper
\begin_layout Itemize
When 
\begin_inset Formula $ $
\end_inset


\begin_inset Formula $\alpha_{SVM}<\alpha_{Bay}$
\end_inset

 but 
\begin_inset Formula $J_{SVM}>J_{bay}$
\end_inset

, that means the 
\series bold
target function
\series default
 of baysiean algorithm is wrong (optimizating your function did not help
 fit the problem)!
\end_layout

\end_deeper
\begin_layout Itemize
Error Process
\end_layout

\begin_deeper
\begin_layout Itemize
For a pipeline problem, you need to estimate pahse 1...3..
\end_layout

\begin_layout Itemize
Each phase would incure errors
\end_layout

\begin_layout Itemize
Adding each pahse as given, rather than to estimate, see how much accuracy
 you increase
\end_layout

\end_deeper
\begin_layout Itemize
Ablative 消融的 Analysis
\end_layout

\begin_deeper
\begin_layout Itemize
Backward of Error Analysis
\end_layout

\begin_layout Itemize
Adding each more feature of your algorithm, see how much accuracy you would
 get.
\end_layout

\end_deeper
\begin_layout Part
Statistical Learning Theory
\end_layout

\begin_layout Standard
Statistical learning theory is a framework for machine learning drawing
 from the fields of statistics and functional analysis.[1] Statistical learning
 theory deals with the problem of finding a predictive function based on
 data.
 
\end_layout

\begin_layout Standard
For my understanding, learning theory is to use functional analysis to formalize
 bsic machine learning ideas, including
\end_layout

\begin_layout Itemize
Loss Function
\end_layout

\begin_deeper
\begin_layout Itemize
Regularization: 
\end_layout

\end_deeper
\begin_layout Itemize
Loss Function + Pubishment on Complexity
\end_layout

\begin_layout Standard
Reference
\end_layout

\begin_layout Itemize
See Andrew Ng Lecture Notes 4
\end_layout

\begin_layout Itemize
http://www.kyb.mpg.de/fileadmin/user_upload/files/publications/pdfs/pdf2819.pdf
\end_layout

\begin_layout Section
Theory 
\end_layout

\begin_layout Subsection
Hoeffding inequality/ Chernoff bound
\end_layout

\begin_layout Standard
When 
\begin_inset Formula $z_{i}$
\end_inset

 is from Bernoulli (
\begin_inset Formula $\phi$
\end_inset

) and 
\begin_inset Formula $\hat{\phi}$
\end_inset

 is estimated as the simple sample mean of 
\begin_inset Formula $\frac{1}{m}1_{z_{i}=1}$
\end_inset

, we have
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
P(|\phi-\hat{\phi}|>\gamma)\le2exp(-2\gamma^{2}m)
\]

\end_inset


\end_layout

\begin_layout Paragraph*
Proof: Min Traning error means min in generallization error, when trainning
 set 
\begin_inset Formula $m$
\end_inset

 goes to larger and larger.
\end_layout

\begin_layout Standard
Lecture 9 minitues 30:30
\end_layout

\begin_layout Itemize
Sample Complexity Bound: Question: given error tolerance 
\begin_inset Formula $\delta$
\end_inset

, how large sample do you need.
\end_layout

\begin_layout Itemize
Error Bound: Hold 
\begin_inset Formula $m$
\end_inset

 and 
\begin_inset Formula $\delta$
\end_inset

 fixed, Solve for 
\begin_inset Formula $\gamma$
\end_inset

 (error)
\end_layout

\begin_layout Itemize
Union bond
\end_layout

\begin_deeper
\begin_layout Itemize
Let 
\begin_inset Formula $A_{1}..A_{k}$
\end_inset

 be 
\begin_inset Formula $k$
\end_inset

 not necessarily independent (
\begin_inset Formula $P(A_{1}\cup A_{2}..A_{K})\le P(A_{1})+P(A_{2})...)$
\end_inset


\end_layout

\begin_layout Itemize
Let 
\begin_inset Formula $Z_{1}$
\end_inset

...
\begin_inset Formula $Z_{m}$
\end_inset

 be 
\begin_inset Formula $m$
\end_inset

 iid Bernoul(
\begin_inset Formula $\phi$
\end_inset

).
 Then estiamted mean of 
\begin_inset Formula $Z$
\end_inset

 is 
\begin_inset Formula $\hat{\phi}=\frac{1}{m}\sum Z_{i}$
\end_inset

.
 Then we have (
\begin_inset Index idx
status open

\begin_layout Plain Layout
Chernoff bound
\end_layout

\end_inset


\series bold
Chernoff bound
\series default
) 
\begin_inset Formula 
\[
P(|\hat{\phi}-\phi|>\gamma)\le2exp(-2\gamma^{2}m)
\]

\end_inset


\end_layout

\end_deeper
\begin_layout Itemize
Apply it on Logistic Regression: Video 10 begining
\end_layout

\begin_layout Section
Empirical Risk Minimization (ERM) 
\end_layout

\begin_layout Standard
ERM is a simplifed Machine Learning model
\end_layout

\begin_layout Standard
Empirical Risk Minimization (ERM) (focus of logistic and SVM): 
\begin_inset Formula $\hat{\Theta}=arg,min_{\theta}\hat{\varepsilon}(h_{\theta})$
\end_inset


\end_layout

\begin_layout Paragraph*
Re-write (rethink) the ERM not as a problem of choosing variables, but choosing
 functions:
\end_layout

\begin_layout Standard
Define 
\begin_inset Formula 
\begin{eqnarray*}
H & = & \{h_{\theta}\rightarrow\Theta\in R^{X+1}\}
\end{eqnarray*}

\end_inset

 and 
\begin_inset Formula 
\[
\Theta:X\rightarrow\{0,1\}
\]

\end_inset

 (linear classifer)
\end_layout

\begin_layout Standard
So we want to choose the classifier among 
\begin_inset Formula $H$
\end_inset

 to min the training or.
 
\begin_inset Formula 
\[
\hat{h}=arg.min_{h\in H}\hat{\epsilon}_{s}(h)
\]

\end_inset


\end_layout

\begin_layout Standard
Ultimate Goal: in prediction (no hat in 
\begin_inset Formula $\epsilon$
\end_inset

), Generalization Error, 
\begin_inset Formula $\epsilon(h)=P_{(x,y)}\{h(x)\ne y\}$
\end_inset

.
\end_layout

\begin_layout Section
Variance and Bias Trade-off Theory
\end_layout

\begin_layout Standard
Lecture Note 4: P6-P7 for derivation.
\end_layout

\begin_layout Itemize
Thus,
\begin_inset Formula $\hat{\epsilon}(h)$
\end_inset

 is exactly the mean of the 
\begin_inset Formula $m$
\end_inset

 random variables Zj that are drawn iid from a Bernoulli distribution with
 mean 
\begin_inset Formula $\epsilon(h)$
\end_inset

.
\end_layout

\begin_layout Itemize
\begin_inset Formula $K$
\end_inset

 is the number of possible hypothesis in a finite Hypothesis class 
\begin_inset Formula $H$
\end_inset

, we have
\end_layout

\begin_layout Itemize
\begin_inset Formula $1-\delta$
\end_inset

 is the probability that training error will be within 
\begin_inset Formula $\gamma$
\end_inset

 of generalization error?
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
|\hat{\epsilon}(h)-\epsilon(h)|\le\sqrt{\frac{1}{2m}\times log\frac{2k}{\gamma\delta}}
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\hat{\epsilon}(h)\le(min_{h\in H}\epsilon(h))+2\sqrt{\frac{1}{m}log\frac{2k}{\delta}}
\]

\end_inset


\end_layout

\begin_layout Standard
where 
\end_layout

\begin_layout Enumerate
the first term represents size of biaes: the larger the possible hypothesis
 sets, the smaller 
\begin_inset Formula $(min_{h\in H}\epsilon(h))$
\end_inset


\end_layout

\begin_layout Enumerate
and the second terms represents size of variance: the larger the the possible
 hypothesis sets 
\begin_inset Formula $k$
\end_inset

.
 the more complex the model, and thus the larger the variance.
\end_layout

\begin_layout Standard
This is proved by letting equal the √· term, using our previous argument
 that uniform convergence occurs with probability at least 1 − , and then
 noting that uniform convergence implies "(h) is at most 2higher than "(h∗)
 = minh∈H "(h) (as we showed previously).
\end_layout

\begin_layout Standard
Corollary: Let |H| = k, and let any 
\begin_inset Formula $\delta,\gamma$
\end_inset

, be fixed.
 Then for 
\begin_inset Formula $\hat{\epsilon}(h)\le(min_{h\in H}\epsilon(h))+2\gamma$
\end_inset

 to hold with probability at least 1 − 
\begin_inset Formula $\delta$
\end_inset

 , it suffices that
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
m & \ge & \frac{1}{2\gamma^{2}}log(\frac{2k}{\delta})\\
 & = & O(\frac{1}{\gamma^{2}}log\frac{k}{\gamma})
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Subsubsection*
The case of infinite H
\end_layout

\begin_layout Standard
Lecture Note 4: P8-P9 for derivation.
\end_layout

\begin_layout Standard
Thus, with probability at least 1 − 
\begin_inset Formula $\delta$
\end_inset

, we also have that:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\hat{\epsilon}(h)\le\epsilon(h^{*})+O(\sqrt{\frac{d}{2m}log\frac{m}{d}+\frac{1}{m}log\frac{1}{\delta}}
\]

\end_inset


\end_layout

\begin_layout Standard
where 
\begin_inset Formula $d$
\end_inset

 is the Vapnik-Chervonenkis dimension, written VC(H)
\end_layout

\begin_layout Section
Bayesian Statistic Regulation: NOT FINISH
\end_layout

\begin_layout Standard
Lecture Vedio 11
\end_layout

\begin_layout Paragraph
Frequentists Way:
\end_layout

\begin_layout Itemize
True value of 
\begin_inset Formula $\theta$
\end_inset

 is not a random variable, there is a fixed true value for it.
\end_layout

\begin_layout Itemize
Max Likiehood is the way to get this true value.
\end_layout

\begin_layout Itemize
Choose parameters to get max likelihood 
\begin_inset Formula 
\[
max_{\theta}\prod P(y^{i}|x^{i},\theta)
\]

\end_inset


\end_layout

\begin_layout Itemize
Essentiialy you are minimizing 
\begin_inset Formula $min_{\theta}\sum||y^{i}-\theta^{T}x^{i}||^{2}$
\end_inset


\end_layout

\begin_layout Paragraph
Bayesian Way:
\end_layout

\begin_layout Itemize
No fixed value of 
\begin_inset Formula $\theta$
\end_inset

, as it is a random variable.
 
\end_layout

\begin_layout Itemize
No meaning to talk about one fixd value, we should talk 
\begin_inset Formula $\theta$
\end_inset

 in terms of its diftribution.
\begin_inset Formula 
\[
\hat{\Theta}{}_{map}=arg_{\theta}P(\theta|y,x)=arg_{max}\prod P(y^{i}|x^{i},\theta)P(\theta)
\]

\end_inset


\end_layout

\begin_layout Itemize
Essentiialy (For Gaussian Prior in Linear Regression) you are minimizing
 
\begin_inset Formula $min_{\theta}\sum||y^{i}-\theta^{T}x^{i}||^{2}+\lambda||\theta_{i}||^{2}$
\end_inset

.
 That means Bayesian Algotithm punish additional variables (irrelevant variables
 should have 
\begin_inset Formula $\theta_{i}$
\end_inset

 be 0).
\end_layout

\begin_layout Standard
Essentially Gradient Asent.
\end_layout

\begin_layout Part
Evaluate Classifciaton Model
\end_layout

\begin_layout Section
Model Fit Overall
\end_layout

\begin_layout Subsection
Contingency Table
\end_layout

\begin_layout Standard
\align center
\begin_inset Tabular
<lyxtabular version="3" rows="4" columns="3">
<features rotate="0" tabularvalignment="middle">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $Y=1$
\end_inset

 (Positive)
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $Y=0$
\end_inset

 (Negative)
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $\hat{Y}=1$
\end_inset

 (Alternative)
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
TP
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
Type 1 (FP)
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $\hat{Y}=0$
\end_inset

 (Null)
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
Type 2 (FN)
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
TN
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
P
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
N
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\begin_layout Itemize

\series bold
Sensitivity
\change_inserted 16419249 1468345728
/precision
\change_unchanged

\series default
 or true positive rate (TPR) 
\begin_inset Formula $=\frac{TP}{P}=\frac{TP}{TP+FN}$
\end_inset

, where 
\begin_inset Formula $TP$
\end_inset

 as True Positive, and 
\begin_inset Formula $P$
\end_inset

 as Positive.
\end_layout

\begin_deeper
\begin_layout Itemize

\series bold
Sensitivity = Power = 1 - Type II
\end_layout

\end_deeper
\begin_layout Itemize

\series bold
Specificity
\series default
: True negative (TN) 
\begin_inset Formula $=\frac{TN}{N}=\frac{TN}{TN+FP}$
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize

\series bold
Specificity = 1 - Type 1
\end_layout

\begin_layout Itemize
When there are a lot of negative observations (good people), the model tends
 to have higher 
\series bold
Specificity than Sensitivity, as fitting the negative people is much easier
 than identifying the positive people.
 P160ISL
\end_layout

\end_deeper
\begin_layout Itemize
False positive (FP) :eqv.
 with false alarm, Type I error.
\end_layout

\begin_layout Itemize
False positive rate (FPR) eqv.
 with 
\series bold
fall-out
\series default
 
\begin_inset Formula $\mathit{FPR}=\mathit{FP}/N=\mathit{FP}/(\mathit{FP}+\mathit{TN})$
\end_inset


\end_layout

\begin_layout Itemize
False negative (FN) :eqv.
 with miss, Type II error.
 
\end_layout

\begin_layout Itemize
If we let 
\begin_inset Formula $\hat{Y}=1$
\end_inset

 whenever 
\begin_inset Formula $P(\hat{Y}=1|X)>=Threshold$
\end_inset

, where for example 
\begin_inset Formula $Threshold=0.5$
\end_inset

.
 Then we can calculate the overall correct prediction rate is
\begin_inset Formula 
\[
AccuracyRatio_{full}=\frac{\#N_{1}+\#N_{0}}{\#(Y=1)+\#(Y=0)}.
\]

\end_inset

 
\begin_inset Formula $AccuracyRatio_{full}$
\end_inset

 Should be much better than 
\begin_inset Formula $AccuracyRatio_{naive}$
\end_inset

.
 
\end_layout

\begin_layout Subsection
Global vs Local Fit Measure:
\end_layout

\begin_layout Subsubsection

\series bold
Global measure: 
\end_layout

\begin_layout Standard

\series bold
Measure the overall model fitting at all levels of cut off point
\end_layout

\begin_layout Itemize
Gini index
\end_layout

\begin_layout Itemize
Mean difference D between mean score of goods and bads.
\end_layout

\begin_layout Itemize
KS: The KS is also ideal for local if the expected cutoff value is near
 that point where KS is realized.
 
\end_layout

\begin_layout Itemize
Information statistics: we can also use it for local measure.
\end_layout

\begin_layout Subsubsection

\series bold
Local Fit Measure
\end_layout

\begin_layout Itemize

\series bold
we need to have the best performance of given scoring model nearby expected
 cutoff value.

\series default
 Hence we should judge quality indexes from this point of view.
 Especially we can focus on region of scores where the cutoff is expected.
 Overall, the Lift seems to be the best choice for our purpose.
\end_layout

\begin_layout Enumerate
Lorenz curve from Gini graph.
\end_layout

\begin_layout Enumerate
Lift
\end_layout

\begin_layout Enumerate
Cumulative Lift
\end_layout

\begin_layout Enumerate
ROC 
\end_layout

\begin_layout Subsection

\series bold
Business
\series default
:
\end_layout

\begin_layout Standard
But strategy just use the score to decide the cut-off.
 Ideally those who develop the score shall also decide the cut-off (decide
 the strategy), because 
\end_layout

\begin_layout Itemize
To better develop of score, need to know where the cut-off is.
 So that the modeler can adjust the model according to the performance around
 the cut-off.
\end_layout

\begin_layout Itemize
But nowadays Strategy will decide the cut-off based on finished score model.
\end_layout

\begin_layout Itemize
This is the problem of chicken or egg, which comes first.
\end_layout

\begin_layout Itemize
The only solution is decide the cut-off and develop the model at the same
 time.
 Or in other words, let the same people do both, and treat the cut-off point
 as part of the model itself.
\end_layout

\begin_layout Subsection
Value function
\end_layout

\begin_layout Enumerate
If we reject all people below 
\begin_inset Formula $a$
\end_inset

, 
\end_layout

\begin_deeper
\begin_layout Enumerate
How many good people we reject (FP) and how many bad people we reject (TP).
 
\end_layout

\begin_layout Enumerate
How many bad people we fail to reject (FN) and how many good people we can
 approve.
 (TN)
\end_layout

\end_deeper
\begin_layout Enumerate
Assign a dollar value to each account in FP, TP, FN and TN.
 Then simulate the score from low to high to find the max summed value 
\end_layout

\begin_deeper
\begin_layout Enumerate
It is OK to reject a good people, but it incurs high cost by not identifying
 a default one.
\end_layout

\end_deeper
\begin_layout Subsection
Naive Binary Model
\end_layout

\begin_layout Standard
Idea: your model shall at least be better than this naive model
\end_layout

\begin_layout Standard
\begin_inset Formula $AccuracyRatio_{naive}$
\end_inset

 could just be 
\begin_inset Formula $0.5$
\end_inset

, or the 
\begin_inset Formula $AccuracyRatio_{naive}$
\end_inset

 calculated from the 
\begin_inset Formula $naive$
\end_inset

 model.
 The 
\begin_inset Formula $naive$
\end_inset

 model predicts 
\begin_inset Formula 
\[
P(\hat{Y}=1)=\frac{\#(Y=1)}{\#(Y=1)+\#(Y=0)}
\]

\end_inset

 
\begin_inset Formula 
\[
P(\hat{Y}=0)=\frac{\#(Y=0)}{\#(Y=1)+\#(Y=1)}
\]

\end_inset

which means, the probability that you guess 
\begin_inset Formula $\hat{Y}=1$
\end_inset

 is just the ratio 
\begin_inset Formula $\frac{\#Y=1}{\#Y=1+\#Y=0}$
\end_inset

 in development sample.
 Thus
\begin_inset Formula 
\[
AccuracyRatio_{naive}=\frac{P(\hat{Y}=1)\times\#(Y=1)+P(\hat{Y}=0)\times\#(Y=0)}{\#(Y+1)+\#(Y+0)}
\]

\end_inset


\end_layout

\begin_layout Standard
In addition, we can also choose the 
\begin_inset Formula $Threshold$
\end_inset

 to maximize 
\begin_inset Formula $AccuracyRatio_{full}$
\end_inset

.
\end_layout

\begin_layout Section
Evaluate Model: based on CDF
\end_layout

\begin_layout Subsection
Empirical Cumulative Distribution of Scores
\end_layout

\begin_layout Itemize
For the 
\begin_inset Formula $n$
\end_inset

 good people
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula 
\begin{equation}
F_{n,good}(a)=\frac{1}{n}\sum_{i}^{n}I(s_{i}\le a)\times I(D_{i}=1)\label{eq:Empirical_CDF_Scores}
\end{equation}

\end_inset


\end_layout

\begin_layout Itemize
That means, 
\begin_inset Formula $F(a)$
\end_inset

 is the percentage of all people with score less than 
\begin_inset Formula $a$
\end_inset

 and being good among all good people.
\end_layout

\end_deeper
\begin_layout Itemize
For the 
\begin_inset Formula $n$
\end_inset

 bad people
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula 
\[
F_{n,bad}(a)=\frac{1}{m}\sum_{i}^{m}I(s_{i}\le a)\times I(D_{i}=0)
\]

\end_inset


\end_layout

\end_deeper
\begin_layout Itemize
For 
\begin_inset Formula $N$
\end_inset

 everyone (population).
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula 
\[
F_{n,ALL}(a)=\frac{1}{N}\sum_{i}^{N}I(s_{i}\le a)
\]

\end_inset


\end_layout

\begin_layout Itemize
\begin_inset Formula $F_{ALL}(a)$
\end_inset

 is the percentage of people with score below 
\begin_inset Formula $a$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Itemize

\series bold
Idea: For a good model, the 
\begin_inset Formula $F_{m.bad}(a)$
\end_inset

 shall grow fast in the small 
\begin_inset Formula $a$
\end_inset

, and 
\begin_inset Formula $F_{n.good}(a)$
\end_inset

 shall grow really slow during small 
\begin_inset Formula $a$
\end_inset

.
 See the KS section in Evaluate Model section in this part.
\end_layout

\begin_layout Subsection
Visualization of Scores
\end_layout

\begin_layout Paragraph*
Box plot for scores of good people and bad people
\end_layout

\begin_layout Paragraph*

\series bold
Rank the estimated/predicted value with the true value:
\end_layout

\begin_layout Standard
To see whether they are well-ordered.
 Below estimated values (BT scores) are ordered into ten deciles.
 Then rank the true values (Response Rate) with the estimated values.
\end_layout

\begin_layout Standard
\align center
\begin_inset Graphics
	filename Lyx_Picture/BT_Score_Rank.jpg
	lyxscale 60
	scale 60

\end_inset


\end_layout

\begin_layout Standard
PS.
 Looks like the log curve, which is the feature of logit regression.
\end_layout

\begin_layout Standard
The better model should have a better separation, which means to predict
 correctly more often in binary model, or in the example below, to have
 a higher response rate in top decile and a lower response rate in bottom
 decile.
\end_layout

\begin_layout Standard
So A sharper curve is better.
\end_layout

\begin_layout Standard
\align center
\begin_inset Graphics
	filename Lyx_Picture/OLD_BT_NEW_BT_SCORE.jpg
	lyxscale 50
	scale 50

\end_inset


\end_layout

\begin_layout Subsection
Gini Index and Lorenz Curve in Score Model
\end_layout

\begin_layout Standard
Plot 
\begin_inset Formula $F_{bad}$
\end_inset

 in x-axis and 
\begin_inset Formula $F_{good}$
\end_inset

 in y.
\end_layout

\begin_layout Standard
Each point of curve represents some value of given score.
 If we assume this value as cutoff value, we can read the proportion of
 rejected bad and good clients.
 
\end_layout

\begin_layout Standard
An example of Lorenz curve is given in Figure 2.
 
\series bold
We can see that by rejection (rejection in application, but cover/include
 in statistics) of 20% of good clients we reject almost 60% of bad clients
 at the same moment.
\end_layout

\begin_layout Standard
\align center
\begin_inset Graphics
	filename Lyx_Picture/Lorenz_Curve.png
	scale 50

\end_inset


\end_layout

\begin_layout Standard
Gini index.
 This index describes a global quality of scoring function.
 It takes values between -1 and 1.
 
\end_layout

\begin_layout Standard
Function form: see
\emph on
 Text_How to Measure Quality of Credit Scoring Models
\end_layout

\begin_layout Itemize
The ideal model, i.e.
 scoring function that perfectly separate good and bad clients, has the
 Gini index equal to 1 (perfect discrimination, always reject the 100% of
 bads but 0 goods)
\end_layout

\begin_layout Itemize
On the other hand, model that assigns a random score to the client has this
 index equal to 0.
 (perfect equity)
\end_layout

\begin_layout Subsection
Lift Chart / Cumulative Accuracy Profile
\end_layout

\begin_layout Itemize
Plot 
\begin_inset Formula $F_{all}$
\end_inset

 against 
\begin_inset Formula $F_{bad}$
\end_inset

 
\end_layout

\begin_layout Itemize
Advantage of this figure is that one can easily read the proportion of rejected
 bads vs.
 proportion of all rejected.
 
\end_layout

\begin_deeper
\begin_layout Itemize

\series bold
We can see that if we want to reject 70% of bads, we have to reject about
 40% of all applicants.
\end_layout

\begin_layout Itemize
An example of Lift chart is displayed in Figure 4.
 The ideal model is now represented by polyline from [0, 0] through [pB,
 1] to [1, 1] (assuming there are pB percentage people got bad).
\end_layout

\end_deeper
\begin_layout Standard
\align center
\begin_inset Graphics
	filename Lyx_Picture/Lift.png
	lyxscale 70
	scale 50

\end_inset


\end_layout

\begin_layout Subsection
Cumulative Lift
\end_layout

\begin_layout Itemize

\series bold
How many times (percentage of being better than random), at a given level
 of rejection 
\begin_inset Formula $a$
\end_inset

, is the scoring model better than random selection (random model).
\end_layout

\begin_layout Itemize
More precisely, the ratio indicates the proportion of bad clients with less
 than a score a, to the proportion of bad clients in the general population.
 Formally, it can be expressed by
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
CumLift(a)=\frac{BadRate(a)}{BadRate(+\infty)}=\frac{F_{n.bad}(a)}{F_{N.ALL}(a)}
\]

\end_inset


\end_layout

\begin_layout Standard
Assume that we have a score of 1000 clients, of which 50 are bad.
 The proportion of bad clients is 5%.
 Sort customers according to score and split into ten groups, i.e., divide
 it by deciles of score.
 In each group, in our case around 100 clients, then count bad clients.
 This will get their share in the group (Bad Rate).
 Absolute Lift in each group is then given by the ratio of the share of
 bad clients in the group to the proportion of bad clients in total.
\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename Lyx_Picture/Absolute and Cumulative Lift.png
	lyxscale 70
	scale 70

\end_inset


\end_layout

\begin_layout Subsection
Kolmogorov–Smirnov Test & Statistic (KS)
\end_layout

\begin_layout Standard
Also see the KS section in Basic Statistics section.
\end_layout

\begin_layout Standard
The larger the KS statistic, the better.
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
KS=max_{a}\left\{ F_{n.bad}(a)-F_{n.good}(a)\right\} 
\]

\end_inset


\change_inserted -445235034 1465970871
where 
\begin_inset Formula $\alpha$
\end_inset

 is the score.
\change_unchanged

\end_layout

\begin_layout Standard
Figure 1 gives an example of estimation of distribution functions of good
 and bad clients, including an estimate of KS statistics.
\end_layout

\begin_layout Standard

\series bold
It can be seen, for example, that the score around 2.5 and smaller has population
 of approximately 30% of good clients and 70%.
\end_layout

\begin_layout Standard
\align center
\begin_inset Graphics
	filename Lyx_Picture/KS.png
	lyxscale 140

\end_inset


\end_layout

\begin_layout Subsection

\series bold
KS in compare Distribution
\end_layout

\begin_layout Standard
To test whether two samples are from the same distribution, or whether the
 estimated/predicted values are from the same distribution as the true value,
 we use:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
D_{{n,n'}}=\sup_{x}|F_{{1,n}}(x)-F_{{2,n'}}(x)|
\]

\end_inset


\end_layout

\begin_layout Standard
The null hypothesis, two samples are from the same distribution, is rejected
 at level 
\begin_inset Formula $\alpha$
\end_inset

 if 
\begin_inset Formula $D_{{n,n'}}>c(\alpha)\sqrt{\frac{n+n'}{nn'}}$
\end_inset

.
 The value of 
\begin_inset Formula $c(\alpha)$
\end_inset

 is given in the table.
\end_layout

\begin_layout Standard
So for a model, the larger the KS statistic, the better.
\end_layout

\begin_layout Subsection

\series bold
Relative Operating Characteristic: 
\series default
ROC curve (Binary)
\end_layout

\begin_layout Standard
In signal detection theory, a 
\series bold
receiver/relative operating characteristic 
\series default
(ROC), is a graphical plot which illustrates the performance of a 
\series bold
binary
\series default
 classifier system as its discrimination threshold is varied.
\end_layout

\begin_layout Itemize
CDF of false positive rate FPR is given by 
\begin_inset Formula $FPR(T)=\int_{T}^{\infty}P_{0}(T)dT$
\end_inset

 
\end_layout

\begin_layout Itemize
CDF of true positive rate (sensitivity) is 
\begin_inset Formula $TPR(T)=\int_{T}^{\infty}P_{1}(T)dT$
\end_inset

.
 
\end_layout

\begin_layout Itemize
Then plot 
\begin_inset Formula $TPR(T)$
\end_inset

 in y-axis against 
\begin_inset Formula $FPR(T)$
\end_inset

 in x-axis
\end_layout

\begin_layout Standard
\align center
\begin_inset Graphics
	filename Lyx_Picture/ROC_Curve.png
	lyxscale 50
	scale 50

\end_inset


\end_layout

\begin_layout Standard

\series bold
For any given 
\begin_inset Formula $\alpha$
\end_inset

, ROC will tell you how many percentage defaults you truly indentify, but
 at the same time how many percentage of good people you reject.
\end_layout

\begin_layout Subsubsection*
Naive Model: Diagonal Line
\end_layout

\begin_layout Standard
A completely random guess, according to the 
\begin_inset Formula $P(Y)=\frac{\#Y}{\#N+\#Y}$
\end_inset

 would give a point along a diagonal line (the so-called line of no-discriminati
on).
\end_layout

\begin_layout Subsubsection
Evaluation the Model: AUC
\end_layout

\begin_layout Standard
Points above the diagonal represent good classification results (better
 than random), points below the line poor results (worse than random).
 Note that the output of a consistently poor predictor could simply be inverted
 to obtain a good predictor.
\end_layout

\begin_layout Section
Evaluate Model: based on PDF
\end_layout

\begin_layout Subsection
ANOVA in score model: Mean Difference
\end_layout

\begin_layout Standard
Assign estimated scores to good and bad people, then use ANOVA techniques
 to see whether there is significant difference
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
D=\frac{M_{g}-M_{b}}{S}
\]

\end_inset


\end_layout

\begin_layout Subsection
Information Value
\end_layout

\begin_layout Standard
用于看某 variable 
\begin_inset Formula $x$
\end_inset

 是否对binary result 
\begin_inset Formula $y$
\end_inset

 有prediction power.
\end_layout

\begin_layout Enumerate
For all the data, 
\begin_inset Formula $m_{1}=\#\{y=1\}$
\end_inset

 and 
\begin_inset Formula $m_{0}=\#\{y=0\}$
\end_inset

.
 Divide all data into 
\begin_inset Formula $K$
\end_inset

 (usually 
\begin_inset Formula $K=10)$
\end_inset

 pieces by 
\begin_inset Formula $x$
\end_inset

's values.
\end_layout

\begin_layout Enumerate
Within the 
\begin_inset Formula $k^{th}$
\end_inset

 piece, we have 
\begin_inset Formula $m_{1k}=\#\{y=1\}$
\end_inset

 and 
\begin_inset Formula $m_{0k}=\#\{y=0\}$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
IVx=\text{∑}_{i}^{10}(bad_{i}\text{−}good_{i})ln(\frac{bad_{i}}{good_{i}})
\]

\end_inset


\end_layout

\begin_layout Standard
Generalized Form
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
IV=\sum_{k=1}^{K}[\frac{m_{1k}}{m_{1}}-\frac{m_{0k}}{m_{0}}]ln(\frac{m_{1k}/m_{1}}{m_{0k}/m_{0}})
\]

\end_inset


\end_layout

\begin_layout Standard
where
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
 
\begin_inset Formula $ln(\frac{m_{1k}/m_{1}}{m_{0k}/m_{0}})$
\end_inset

 is called weighted of evidence.
\end_layout

\begin_layout Standard
If the classifier cannot classify, then it has same effect as a random variable,
 and we have 
\begin_inset Formula $\frac{m_{1k}}{m_{1}}\thickapprox\frac{m_{0k}}{m_{0}}$
\end_inset

.
 Then 
\begin_inset Formula $IV$
\end_inset

 would be smaller.
\end_layout

\begin_layout Standard
http://opinions5.blogspot.com/2011/09/information-value.html
\end_layout

\begin_layout Part
GLM I
\end_layout

\begin_layout Standard
Generalized Linear Model
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
G(y)=\alpha+\beta x
\]

\end_inset

where 
\begin_inset Formula $G$
\end_inset

 is the Link Function.
\end_layout

\begin_layout Standard
For example, Link Function for Logit regression is the odds ratio: 
\begin_inset Formula $G=log\frac{P(Y=1)}{1-P(Y=1)}$
\end_inset

 .
\end_layout

\begin_layout Section
OLS Basics
\end_layout

\begin_layout Subsection
Dimention
\end_layout

\begin_layout Labeling
\labelwidthstring 00.00.0000
\begin_inset Formula $X=N\times P$
\end_inset

, 
\begin_inset Formula $x_{i}=1\times P$
\end_inset

f
\end_layout

\begin_layout Labeling
\labelwidthstring 00.00.0000
\begin_inset Formula $y_{i}=1$
\end_inset

, 
\begin_inset Formula $Y=N\times1$
\end_inset


\end_layout

\begin_layout Labeling
\labelwidthstring 00.00.0000
\begin_inset Formula $\beta=P\times1$
\end_inset


\end_layout

\begin_layout Labeling
\labelwidthstring 00.00.0000
\begin_inset Formula $S(x)=x\beta$
\end_inset

 is the span in linear subspace of 
\begin_inset Formula $x$
\end_inset

.
 It repreasents a vector in the subspace 
\begin_inset Formula $x$
\end_inset

 -- the feature space.
\end_layout

\begin_layout Subsection
Minimize Cov of error Approach to get Beta: Geometry
\end_layout

\begin_layout Standard
http://www.econ.uiuc.edu/~wsosa/econ507/OLSGeometry.pdf
\end_layout

\begin_layout Itemize
For a dataset like below 
\begin_inset Formula 
\[
\left[Y,X\right]=\begin{bmatrix}y_{1} & x_{11} & \cdots & x_{p1}\\
y_{2} & x_{12} & \cdots & x_{p2}\\
\vdots & \vdots & \ddots & \vdots\\
y_{n} & x_{1n} & \cdots & x_{pn}
\end{bmatrix}
\]

\end_inset


\end_layout

\begin_layout Itemize

\series bold
Each column is a Point/Vector in the space with 
\begin_inset Formula $N$
\end_inset

 dimention.
 
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Graphics
	filename H:/jpmDesk/Desktop/Personal/Dropbox/Library/Lyx_Picture/OLSGeometry.png

\end_inset


\end_layout

\end_deeper
\begin_layout Itemize
Thos 
\begin_inset Formula $P$
\end_inset

 Points/Vector made by matrix 
\begin_inset Formula $X$
\end_inset

 can make up a plane
\begin_inset Formula $S(X)$
\end_inset

 (if there are only one features, 
\begin_inset Formula $S(X)$
\end_inset

 is a line, like the picture above), which can be seen as a linear space
 (feature space) 
\begin_inset Formula $S(X)$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Itemize
By definition of span 
\begin_inset Formula $S(X)$
\end_inset

, 
\begin_inset Formula $X\beta$
\end_inset

 must be on that space, or in this plane.
\end_layout

\begin_layout Itemize
\begin_inset Formula $X\beta$
\end_inset

 is a vector in the 
\begin_inset Formula $R^{N}$
\end_inset

space.
\end_layout

\begin_layout Itemize
\begin_inset Graphics
	filename H:/jpmDesk/Desktop/Personal/Dropbox/Library/Lyx_Picture/OLSGeometry2.png
	lyxscale 50
	scale 50

\end_inset


\end_layout

\end_deeper
\begin_layout Itemize
Least Square is to find 
\begin_inset Formula $\beta$
\end_inset

 that makes distance between point 
\begin_inset Formula $Y$
\end_inset

 and in 
\begin_inset Formula $S(X)$
\end_inset

, defined by 
\begin_inset Formula $X\beta$
\end_inset

, that overall are closest to 
\begin_inset Formula $Y$
\end_inset

, 
\end_layout

\begin_deeper
\begin_layout Itemize
residual 
\begin_inset Formula $u=Y-X\beta$
\end_inset

 is a vector in the 
\begin_inset Formula $R^{N}$
\end_inset

 space that is orthognal to the 
\begin_inset Formula $S(X)$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $X^{T}(Y-X\beta)=\overrightarrow{0}$
\end_inset

 
\end_layout

\end_deeper
\end_deeper
\begin_layout Subsection
Orthogonality principle (geometry) approch to get Beta
\end_layout

\begin_layout Standard
The error vector should be 
\series bold
perpendicular
\series default
 to all of the explanatory vectors.
 This may be expressed directly in terms of the matrix X:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
X^{T}(Y-\beta X)=0
\]

\end_inset


\end_layout

\begin_layout Standard
Solve this we get 
\begin_inset Formula $\beta=\left(X^{T}X\right)^{-1}X^{T}Y$
\end_inset


\end_layout

\begin_layout Subsection
about 
\begin_inset Formula $X^{T}X$
\end_inset


\end_layout

\begin_layout Standard
Matrix 
\begin_inset Formula $X^{T}X$
\end_inset

 will be diagonal, and the mth diagonal element will be the squared norm
 of the corresponding explanatory vector, 
\begin_inset Formula $||x_{m}||^{2}$
\end_inset

.
\end_layout

\begin_layout Standard
The inverse of this matrix will also be diagonal, with diagonal elements
 
\begin_inset Formula $1/||x_{m}||^{2}$
\end_inset

.
\end_layout

\begin_layout Subsection
Degrees of freedom
\end_layout

\begin_layout Itemize
\begin_inset Formula $df=n-k-1$
\end_inset

: df of OLS.
 
\series bold
degrees of freedom
\begin_inset Index idx
status open

\begin_layout Plain Layout
degrees of freedom
\end_layout

\end_inset


\series default
 is the number of values in the final calculation of a statistic that are
 free to vary.
\end_layout

\begin_layout Subsection
Error
\end_layout

\begin_layout Itemize
\begin_inset Formula $\hat{\sigma}^{2}=\sum\hat{u}^{2}/(n-k-1)=\frac{(Y-X\beta)'(Y-X\beta)}{df}=SSR/df$
\end_inset

: The unbiased estimator of error term 
\begin_inset Formula $\sigma^{2}$
\end_inset

.
\end_layout

\begin_layout Itemize
As (
\begin_inset Formula $N-k-1)\hat{\sigma}^{2}\sim\sigma^{2}\chi_{n-k-1}^{2}$
\end_inset


\end_layout

\begin_layout Standard
ESL p242
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
Err(x_{0})=E\left[Y-\hat{f_{p}}(x_{0})\right]^{2}|X=x_{0}=\sigma_{\epsilon}^{2}+\left[f(x_{0})-E\hat{f_{p}}(x_{0}))\right]^{2}+||h(x_{0})||^{2}\sigma_{\epsilon}^{2}
\]

\end_inset


\end_layout

\begin_layout Subsection
Variance and covariance of estimator
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray}
var(\hat{\beta_{j}}) & = & \frac{\hat{\sigma}^{2}}{SST_{j}(1-R_{j}^{2})}\label{eq:var of estimator}\\
 & \text{＝} & \hat{\sigma}^{2}v_{j}
\end{eqnarray}

\end_inset


\end_layout

\begin_layout Standard
where 
\begin_inset Formula $v_{j}$
\end_inset

 is the 
\begin_inset Formula $j^{th}$
\end_inset

 diagnoal element of matrix 
\begin_inset Formula $(X^{T}X)^{-1}$
\end_inset

.
 Thus you can derive the 
\begin_inset Formula $z$
\end_inset

 score: 
\begin_inset Formula $\frac{\hat{\beta}}{sd(\hat{\beta})}\sim T_{N-k-1}$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
cov(\hat{\beta}) & = & E\left[(\hat{\beta}-\beta)(\hat{\beta}-\beta)'\right]\\
 & = & E\left[(X'X)^{-1}X'ee'X(X'X)^{-1}\right]\\
 & = & (X^{T}X)^{-1}\sigma^{2}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
where
\end_layout

\begin_layout Itemize
\begin_inset Formula $\text{\beta}=\left(X^{T}X\right)^{-1}X^{T}(Y-e)$
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $E(ee')=\sigma^{2}I$
\end_inset


\end_layout

\begin_layout Itemize
\begin_inset Formula $SST_{J}=\sum(x_{ij}-x_{j})^{2}=(X^{T}X)$
\end_inset

 and 
\begin_inset Formula $R_{j}^{2}$
\end_inset

 from regression 
\begin_inset Formula $x_{j}\sim X_{-j}$
\end_inset


\end_layout

\end_deeper
\begin_layout Standard
Thus we have a multi-normial distribution
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\hat{\beta}\sim N(\beta,\mbox{ }(X^{T}X)^{-1}\sigma^{2})
\]

\end_inset


\end_layout

\begin_layout Subsection
\begin_inset Index idx
status open

\begin_layout Plain Layout
Hat Matrix / Influence Matrix
\end_layout

\end_inset

Hat Matrix / Influence Matrix
\end_layout

\begin_layout Itemize

\series bold
http://www.stat.ucla.edu/~cocteau/stat201b/lectures/
\end_layout

\begin_layout Itemize

\series bold
http://polisci.msu.edu/jacoby/icpsr/regress3/lectures/week3/11.Outliers.pdf
\end_layout

\begin_layout Standard
Hat matrix, H, sometimes also called influence matrix[1] and projection
 matrix, maps the vector of observed values to the vector of fitted values
 (or predicted values).
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\hat{y}=Hy
\]

\end_inset


\end_layout

\begin_layout Standard
where 
\begin_inset Formula $H=X(X^{T}X)^{-1}X^{T}$
\end_inset

.
\end_layout

\begin_layout Itemize
It describes the influence each observed value has on each fitted value
\end_layout

\begin_layout Itemize

\series bold
Property
\series default
: Symmetric and Idempotent (HH=H), but not diagonal.
\end_layout

\begin_layout Subsection

\series bold
Geometry explain: ESL P65
\end_layout

\begin_layout Standard
See ESL P61.
 Figure 3.1 and 3.2
\end_layout

\begin_layout Standard
Vectors span a subspace of 
\begin_inset Formula $R^{N}$
\end_inset

, also referred to as the column space of 
\begin_inset Formula $X$
\end_inset

.
 We minimize 
\begin_inset Formula 
\[
RSS(β)=∥y−Xβ∥^{2}
\]

\end_inset

 by choosing 
\begin_inset Formula $\hat{\beta}$
\end_inset

 so that the residual vector 
\begin_inset Formula $y\text{−}\text{\hat{y}}$
\end_inset

 is 
\series bold
orthogonal
\series default
 to this subspace.
 This orthogonality is expressed in (3.5) 
\begin_inset Formula $X^{T}(y\text{−}Xβ)=0$
\end_inset

, and the resulting estimate 
\begin_inset Formula $\text{\hat{y}}$
\end_inset

 is hence the orthogonal projection of y onto this subspace.
 
\end_layout

\begin_layout Standard
The hat matrix 
\begin_inset Formula $H$
\end_inset

 computes the orthogonal projection, and hence it is also known as a projection
 matrix.
\end_layout

\begin_layout Subsection
Intrepretation of 
\begin_inset Formula $\beta_{j}$
\end_inset


\end_layout

\begin_layout Standard
P70 ESL
\end_layout

\begin_layout Standard
Hence stated more generally, we have shown that the jth multiple regression
 coefficient is the univariate regression coefficient of y on 
\begin_inset Formula $x_{j·012...(j−1)(j+1)...,p}$
\end_inset

, the residual after regressing 
\begin_inset Formula $x_{j}$
\end_inset

 on 
\begin_inset Formula $x_{0}$
\end_inset

, 
\begin_inset Formula $x_{1}$
\end_inset

, .
 .
 .
 , 
\begin_inset Formula $x_{j−1}$
\end_inset

, 
\begin_inset Formula $x_{j+1}$
\end_inset

, .
 .
 .
 , 
\begin_inset Formula $x_{p}$
\end_inset

:
\end_layout

\begin_layout Standard
The multiple regression coefficient 
\begin_inset Formula $\hat{β}_{j}$
\end_inset

 represents the additional contribution of xj on y, after xj has been adjusted
 for x0, x1, .
 .
 .
 , xj−1, xj+1, .
 .
 .
 , xp.
\end_layout

\begin_layout Subsection

\series bold
Graphical Representation
\end_layout

\begin_layout Standard
Figure 3.1 illustrates the geometry of least-squares fitting in the 
\begin_inset Formula $\mathbb{R}^{p+1}$
\end_inset

-dimensional
\end_layout

\begin_layout Standard
This orthogonality is expressed in (3.5), and the resulting estimate 
\begin_inset Formula $\hat{y}$
\end_inset

 is hence the orthogonal projection of y onto this subspace.
 The hat matrix H computes the orthogonal projection, and hence it is also
 known as a projection matrix.
\end_layout

\begin_layout Subsection

\series bold
Consistency and Efficiency
\end_layout

\begin_layout Itemize

\series bold
Consistency
\series default
 of an estimator
\begin_inset Index idx
status open

\begin_layout Plain Layout
Consistency of an estimator
\end_layout

\end_inset

: 
\begin_inset Formula $\underset{n\to\infty}{lim}\hat{\theta}=\theta.$
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Consistent estimators are convergent and 
\series bold
asymptotically unbiased
\series default
 (hence converge to the correct value).
\end_layout

\begin_layout Itemize
For any unbiased estimator, if it can converge as 
\begin_inset Formula $n\rightarrow\infty$
\end_inset

 (variance goest to 0), then it must be consistent.
\end_layout

\end_deeper
\begin_layout Itemize

\series bold
Efficiency
\series default
 of an estimator
\begin_inset Index idx
status open

\begin_layout Plain Layout
Efficiency of an estimator
\end_layout

\end_inset

: Size of variance (or consistency of variance).
\end_layout

\begin_deeper
\begin_layout Itemize
It is the same as consistency of the estimated variance of that estimator.
 Thus inefficiency means you cannot fully trust the statistical inference.
\end_layout

\begin_deeper
\begin_layout Itemize

\series bold
Non-Efficiency may sometimes lead to smaller errors, leading to a false
 significance, for example the case of positive serial correlation of residuals.
\end_layout

\begin_layout Itemize
It may also lead to larger errors.
 Then if you see significance of estimators, you can trust this result as
 it will be still significant under robust errors.
\end_layout

\end_deeper
\end_deeper
\begin_layout Section
SVD in Least Square Optimization
\end_layout

\begin_layout Standard
reference: http://www.cns.nyu.edu/~eero/NOTES/leastSquares.pdf
\end_layout

\begin_layout Standard
In the more general situation that the columns of X are not orthogonal (
\series bold
multi-colinearity
\series default
), the solution is best understood by rewriting the explanatory matrix using
 the singular value decomposition (SVD), 
\begin_inset Formula $X=USV^{Tt}$
\end_inset

 , (where U and V are orthogonal, and S is diagonal).
\end_layout

\begin_layout Standard
The optimization problem is now written as
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
min_{\beta}||Y-USV^{T}\beta||^{2}
\]

\end_inset


\end_layout

\begin_layout Standard
We can express the error vector in a more useful coordinate system by multipying
 it by the matrix 
\begin_inset Formula $U^{T}$
\end_inset

 (note that this matrix is orthogonal and won’t change the vector length,
 and thus will not change the value of the error function):
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
min_{\beta}||U^{T}(Y-USV^{T}\beta)||^{2}=min_{\beta}||U^{T}Y-SV^{T}\beta||^{2}
\]

\end_inset

where we’ve used the fact that 
\begin_inset Formula $U^{T}$
\end_inset

 is the inverse of U (since U is orthogonal).
\end_layout

\begin_layout Standard
We can write the error function as 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
min_{\beta}||Y^{*}-S\beta^{*}||^{2}
\]

\end_inset


\end_layout

\begin_layout Standard
Why is this easier? The matrix 
\begin_inset Formula $S$
\end_inset

 is diagonal, and has 
\begin_inset Formula $M$
\end_inset

 columns.
 So the first M elements of the vector 
\begin_inset Formula $S\beta^{*}$
\end_inset

 are of the form 
\begin_inset Formula $S_{mm}p_{m}^{*}$
\end_inset

, and 
\series bold
remaining N−M elements are zero.
\end_layout

\begin_layout Standard
Thus the solution is 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\beta^{*}=S^{\#}Y^{*}
\]

\end_inset

where 
\begin_inset Formula $S^{\#}$
\end_inset

 is a diagonal matrix whose mth diagonal element is 
\begin_inset Formula $1/S_{mm}$
\end_inset

,.
 
\end_layout

\begin_layout Standard
Finally, we must transform our solution back to the original parameter space:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\beta=V\beta^{*}=VS^{\#}Y^{*}=VS^{\#}U^{T}Y
\]

\end_inset


\end_layout

\begin_layout Standard
You should be able to verify that this is equivalent to the solution we
 obtained using the orthogonality principle, 
\begin_inset Formula $\beta=\left(X^{T}X\right)^{-1}X^{T}Y$
\end_inset

, by substuting the SVD into the expression.
\end_layout

\begin_layout Section
Gauss-Markov Assumptions (for cross-sectional analysis)
\end_layout

\begin_layout Paragraph*

\series bold
1.
 Linear model
\series default
:
\end_layout

\begin_layout Paragraph*
2.
 Random sampling
\end_layout

\begin_layout Standard
Random sample of n observations (for cross-sectional analysis) from same
 population.
\end_layout

\begin_layout Standard
This framework allows one to state asymptotic results.
 Also allow all other properties of population can be applied onto the sample
 data.
\end_layout

\begin_layout Paragraph*

\series bold
3.
 Exogeneity
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
E(u|X)=0
\]

\end_inset


\end_layout

\begin_layout Itemize
which means that the period t disturbance is independent of current and
 past values of x, but not necessarily independent of future x
\end_layout

\begin_deeper
\begin_layout Itemize
When 
\begin_inset Formula $x$
\end_inset

 (covariates) are correlated with the 
\begin_inset Formula $\epsilon$
\end_inset

 error terms of a regression relationship, ordinary linear regression generally
 produces 
\series bold
biased
\series default
 and 
\series bold
inconsistent
\series default
 estimates.
\end_layout

\begin_layout Itemize
Such correlation may occur when a key variable is missing.
 (Residual Plot can check it)
\end_layout

\end_deeper
\begin_layout Paragraph

\series bold
4.
 No perfect collinearity: Full Rank
\end_layout

\begin_layout Enumerate
Multicollinearity does not reduce the predictive power or reliability of
 the model as a whole, at least within the sample data themselves; 
\end_layout

\begin_deeper
\begin_layout Enumerate
it only affects calculations regarding individual predictors (即individual
 variable's coefficient).
\end_layout

\end_deeper
\begin_layout Paragraph

\series bold
5.
 Homoskedasticity
\series default
:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
var(u^{t}|X)=\sigma^{2}
\]

\end_inset


\end_layout

\begin_layout Itemize
If Variance of Residuals are Heteroscedasticity?
\end_layout

\begin_deeper
\begin_layout Itemize
Inconsistent + Inefficiency.
 Biased standard errors lead to biased inference, so results of hypothesis
 tests are possibly wrong.
 
\end_layout

\begin_deeper
\begin_layout Itemize
The coefficient is still unbiased
\end_layout

\end_deeper
\begin_layout Itemize
In time series, a strong assumption requires 
\begin_inset Formula $X$
\end_inset

 stands for all 
\begin_inset Formula $K–1$
\end_inset

 regressors and all 
\begin_inset Formula $T$
\end_inset

 time periods:
\end_layout

\end_deeper
\begin_layout Itemize

\series bold
1: Linearity is about form of model
\end_layout

\begin_layout Itemize

\series bold
3-5 are about population.
\end_layout

\begin_layout Itemize

\series bold
2: is about whether 3-5 can be applied onto sample data asymptotically
\end_layout

\begin_layout Subsubsection*
Theorem from Assumptions:
\end_layout

\begin_layout Itemize
2+3+4 = Unbiasedness of estimator
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula 
\begin{eqnarray*}
E(\hat{\beta}) & = & E\left[(X'X)^{-1}X'Y\right]\\
 & = & E\left[(X'X)^{-1}X'(X\beta+u)\right]\\
 & = & (X'X)^{-1}X'X\beta=\beta
\end{eqnarray*}

\end_inset


\end_layout

\end_deeper
\begin_layout Itemize
2+3+4+5 together will give you variance of estimator as Formula 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:var of estimator"

\end_inset

.
 Note that you have to use 5 here.
\end_layout

\begin_deeper
\begin_layout Itemize
heteroskedasticity (failure of Assumption MLR.5) does not cause OLS to be
 biased.
 However, OLS no longer has the smallest variance among linear unbiased
 estimators in the presence of heteroskedasticity.
\end_layout

\end_deeper
\begin_layout Itemize

\series bold
Gauss-Markov Theorem
\series default
: 1+2+3+4+5 = BLUE
\end_layout

\begin_layout Itemize
1-5 is are called 
\series bold
Gauss-Markov Theorem
\series default

\begin_inset Index idx
status open

\begin_layout Plain Layout
Gauss-Markov Theorem
\end_layout

\end_inset

 and assure OLS is BLUE
\end_layout

\begin_layout Subsection
Asymptotic Normality Theorem
\end_layout

\begin_layout Standard
The Asymptotic Normality Theorem (Wooldridge书 p169)
\end_layout

\begin_layout Enumerate
\begin_inset Formula $\hat{\beta}_{j}$
\end_inset

 is asymptotically normally distributed with 
\end_layout

\begin_deeper
\begin_layout Enumerate
\begin_inset Formula 
\[
(\hat{\beta}_{j}-\beta_{j})/se(\hat{\beta}_{j})\sim^{asy}N(0,1)
\]

\end_inset


\end_layout

\begin_layout Enumerate
or more precisely is is actually 
\begin_inset Formula $\sim^{asy}T_{n-k-1}$
\end_inset


\end_layout

\end_deeper
\begin_layout Enumerate
\begin_inset Formula $\hat{\sigma}$
\end_inset

 is a consistent estimator of 
\begin_inset Formula $\sigma$
\end_inset


\end_layout

\begin_layout Paragraph
Assumptions in Time Series (see time series)
\end_layout

\begin_layout Subsection
Missing Key Variables
\end_layout

\begin_layout Itemize

\series bold
May
\series default
 lead to Heteroscedasticity..
\end_layout

\begin_layout Itemize
Although the omission of relevant regressors from the standard linear regression
 model 
\series bold
generally may bias
\series default
 the OLS parameter estimator, this bias vanishes if the included and excluded
 regressors are orthogonal (uncorrelated in the sample).
\end_layout

\begin_layout Section
Outliers
\end_layout

\begin_layout Standard

\series bold
Definition:
\end_layout

\begin_layout Itemize
A regression outlier is an observation that has an unusual value of the
 dependent variable Y, conditional on its value of the independent variable
 X 
\end_layout

\begin_layout Itemize
In other words,for a regression outlier, neither theX nor the Y value is
 necessarily unusual on its own.
\end_layout

\begin_layout Subsubsection
Leverage Effect
\begin_inset Index idx
status open

\begin_layout Plain Layout
Leverage Effect
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Leverage is a measure of how much each data point influences the regression.
 Because the regression must pass through the centroid, points that lie
 far from the centroid have greater leverage, and their leverage increases
 if there are fewer points nearby.
 As a result, leverage reflects both the distance from the centroid and
 the isolation of a point.
\end_layout

\begin_layout Standard
\begin_inset Index idx
status open

\begin_layout Plain Layout
High Leverage Points
\end_layout

\end_inset

High Leverage Points: note that this is different from outliers: An observation
 that has an unusual X value—i.e., it is far from the mean of X—has 
\series bold
leverage on (i.e., the potential to influence, just potential)
\series default
 the regression line.
\end_layout

\begin_layout Standard
It is measured by the "leverage" 
\begin_inset Formula $h_{ii}$
\end_inset

 (also called hat values
\begin_inset Index idx
status open

\begin_layout Plain Layout
hat values
\end_layout

\end_inset

) is the 
\begin_inset Formula $i^{th}$
\end_inset

 diagonal entry in the 
\series bold
hat matrix
\series default
.
 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
h_{ii}=\frac{1}{n}+\frac{x_{i}-\bar{x}}{\sum(x_{j}-\bar{x})^{2}}
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset CommandInset ref
LatexCommand ref
reference "ISL"

\end_inset

 P112
\end_layout

\begin_layout Itemize
The further away from mean of 
\begin_inset Formula $X$
\end_inset

 the large the observation's leverage effect.
 
\end_layout

\begin_layout Itemize
High leverage does not necessarily mean that it influences the regression
 coefficients – It is possible to have a high leverage and yet follow straight
 in line with the pattern of the rest of the data.
 In that case, 
\series bold
the observation is not an outlier.
\end_layout

\begin_layout Itemize
Outliers may not have an high leverage, when 
\begin_inset Formula $x_{i}$
\end_inset

 is close to 
\begin_inset Formula $\bar{x}$
\end_inset

 but 
\begin_inset Formula $y_{i}$
\end_inset

 is higher than it should be.
 Then in this case this observation does not have very large impact on the
 value of coeff.
 
\end_layout

\begin_layout Itemize

\series bold
Small samples are especially vulnerable to high leverage points.
\end_layout

\begin_layout Itemize
The most dangeous thing is an observation being both an outlier and high-leverag
e
\end_layout

\begin_layout Subsubsection
Cook's distance
\end_layout

\begin_layout Standard
R diagnostic plot also contours values of Cook’s distance, which measures
 how much the regression would change if a point was deleted.
 Cook’s distance is increased by leverage and by large residuals: a point
 far from the centroid with a large residual can severely distort the regression
\end_layout

\begin_layout Standard
On the 4th diagnostic plot in R, you want to see that the red smoothed line
 stays close to the horizontal gray dashed line and that no points have
 a large Cook’s distance (i.e, >0.5).
 Both are true here.
\end_layout

\begin_layout Subsubsection
Identify outliers
\end_layout

\begin_layout Enumerate
Hat values 
\begin_inset Formula $h_{ii}$
\end_inset

 exceeding 
\series bold
about twice the average hat-value
\series default
 should be considered noteworthy
\end_layout

\begin_layout Enumerate
Or larger than 
\begin_inset Formula $(p+1)/n$
\end_inset

 where 
\begin_inset Formula $p$
\end_inset

 is the number parameters and 
\begin_inset Formula $n$
\end_inset

 is number of accounts (http://www.stat.ucla.edu/~cocteau/stat201b/lectures/lecture
3.pdf).
\end_layout

\begin_layout Subsection
Solve it
\end_layout

\begin_layout Itemize
"Unequal error variance is worth correcting only when the problem is severe."
\end_layout

\begin_layout Itemize
"heteroscedasticity has never been a reason to throw out an otherwise good
 model.
\end_layout

\begin_layout Enumerate
Bypass it, not solve it: 
\series bold
Use Heteroscedasticity-consistent standard error
\series default
s: 
\begin_inset Formula $var(\hat{\beta})$
\end_inset

 is a function of 
\begin_inset Formula $\sigma_{i}^{2}$
\end_inset

(now as Heteroscedasticity exists, each 
\begin_inset Formula $\hat{\epsilon_{i}}$
\end_inset

 has different 
\begin_inset Formula $\sigma_{i}^{2}$
\end_inset

 ) and 
\begin_inset Formula $x$
\end_inset

 to make it did not change with value of 
\begin_inset Formula $x.$
\end_inset


\end_layout

\begin_layout Enumerate

\series bold
View logged data
\series default
.
 Unlogged series that are growing exponentially often appear to have increasing
 variability as the series rises over time.
 log() function will punish larger values in exponentially harh ways.
 The variability in percentage terms may, however, be rather stable.
 
\end_layout

\begin_deeper
\begin_layout Enumerate
That means, sometimes Not using dollar amount, using percent (log)
\end_layout

\end_deeper
\begin_layout Enumerate
Use a 
\series bold
different specification
\series default
 for the model (different X variables, or perhaps non-linear transformations
 of the X variables).
 [Find an 
\begin_inset Formula $x$
\end_inset

 to catch the variance pattern of 
\begin_inset Formula $y$
\end_inset

]
\end_layout

\begin_layout Enumerate
Apply a 
\series bold
weighted least squares estimation method
\series default
: for example, the weights are directly related to the magnitude of the
 dependent variable.
\end_layout

\begin_layout Subsection
Standardized Residuals
\end_layout

\begin_layout Standard
Standardized Residuals are just are residuals rescaled so that they have
 a mean of zero and a variance of one.
 Often used in R's diagnostic plot.
\end_layout

\begin_layout Subsection
Check Model: Residual vs.
 Fitted Plot
\end_layout

\begin_layout Standard
Both of these two plots should not have any pattern.
 You can identify whether we need another key term, like timing term or
 squared term.
\end_layout

\begin_layout Enumerate
Residuals versus fitted
\end_layout

\begin_layout Enumerate
Scale-location plot: Studentized residuals versus fitted
\end_layout

\begin_layout Itemize
A trend in the mean residuals suggests a violation in the assumption of
 independent response variables.
 --- BIG problem: biased (Omitted-variable bias)
\end_layout

\begin_layout Itemize
A trend in the variability of the residuals suggests that the variance is
 related to the mean, violating the constant variance assumption--- Heteroscedas
ticity
\end_layout

\begin_layout Subsection
\begin_inset Index idx
status open

\begin_layout Plain Layout
QQ plot
\end_layout

\end_inset

QQ plot
\end_layout

\begin_layout Itemize
Q stands for quantile is a probability plot, which is a graphical method
 for comparing two probability distributions by plotting their quantiles
 against each other.
 
\end_layout

\begin_layout Itemize
Often 
\begin_inset Formula $y$
\end_inset

 is values (from 
\begin_inset Formula $-\infty$
\end_inset

to 
\begin_inset Formula $+\infty$
\end_inset

 ) from empirical distribution (residuals in our case) and the 
\begin_inset Formula $x$
\end_inset

 is values (from 
\begin_inset Formula $-\infty$
\end_inset

 to 
\begin_inset Formula $+\infty$
\end_inset

 ) from standard normal.
\end_layout

\begin_layout Itemize
A point (-3,-8) means a -3 value in normal distribution has the same quantitle
 as the -8 value in Empirical distirbution.
\end_layout

\begin_layout Section
Serial Correlation (见 time series)
\end_layout

\begin_layout Subsection*
Serial correlation in Panel data
\end_layout

\begin_layout Standard
Because serial correlation in linear panel-data models biases the standard
 errors and causes the results to be less efficient, researchers need to
 identify serial correlation in the idiosyncratic error term in a panel-data
 model.
\end_layout

\begin_layout Standard
Robinson (1982) showed that, when residuals are serially correlated, MLE
 remain unbiased in large samples (consistency) but they are not efficient.
 This result is precisely analogous to that of autocorrelated linear models.
\end_layout

\begin_layout Subsection*
Serial correlation in Time Series
\end_layout

\begin_layout Section
Spurious correlation in cross-sectional
\end_layout

\begin_layout Standard
This is also the 
\series bold
Reverse Causality or Confounding (迷惑) variables
\series default
.
 “Spurious correlation” is to describe a situation where two variables are
 related through their 
\series bold
correlation with a third variable
\series default
.
 In particular, if we regress y on x, we find a significant relationship.
 But when we control for another variable, say z, the partial effect of
 x on y becomes zero.
 
\end_layout

\begin_layout Itemize
The dependent variable causes at least one of the covariates ("reverse"
 causation), when there are relevant explanatory variables which are omitted
 from the model.
\end_layout

\begin_layout Itemize
Reverse Causation is through that omitted relevant variable.
 So control it to cut off the reverse causation.
\end_layout

\begin_layout Section
Heteroscedasticity
\end_layout

\begin_layout Enumerate
\begin_inset Formula $Var(\hat{\epsilon}_{i}|x)=\sigma^{2}$
\end_inset

 for all 
\begin_inset Formula $i$
\end_inset

, or in multi-variate: 
\begin_inset Formula $E[uu']\ne\sigma^{2}I_{n}$
\end_inset

 .
\end_layout

\begin_layout Enumerate
That means, with conditional expectations of 
\begin_inset Formula $Y_{t}$
\end_inset

 given 
\begin_inset Formula $X_{t}$
\end_inset

, the sequence 
\begin_inset Formula ${Y_{t}}$
\end_inset

 is said to be heteroscedastic if the conditional variance of 
\begin_inset Formula $Y_{t}$
\end_inset

 given 
\begin_inset Formula $X_{t}$
\end_inset

, changes with 
\begin_inset Formula $t$
\end_inset


\end_layout

\begin_layout Enumerate
If 
\begin_inset Formula $x$
\end_inset

 is independent (strong 
\series bold
Exogeneity
\series default
) with 
\begin_inset Formula $\hat{\epsilon}$
\end_inset

, then homoscedasticity should hold naturally.
 But normally we only require a weak version of 
\series bold
Exogeneity: 
\begin_inset Formula $E(\hat{\epsilon}|x)=0$
\end_inset


\end_layout

\begin_layout Standard

\series bold
Example:
\series default
 A classic example of heteroscedasticity is that of income versus expenditure
 on meals.
 As one's income increases, the variability of food consumption will increase.
 A poorer person will spend a rather constant amount by always eating inexpensiv
e food; a wealthier person may occasionally buy inexpensive food and at
 other times eat expensive meals.
 Those with higher incomes display a greater variability of food consumption.
 
\end_layout

\begin_layout Subsection
Heteroscedasticity in Non-Linear GLM
\end_layout

\begin_layout Itemize
http://davegiles.blogspot.com/2011/05/gripe-of-day.html
\end_layout

\begin_layout Itemize
In OLS, Heteroscedasticity will not affect consistency and unbiadness.
\end_layout

\begin_layout Itemize
However, these results change (for the worse) in the context of such non-linear
 models as Logit, Probit, Tobit, etc.
 For example, in the presence of heteroskedasticity, the 
\series bold
MLE's of the parameters of these models are inconsistent.
\end_layout

\begin_deeper
\begin_layout Itemize
I (Fan) think the reason might be, missing a key variable in Logit and cause
 Heteroscedasticity, for example, means your MLE specification is essentially
 wrong! So the estimators you got from MLE is also wrong.
\end_layout

\end_deeper
\begin_layout Itemize
So, worrying about reporting 'robust' standard errors is of second-order
 importance.
 There are much more serious matters to be concerned about! If the parameter
 estimates themselves are inconsistent, it doesn't matter how big your sample
 is, or what you do to 'patch up' the (meaningless) standard errors, you're
 in over your head! Similarly, the omission of relevant covariates (
\series bold
even if they are uncorrelated with the included ones
\series default
) also renders the MLE's of the parameters themselves inconsistent in Logit
 and Probit models.
\end_layout

\begin_layout Itemize

\series bold
So we need to
\end_layout

\begin_deeper
\begin_layout Enumerate
Check the Heteroscedasticity.
 
\end_layout

\begin_layout Enumerate
Check the mis-specification in GLM: 
\end_layout

\begin_layout Itemize
\begin_inset Index idx
status open

\begin_layout Plain Layout
Lagrange Multiplier (LM) tests
\end_layout

\end_inset

Lagrange Multiplier (LM) tests for this and other types of mis-specification
 in Logit, Probit, and related models have long been available
\end_layout

\end_deeper
\begin_layout Subsection
Check Heteroscedasticity
\end_layout

\begin_layout Enumerate
Just plot the error with value of 
\begin_inset Formula $x$
\end_inset

, if the dispersion of error change with 
\begin_inset Formula $x$
\end_inset

, then there is a problem.
\end_layout

\begin_layout Enumerate
Test Heteroscedasticity: Breusch–Pagan test
\begin_inset Index idx
status open

\begin_layout Plain Layout
Breusch–Pagan test
\end_layout

\end_inset

 / 
\begin_inset Index idx
status open

\begin_layout Plain Layout
White test
\end_layout

\end_inset

White test
\end_layout

\begin_deeper
\begin_layout Enumerate
Null: NO Heteroscedasticity.
\end_layout

\begin_layout Enumerate
In R, this test is performed by function ncvTest available in the car package,
 or by function bptest available in the lmtest package.
\end_layout

\begin_layout Enumerate
Step 1: construct normal regression
\end_layout

\begin_layout Enumerate
\begin_inset Formula 
\[
y=\beta_{0}+\beta_{1}x+u
\]

\end_inset


\end_layout

\begin_layout Enumerate
Step 2: see whether 
\begin_inset Formula $\sigma^{2}$
\end_inset

 is related to values of 
\begin_inset Formula $x$
\end_inset

 (auxiliary regression) and do a F test:
\end_layout

\begin_layout Enumerate
\begin_inset Formula 
\[
\hat{u}^{2}=\gamma_{0}+\gamma_{1}x+v
\]

\end_inset


\end_layout

\end_deeper
\begin_layout Subsection
Solve it
\end_layout

\begin_layout Itemize
"Unequal error variance is worth correcting only when the problem is severe."
\end_layout

\begin_layout Itemize
"heteroscedasticity has never been a reason to throw out an otherwise good
 model.
\end_layout

\begin_layout Enumerate
Bypass it, not solve it: 
\series bold
Use Heteroscedasticity-consistent standard error
\series default
s: 
\begin_inset Formula $var(\hat{\beta})$
\end_inset

 is a function of 
\begin_inset Formula $\sigma_{i}^{2}$
\end_inset

(now as Heteroscedasticity exists, each 
\begin_inset Formula $\hat{\epsilon_{i}}$
\end_inset

 has different 
\begin_inset Formula $\sigma_{i}^{2}$
\end_inset

 ) and 
\begin_inset Formula $x$
\end_inset

 to make it did not change with value of 
\begin_inset Formula $x.$
\end_inset


\end_layout

\begin_layout Enumerate

\series bold
View logged data
\series default
.
 Unlogged series that are growing exponentially often appear to have increasing
 variability as the series rises over time.
 The variability in percentage terms may, however, be rather stable.
 
\end_layout

\begin_deeper
\begin_layout Enumerate
That means, sometimes Not using dollar amount, using percent (log)
\end_layout

\end_deeper
\begin_layout Enumerate
Use a 
\series bold
different specification
\series default
 for the model (different X variables, or perhaps non-linear transformations
 of the X variables).
 [Find an 
\begin_inset Formula $x$
\end_inset

 to catch the variance pattern of 
\begin_inset Formula $y$
\end_inset

]
\end_layout

\begin_layout Enumerate
Apply a 
\series bold
weighted least squares estimation method
\series default
: for example, the weights are directly related to the magnitude of the
 dependent variable.
\end_layout

\begin_layout Subsection
Weighted Regression
\end_layout

\begin_layout Standard
To minimize 
\begin_inset Formula $sum(w_{i}\times e_{i}^{2})$
\end_inset

, instead of 
\begin_inset Formula $sum(e^{2})$
\end_inset

.
 
\end_layout

\begin_layout Standard
That means you create new data 
\begin_inset Formula $\sqrt{w}Y$
\end_inset

 and 
\begin_inset Formula $\sqrt{w}X$
\end_inset

 and try to minimize 
\begin_inset Formula $(\sqrt{w}Y-\sqrt{w}\beta'X)'(\sqrt{w}Y-\sqrt{w}\beta'X)$
\end_inset

, where 
\begin_inset Formula $w$
\end_inset

 is a vector (not a constant!)
\end_layout

\begin_layout Standard
But, in software, 
\begin_inset Formula $w$
\end_inset

 will not show up in the fomula 
\begin_inset Formula $y\sim x+e$
\end_inset

, it only shows up in the max of MLE process.
\end_layout

\begin_layout Subsubsection
Usage
\end_layout

\begin_layout Itemize
For Heteroscedasticity, if we know the noise variance 
\begin_inset Formula $σ_{i}^{2}$
\end_inset

 at each measurement i, and set 
\begin_inset Formula $w_{i}=1/σ_{i}^{2}$
\end_inset

, we get the heteroskedastic MLE, and perfectly recover efficiency.
\end_layout

\begin_layout Itemize
If you think observation 
\begin_inset Formula $i$
\end_inset

 has a 
\begin_inset Formula $\sqrt{w}_{i}$
\end_inset

 importance (might be number of observations 
\begin_inset Formula $i$
\end_inset

 represents), then you can add 
\begin_inset Formula $\sqrt{w_{i}}$
\end_inset

 as the weight.
 
\end_layout

\begin_layout Subsubsection
Weighted Error
\end_layout

\begin_layout Standard
So 
\begin_inset Formula $W$
\end_inset

 is the diagnoal matrix with diagnal as weights.
 
\begin_inset Formula 
\[
\hat{\sigma}_{w}^{2}=\frac{(Y-\hat{Y})^{T}W(Y-\hat{Y})}{n-p-1}
\]

\end_inset


\end_layout

\begin_layout Standard
Note that the output of softwares are normally unweighted errors 
\begin_inset Formula $e_{i}=Y-\hat{Y}$
\end_inset

.
 You have to derive 
\begin_inset Formula $e_{i,W}=\sqrt{w_{i}}e_{i}$
\end_inset

 to do residual analysis by your self.
\end_layout

\begin_layout Subsubsection
Weighted Variance of Estimates
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
Var(\hat{\beta}_{w})=\sigma_{w}^{2}(XWX)^{-1}
\]

\end_inset


\end_layout

\begin_layout Subsubsection
Weighted Regression in R
\end_layout

\begin_layout Standard
The output residual 
\begin_inset Formula $e$
\end_inset

in R is still 
\begin_inset Formula $e=Y-\hat{Y}$
\end_inset

.
 But in the residual summary of the weighted least squares analysis, this
 is based on 
\begin_inset Formula $e_{i}\sqrt{w_{i}}$
\end_inset

 instead of the raw residuals 
\begin_inset Formula $e_{i}$
\end_inset

.
 In addition, to see if a reasonable weighting has been done, plot 
\begin_inset Formula $e_{i}\sqrt{w_{i}}$
\end_inset

 instead of 
\begin_inset Formula $e_{i}$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout

<<>>=
\end_layout

\begin_layout Plain Layout

et.seed(100)
\end_layout

\begin_layout Plain Layout

n = 100 
\end_layout

\begin_layout Plain Layout

x = rnorm(n,0,3)
\end_layout

\begin_layout Plain Layout

std = sapply(x,function(x){1+0.5*x^2}) 
\end_layout

\begin_layout Plain Layout

noise =   rnorm(n,0,std) 
\end_layout

\begin_layout Plain Layout

y = 3-2*x + noise
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

Weight = 1/std^2 
\end_layout

\begin_layout Plain Layout

Weight_root = Weight^0.5
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

fit_correct = lm(y~x, weights= Weight)
\end_layout

\begin_layout Plain Layout

fit_wrong = lm(y~x, weights= Weight_root)
\end_layout

\begin_layout Plain Layout

fit_no = lm(y~x)
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

# coeffs are quite similae
\end_layout

\begin_layout Plain Layout

fit_correct$coeff
\end_layout

\begin_layout Plain Layout

fit_wrong$coeff
\end_layout

\begin_layout Plain Layout

fit_no$coeff
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

# fit_correct$residuals is still the traditional definition.
 
\end_layout

\begin_layout Plain Layout

residuals = y- (cbind(1,x) %*% fit_correct$coefficients)
\end_layout

\begin_layout Plain Layout

sum(residuals - fit_correct$residuals)
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

# well behaviored 
\end_layout

\begin_layout Plain Layout

plot(x,(fit_correct$residuals*Weight_root))
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

# NOT well behaviored 
\end_layout

\begin_layout Plain Layout

plot(x,(fit_wrong$residuals*Weight_root^0.5))
\end_layout

\begin_layout Plain Layout

plot(x,(fit_no$residuals))
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

@
\end_layout

\end_inset


\end_layout

\begin_layout Section
Collinearity
\end_layout

\begin_layout Subsection
Identify Collinearity: VIF
\end_layout

\begin_layout Standard
\begin_inset CommandInset ref
LatexCommand ref
reference "ISL"

\end_inset

 P116
\end_layout

\begin_layout Enumerate
correlation matrix
\end_layout

\begin_layout Enumerate
VIF: Unfortunately, not all collinearity problems can be detected by inspection
 of the correlation matrix: it is possible for collinear- ity to exist between
 three or more variables even if no pair of variables has a particularly
 high correlation.
 We call this situation 
\series bold
multicollinearity
\series default
.
 
\end_layout

\begin_layout Standard
Instead of inspecting the correlation matrix, a better way to assess multi-
 collinearity is to compute the variance inflation factor (VIF)
\begin_inset Index idx
status open

\begin_layout Plain Layout
variance inflation factor (VIF)
\end_layout

\end_inset

.
 The VIF is the ratio of the variance of βˆj when fitting the full model
 divided by the variance of βˆj if fit on its own.
 The smallest possible value for VIF is 1, which indicates the complete
 absence of collinearity.
 
\end_layout

\begin_layout Standard
Typically in practice there is a small amount of collinearity among the
 predictors.
 As a rule of thumb, a VIF value that exceeds 5 or 10 indicates a problematic
 amount of power
\end_layout

\begin_layout Subsection
Consequense
\end_layout

\begin_layout Standard
Collinearity results in a decline in the t-statistic.
 As a result, in the presence of collinearity, we may fail to reject H0
 : βj = 0.
 This means that the power of the hypothesis test—the probability of correctly
 detecting a non-zero coefficient—is reduced by collinearity.
\end_layout

\begin_layout Subsection
Solution
\end_layout

\begin_layout Enumerate
The first is to drop one of the problematic variables from the regression.
 
\end_layout

\begin_layout Enumerate
The second solution is to combine the collinear variables together into
 a single predictor.
 For in- stance, we might take the average of standardized versions of limit
 and rating in order to create a new variable that measures credit worthiness.
\end_layout

\begin_layout Part
GLM II
\end_layout

\begin_layout Section
MLE theory
\end_layout

\begin_layout Subsection
Likelihood
\end_layout

\begin_layout Standard
Suppose there is a sample 
\begin_inset Formula $x_{1},x_{2},…,x_{n}$
\end_inset

 of n independent and identically distributed observations, coming from
 a distribution with an unknown probability density function 
\begin_inset Formula $f_{0}(·)$
\end_inset

.
 It is however surmised that the function 
\begin_inset Formula $f_{0}$
\end_inset

 belongs to a certain family of distributions {
\begin_inset Formula $ f(·| θ),θ∈Θ $
\end_inset

} (where θ is a vector of parameters for this family), called the parametric
 model, so that 
\begin_inset Formula $f_{0}=f(·| θ_{0}).$
\end_inset

 
\end_layout

\begin_layout Standard
The value 
\begin_inset Formula $θ_{0}$
\end_inset

 is unknown and is referred to as the true value of the parameter vector.
\end_layout

\begin_layout Standard
Then this function will be called the likelihood:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\mathcal{L}(\theta;x_{1},\ldots,x_{n})=f(x_{1},x_{2},\ldots,x_{n}\mid\theta)=\prod_{i=1}^{n}f(x_{i}\mid\theta)
\]

\end_inset


\end_layout

\begin_layout Standard
Then This method of estimation defines a maximum-likelihood estimator (MLE)
 of 
\begin_inset Formula $θ_{0}$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\{\hat{\theta}_{\mathrm{mle}}\}\subseteq\{arg\,max\hat{\ell}(\theta\,;\,x_{1},\ldots,x_{n})\}
\]

\end_inset


\end_layout

\begin_layout Subsection
Deviance
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
D(y)=-2\Big(\log\big(p(y\mid\hat{\theta}_{0})\big)-\log\big(p(y\mid\hat{\theta}_{s})\big)\Big)
\]

\end_inset


\end_layout

\begin_layout Standard
Here 
\begin_inset Formula $\hat{\theta}_{0}$
\end_inset

 denotes the fitted values of the parameters in the model 
\begin_inset Formula $M_{0}$
\end_inset

, while 
\begin_inset Formula $\hat{\theta}_{s}$
\end_inset

 denotes the fitted parameters for the "full model" (or "saturated model")
\end_layout

\begin_layout Standard
"the quantity
\begin_inset Formula $-2\log\big(p(y\mid\hat{\theta}_{0})\big)$
\end_inset

 is sometimes referred to as a deviance.
 This is [...] inappropriate, since unlike the deviance used in the context
 of generalized linear modelling, 
\begin_inset Formula $-2\log\big(p(y\mid\hat{\theta}_{0})\big)$
\end_inset

 does not measure deviation from a model that is a perfect fit to the data."
 However, since the principal use is in the form of the difference of the
 deviances of two models, this confusion in definition is unimportant.
 
\end_layout

\begin_layout Standard
The “−2” in the definition makes the log-likelihood loss for the Gaussian
 distribution match squared-error loss.
\end_layout

\begin_layout Subsection
Residual Check
\end_layout

\begin_layout Standard
\begin_inset CommandInset href
LatexCommand href
name "R code and logit errors"
target "http://freakonometrics.hypotheses.org/8210"

\end_inset


\end_layout

\begin_layout Standard
Also the error terms (the residuals) do not need to be multivariate normally
 distributed.
 
\end_layout

\begin_layout Standard
In the graph below, the middle line you see is lowess (local) regression,
 which is nothing but polynomial fitting (do not care about the color)
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout

<<>>=
\end_layout

\begin_layout Plain Layout

# http://freakonometrics.hypotheses.org/8210
\end_layout

\begin_layout Plain Layout

# http://web.as.uky.edu/statistics/users/pbreheny/760/S13/notes/3-26.pdf FOR
 DIFFERENT RESIDUALS
\end_layout

\begin_layout Plain Layout

n=250 
\end_layout

\begin_layout Plain Layout

set.seed(1) 
\end_layout

\begin_layout Plain Layout

X1=rnorm(n) 
\end_layout

\begin_layout Plain Layout

X2=rnorm(n) 
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

score=X1^ 2 +X2-1 
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

proba=exp(score)/(1+exp(score)) # the logit function 
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

# do bernouli trail (single time binomal) for n times with theta = proba
\end_layout

\begin_layout Plain Layout

Y=rbinom(n,1 ,proba) 
\end_layout

\begin_layout Plain Layout

mean(Y)
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

reg=glm(Y~X1+X2,family=binomial) 
\end_layout

\begin_layout Plain Layout

plot(reg,which=1)
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

# For plotting the estimated effects in the model, try the effects package.
 library(effects) plot(allEffects(yourmodel))
\end_layout

\begin_layout Plain Layout

@
\end_layout

\end_inset


\end_layout

\begin_layout Description
Explain The errors are 
\begin_inset Formula $Y-exp$
\end_inset


\end_layout

\begin_layout Section
Model Unbiasedness Check
\end_layout

\begin_layout Subsection
Studentized residual 
\begin_inset Index idx
status open

\begin_layout Plain Layout
Studentized residual 
\end_layout

\end_inset


\end_layout

\begin_layout Standard
http://en.wikipedia.org/wiki/Studentized_residual
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
var(\widehat{\varepsilon}_{i})=\sigma^{2}(1-h_{ii})
\]

\end_inset


\end_layout

\begin_layout Standard
Thus the Studentized residual is 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\frac{\widehat{\varepsilon}_{i}}{\widehat{\sigma}\sqrt{1-h_{ii}\ }}
\]

\end_inset


\end_layout

\begin_layout Subsubsection*
Why use Studentized residual to identify Heteroscedasticity
\end_layout

\begin_layout Itemize
Moreover, and most importantly, the residuals, unlike the errors, do not
 all have the same variance: the variance decreases as the corresponding
 x-value gets farther from the average x-value.
 
\end_layout

\begin_layout Itemize
This is a feature of the regression better fitting values at the ends of
 the domain, not the data itself, and is also reflected in the influence
 functions of various data points on the regression coefficients: endpoints
 have more influence.
 
\end_layout

\begin_layout Itemize
This can also be seen because the residuals at endpoints depend greatly
 on the slope of a fitted line, while the residuals at the middle are relatively
 insensitive to the slope.
 
\end_layout

\begin_layout Itemize
The fact that the variances of the residuals differ, even though the variances
 of the true errors are all equal to each other, is the principal reason
 for the need for studentization.
\end_layout

\begin_layout Subsection
Check Model: Residual vs.
 Fitted Plot
\end_layout

\begin_layout Standard
Both of these two plots should not have any pattern.
 You can identify whether we need another key term, like timing term or
 squared term.
\end_layout

\begin_layout Enumerate
Residuals versus fitted
\end_layout

\begin_layout Enumerate
Scale-location plot: Studentized residuals versus fitted
\end_layout

\begin_layout Itemize
A trend in the mean residuals suggests a violation in the assumption of
 independent response variables.
 --- BIG problem: biased (Omitted-variable bias)
\end_layout

\begin_layout Itemize
A trend in the variability of the residuals suggests that the variance is
 related to the mean, violating the constant variance assumption--- Heteroscedas
ticity
\end_layout

\begin_layout Subsection
\begin_inset Index idx
status open

\begin_layout Plain Layout
QQ plot
\end_layout

\end_inset

QQ plot
\end_layout

\begin_layout Itemize
Q stands for quantile is a probability plot, which is a graphical method
 for comparing two probability distributions by plotting their quantiles
 against each other.
 
\end_layout

\begin_layout Itemize
Often one is empirical distribution (residuals in our case) and the other
 one is standard normal.
\end_layout

\begin_layout Standard

\series bold
Explain: See the R Code : Model file.
\end_layout

\begin_layout Itemize
If the general trend of the Q–Q plot is steeper than the line y = x, the
 distribution plotted on the vertical axis is more dispersed than the distributi
on plotted on the horizontal axis.
 
\end_layout

\begin_layout Itemize
Like below: a comparison between T(5) distribution and standard normal distribut
ion.
\end_layout

\begin_layout Standard
\align center
\begin_inset Graphics
	filename H:/jpmDesk/Desktop/Personal/Research_personal/Lyx_Picture/QQ_plot1.jpg

\end_inset


\end_layout

\begin_layout Standard
\align center
\begin_inset Graphics
	filename H:/jpmDesk/Desktop/Personal/Research_personal/Lyx_Picture/QQ_plot2.jpg

\end_inset


\end_layout

\begin_layout Section
Model Fit
\end_layout

\begin_layout Subsection
Model Form
\end_layout

\begin_layout Subsubsection
Empirical
\end_layout

\begin_layout Standard
Better to start from a simple model, and then slowly adding more variables
 each time, to see the effect.
\end_layout

\begin_layout Itemize
to the power of 
\begin_inset Formula $n$
\end_inset

? Quadratic? their economic meaning.
\end_layout

\begin_layout Itemize
Interaction terms and their economic meaning.
\end_layout

\begin_deeper
\begin_layout Itemize
For interactions, it is better to first test its level form in the model,
 if there is obviously no relationship between a factor and survival, then,
 it is not likely to be a confounder.
\end_layout

\end_deeper
\begin_layout Itemize
Stepwise AIC/BIC
\end_layout

\begin_deeper
\begin_layout Itemize
Stating model: a really simple model.
 This is the most simplest model that the algorithm will search.
\end_layout

\begin_layout Itemize
End model: a really complex model.This is the most complex model that the
 algorithm will search.
\end_layout

\begin_layout Itemize
The algorithm will randomly check all models between Starting model and
 End model and choose the one with biggest AIC.
\end_layout

\end_deeper
\begin_layout Subsubsection
F-test for OLS
\end_layout

\begin_layout Standard
Note that when errors are not normal this statistic becomes invalid, and
 other tests such as for example Wald test or LR test should be used.
\end_layout

\begin_layout Subsection
Model Form Check
\end_layout

\begin_layout Itemize
The contribution of individual regression coefficients: 
\series bold
Wald’s tests in MLE
\series default
, or t-test in OLS to get the 
\begin_inset Formula $Z-value$
\end_inset


\end_layout

\begin_layout Itemize
The contribution of several coefficients simultaneously: 
\series bold
Deviance tests in MLE
\series default
, or F-tests in OLD.
\end_layout

\begin_layout Subsubsection
\begin_inset Formula $\chi^{2}$
\end_inset

-Test for Logit: To decide wether a variable is needed / Compare two models
 (similar as 
\begin_inset Formula $F$
\end_inset

-test in OLS)
\end_layout

\begin_layout Standard
Assume model 2 has the additional variable where model 1 does not have
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
G & \sim & \chi^{2}=\\
 & = & D_{1}-D_{2}\\
 & = & -2LogLike_{1}-(-2Lolike_{2})\\
 & = & -2log(\frac{L_{1}}{L_{2}})
\end{eqnarray*}

\end_inset

where 
\begin_inset Formula $D$
\end_inset

 means Deviance, It is the fit of the observed values (Y) to the expected
 values.
 The bigger the difference (or "deviance") of the observed values from the
 expected values, the poorer the fit of the model.
\end_layout

\begin_layout Subsubsection
Different Group/ Structural Change: Chow Test
\end_layout

\begin_layout Standard
Suppose that we model our data as
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
y_{t}=a+bx_{1t}+cx_{2t}+\varepsilon
\]

\end_inset

, If we split our data into two groups, then we have 
\begin_inset Formula $y_{t}=a_{1}+b_{1}x_{1t}+c_{1}x_{2t}+\varepsilon$
\end_inset

, and 
\begin_inset Formula $y_{t}=a_{2}+b_{2}x_{1t}+c_{2}x_{2t}+\varepsilon$
\end_inset

, 
\end_layout

\begin_layout Itemize
\begin_inset Formula $H_{0}$
\end_inset

: betas for two groups are different (Does need interaction of the group
 indicator dummy with each variable))
\end_layout

\begin_layout Itemize
\begin_inset Formula $H_{1}$
\end_inset

: Each beta in (7.23) is the same across the two groups.
 (no need interaction of the group indicator dummy with each variable)
\end_layout

\begin_layout Standard
Chow statistics follows the F distribution with 
\begin_inset Formula $k$
\end_inset

 and 
\begin_inset Formula $N_{1}+N_{2}-2k$
\end_inset

 degrees of freedom:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\frac{(S_{C}-(S_{1}+S_{2}))/(k)}{(S_{1}+S_{2})/(N_{1}+N_{2}-2k)}.
\]

\end_inset


\end_layout

\begin_layout Itemize
\begin_inset Formula $S_{C}$
\end_inset

 be the sum of squared residuals from the combined data.
\end_layout

\begin_layout Itemize
\begin_inset Formula $S_{1}$
\end_inset

be the sum of squared residuals from the first group
\end_layout

\begin_layout Itemize
\begin_inset Formula $S_{2}$
\end_inset

be the sum of squared residuals from the second group.
 
\end_layout

\begin_layout Itemize
\begin_inset Formula $N_{1}$
\end_inset

 and 
\begin_inset Formula $N_{2}$
\end_inset

 are the number of observations in each group a
\begin_inset Formula $k$
\end_inset

is the total number of parameters (in this case, 3)
\end_layout

\begin_layout Subsubsection
Unknown structural break
\end_layout

\begin_layout Standard
\begin_inset Foot
status open

\begin_layout Plain Layout
[This section is based on Stock & Watson, 3/e, pp.
 558-561.
 See references there to Quandt (1960) and Andrews (2003).] Also see text_Ch_4_Re
gression with Non-stationary Variables.
\end_layout

\end_inset


\end_layout

\begin_layout Standard
We don't know where is the break, we don't even know whether there is a
 break.
 So QLR is to find
\end_layout

\begin_layout Itemize
Whether there is a break
\end_layout

\begin_layout Itemize
When is the break
\end_layout

\begin_layout Standard

\series bold
For all possible break points 
\begin_inset Formula $\tau_{1}....\tau_{n}$
\end_inset

, do Chow test for them
\series default
 and choose the max
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
QLR=max\{F_{\tau_{1}},F_{\tau_{2}}.....F_{\tau_{n}}\}
\]

\end_inset

where 
\begin_inset Formula $QLR$
\end_inset

 is a statistics with complicated distribution
\end_layout

\begin_layout Itemize
We can check QLR statistics with critical values to decide whether there
 is a break
\end_layout

\begin_layout Itemize
The max 
\begin_inset Formula $F_{\tau_{i}}$
\end_inset

 is the most possible break
\end_layout

\begin_layout Standard
In order to test whether two sub-samples have the same coefficients, we
 must have enough observations in each subsample to get reliable coefficient
 estimates This means that we cannot detect or test potential breakpoints
 that are close to either end of the sample.
 (which means not suggest to define any possible 
\begin_inset Formula $\tau_{i}$
\end_inset

 near ends).
\end_layout

\begin_layout Standard
A conventional choice is to trim 15% of the observations from each end of
 the sample, looking for breakpoints only within the central 70% of observations.
 
\end_layout

\begin_layout Subsection
General Fitting
\end_layout

\begin_layout Subsubsection
Pseudo 
\begin_inset Formula $R^{2}$
\end_inset

 (McFadden Generalized 
\begin_inset Formula $R^{2}$
\end_inset

) (Logistic 
\begin_inset Formula $R^{2}$
\end_inset

, or 
\begin_inset Formula $R^{2}$
\end_inset

 in any MLE)(
\begin_inset Formula $LR^{2}$
\end_inset

) 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
R_{Psedeu}^{2}=1-\frac{log(likelihood_{full})}{log(likelihood_{null})}
\]

\end_inset

where 
\begin_inset Formula $full$
\end_inset

 means the full model, 
\begin_inset Formula $null$
\end_inset

 means to only include intercept and exclude any independent variables
\end_layout

\begin_layout Subsubsection
Information Entropy (GLM)
\end_layout

\begin_layout Standard
Information Criteria: the smaller, the better
\end_layout

\begin_layout Standard
Akaike information criterion (AIC)
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
AIC=2k-2ln(L)
\]

\end_inset

where 
\begin_inset Formula $k$
\end_inset

 is the number of parameters.
\end_layout

\begin_layout Section
Prediction
\end_layout

\begin_layout Subsection
Error: Prediction vs.
 Fitted Value
\end_layout

\begin_layout Standard
A prediction 
\begin_inset Formula $y_{0}=x_{0}\hat{\beta}+u_{0}=\hat{y}_{0}+u_{0}$
\end_inset

.
 So it has two sources of variance:
\end_layout

\begin_layout Enumerate
\begin_inset Index idx
status open

\begin_layout Plain Layout
Fitted Value Error
\end_layout

\end_inset

 Fitted Value Error: 
\begin_inset Formula $var(\hat{y}_{0})=var(x'_{0}\hat{\beta})=||h(x_{0})||^{2}\sigma_{\epsilon}^{2}=x_{0}'\sigma^{2}(X'X)^{-1}x_{0}$
\end_inset


\end_layout

\begin_deeper
\begin_layout Enumerate
\begin_inset Formula $x'_{0}$
\end_inset

 is just a scaler vector.
\end_layout

\begin_layout Enumerate
refer Variance of estimator 
\begin_inset Formula $\hat{\beta}$
\end_inset

as Formula 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:var of estimator"

\end_inset

.
\end_layout

\begin_layout Enumerate
相比与prediction error 中的 unoberserved error 部分，here fitted value error 亦叫
 (in) sampling error。
\end_layout

\begin_layout Enumerate
Average of in-sample error is (with 
\begin_inset Formula $x_{0}$
\end_inset

 taken to be each of the sample values 
\begin_inset Formula $x_{i}$
\end_inset

) is (ESL P243 no derive....) 
\begin_inset Formula 
\[
p/N\sigma^{2}
\]

\end_inset


\end_layout

\end_deeper
\begin_layout Enumerate
Unobserved error: The previous method (just look at Fitted Value Error)
 allows us to put a confidence interval around the OLS estimate of E(y$x1,…,xk),
 for any values of the explanatory variables.
 But this is not the same as obtaining a confidence interval for a new,
 as yet unknown, outcome on y.
 In forming a confidence interval for an outcome on y, we must account for
 another very important source of variation: the variance in the unobserved
 error.
\end_layout

\begin_deeper
\begin_layout Enumerate
其实 用 fitted error 还是 prediction error 主要是哲学上的争论。如果依然认为是用in sample 的data去做预测，则可以用
 fitted error。若认为是在forecasting，是在用新的data，则需要用prediction error。
\end_layout

\begin_layout Enumerate
Thus the total prediction error 
\begin_inset Formula $Var(e^{0})=Var(y^{0})+Var(\mu^{0})=Var(y^{0})+\sigma^{2}$
\end_inset

.
 
\end_layout

\end_deeper
\begin_layout Enumerate
In ESL 243, when talking about the sources of errors, it gives three sources
 of 
\series bold
all possible
\series default
 errors: error + bias + variance
\end_layout

\begin_deeper
\begin_layout Standard
\begin_inset Formula 
\[
Err(x_{0})=E\left[\left(Y-\hat{f_{p}}(x_{0})\right)^{2}|X=x_{0}\right]=\sigma_{\epsilon}^{2}+\left[f(x_{0})-E\hat{f_{p}}(x_{0}))\right]^{2}+||h(x_{0})||^{2}\sigma_{\epsilon}^{2}
\]

\end_inset


\end_layout

\begin_layout Standard
where hat matrix 
\begin_inset Formula $h(x_{0})=X(X^{T}X)^{-1}x_{0}$
\end_inset

 (thus 
\begin_inset Formula $h^{2}(x_{0})=x_{0}^{'}(X^{T}X)^{-1}x_{0}$
\end_inset

).
 Average of it is 
\begin_inset Formula $p/N\sigma^{2}$
\end_inset

.
\end_layout

\begin_layout Enumerate
That means: in sample estimation only considers the variance of model
\end_layout

\begin_layout Enumerate
prediction error will also include the error.
 (if we assume our model is unbiased, then the biadness = 0 )
\end_layout

\end_deeper
\begin_layout Labeling
\labelwidthstring 00.00.0000
See:
\end_layout

\begin_layout Enumerate
http://courses.washington.edu/b515/l6.pdf for fitted value interval
\end_layout

\begin_layout Enumerate
Wooldridge 书p201: for prediction error
\end_layout

\begin_layout Part
Shrinkage Regression
\end_layout

\begin_layout Section
Shrinkage Regression: Ridge and Lasso
\end_layout

\begin_layout Standard
They are essentially GLM with punishment factor
\end_layout

\begin_layout Subsection
Ridge
\end_layout

\begin_layout Standard
ISL P265.
 R function
\end_layout

\begin_layout Standard
ridge regression penalizes the size of the regression coefficients Specifically,
 the ridge regression estimate 
\begin_inset Formula $\beta$
\end_inset

 is defined as the value of β that minimizes
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
RSS_{eidge}=\sum(y-\beta^{T}x)^{2}+\lambda\sum\beta^{2}
\]

\end_inset


\end_layout

\begin_layout Itemize
Shrinkage penalty 
\begin_inset Formula $\lambda\sum\beta^{2}$
\end_inset

 can pull coeffs to 0 ( not shrink the intercept)
\end_layout

\begin_layout Itemize
Select 
\begin_inset Formula $\lambda$
\end_inset

, the tuning factor, is done by CV.
 
\end_layout

\begin_deeper
\begin_layout Itemize

\series bold
Variance vs Bias
\series default
: As λ increases, the flexibility of the ridge regression fit decreases,
 leading to decreased variance but increased bias.
\end_layout

\end_deeper
\begin_layout Itemize
Ridge regression is not robust to scale change (OLS is).
 So you need to standardize them.
\end_layout

\begin_layout Itemize
Works when p > n, where the OLS do not even have a unique solution, whereas
 ridge regression can still perform well by trading off a small increase
 in bias for a large decrease in variance.
 Hence, ridge regression works best in situations where the least squares
 estimates have high variance.
\end_layout

\begin_layout Subsection
Lasso
\end_layout

\begin_layout Standard

\emph on
Difference between Ridge and Lasso, and ℓ1 ℓ2 penalty visualization ISL
 P233
\end_layout

\begin_layout Standard

\series bold
Just replace the shrinkage penalty with 
\begin_inset Formula $\lambda\sum|\beta|$
\end_inset


\end_layout

\begin_layout Itemize

\series bold
In statistical parlance, the lasso uses an ℓ1 (pronounced “ell 1”) penalty
 instead of an ℓ2 penalty.
\end_layout

\begin_layout Itemize
As with ridge regression, the lasso shrinks the coefficient estimates towards
 zero.
 However, in the case of the lasso, the ℓ1 penalty has the effect of forcing
 some of the coefficient estimates to be exactly equal to zero when the
 tuning parameter λ is sufficiently large.
 
\end_layout

\begin_deeper
\begin_layout Itemize
Hence, much like best subset selection, the lasso performs variable selection.
 
\end_layout

\begin_layout Itemize
As a result,models generated from the lasso are generally much easier to
 interpret than those produced by ridge regression.
 
\end_layout

\begin_layout Itemize
We say that the lasso yields sparse models—that is, sparse models that involve
 only a subset of the variables.
\end_layout

\end_deeper
\begin_layout Section
Kernal Shrinkge regression
\end_layout

\begin_layout Standard
Ref: Kernel ridge Regression MaxWelling Department of Computer Science Universit
y of Toronto.
 http://homes.cs.washington.edu/~pedrod/papers/cacm12.pdf
\end_layout

\begin_layout Standard
Minmize sum of squared error 
\begin_inset Formula 
\[
min_{w}\sum\epsilon_{i}^{2}
\]

\end_inset


\end_layout

\begin_layout Standard
subject to 
\begin_inset Formula 
\[
||w||^{2}\le B
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
y_{i}-w^{T}\Phi_{i}=\epsilon_{i}
\]

\end_inset


\end_layout

\begin_layout Standard
This leads to the Lagrangian,
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
L_{p}=\sum_{i}\epsilon^{2}+\sum\beta_{i}[y_{i}-w^{T}\Phi_{i}-\epsilon_{i}]+\lambda(||w||^{2}-B)
\]

\end_inset


\end_layout

\begin_layout Standard
Two of the KKT conditions tell us that at the solution we have:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
2\epsilon_{i}=\beta_{i}
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
2\lambda w=\sum_{i}\beta_{i}\Phi_{i}
\]

\end_inset


\end_layout

\begin_layout Standard
Plugging it back into the Lagrangian, we obtain the dual Lagrangian,
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
L_{D}=\sum_{i}(-\frac{1}{4}\beta_{i}^{2}+\beta_{i}y_{i})-\frac{1}{4\lambda}\sum_{ij}(\beta_{i}\beta_{j}K_{ij})-\lambda B^{2}
\]

\end_inset


\end_layout

\begin_layout Standard
We now redefine 
\begin_inset Formula $\alpha_{i}=\beta_{i}/(2\lambda)$
\end_inset

 to arrive at the following dual optimization problem,
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
max_{\alpha,\lambda}-\lambda^{2}\sum_{i}\alpha+2\lambda^{2}\sum_{i}\alpha_{i}y_{i})-\lambda\sum_{ij}(\alpha_{i}\alpha_{j}K_{ij})-\lambda B^{2}
\]

\end_inset


\end_layout

\begin_layout Standard
subject to 
\begin_inset Formula $\lambda\ge0$
\end_inset


\end_layout

\begin_layout Standard
Taking derivatives w.r.t.
 
\begin_inset Formula $\alpha$
\end_inset

 gives precisely the solution we had already found,
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\alpha_{i}^{*}=(K+\lambda I)^{-1}y
\]

\end_inset


\end_layout

\begin_layout Standard
Formally we also need to maximize over 
\begin_inset Formula $\lambda$
\end_inset

 .
 However, different choices of ¸ correspond to different choices for B.
 Either 
\begin_inset Formula $\lambda$
\end_inset

 or B should be chosen using cross-validation or some other measure, so
 we could as well vary 
\begin_inset Formula $\lambda$
\end_inset

 in this process.
 One big disadvantage of the ridge-regression is that we don’t have sparseness
 in the 
\begin_inset Formula $\alpha$
\end_inset

 vector, i.e.
 there is no concept of support vectors.
 This is useful because when we test a new example, we only have to sum
 over the support vectors which is much faster than summing over the entire
 training-set.
 In the SVM the sparseness was born out of the inequality constraints because
 the complementary slackness conditions told us that either if the constraint
 was inactive, then the multiplier 
\begin_inset Formula $\alpha_{i}$
\end_inset

 was zero.
 There is no such effect here.
\end_layout

\begin_layout Part
Basis Expansions and Regularization
\end_layout

\begin_layout Section
Aadditive expansion in a set of elementary “basis” functions
\end_layout

\begin_layout Standard
ESL 158
\end_layout

\begin_layout Standard
The core idea in this chapter is to augment/replace the vector of inputs
 X with additional variables, which are transformations of X, and then use
 linear models in this new space of derived input features.
\end_layout

\begin_layout Paragraph
\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
Linear basis expansion in X
\end_layout

\end_inset

 Linear basis expansion in X:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
f(x)=\sum_{m=1}^{M}\beta_{m}h_{m}(X)
\]

\end_inset


\end_layout

\begin_layout Standard
where 
\begin_inset Formula $h_{m}:R^{p}\rightarrow R$
\end_inset

.
 The beauty of this approach is that once the basis functions 
\begin_inset Formula $h_{m}$
\end_inset

 have been determined, the models are linear in these new variables, and
 the fitting proceeds as before.
\end_layout

\begin_layout Standard
Example: 
\begin_inset Formula $h_{m}(X)=X_{j}^{2}$
\end_inset

or 
\begin_inset Formula $h_{m}(X)=X_{j}X_{k}$
\end_inset


\end_layout

\begin_layout Subsection
Usage
\end_layout

\begin_layout Standard
ESL P357.
\end_layout

\begin_layout Standard
More generally, we add parameter 
\begin_inset Formula $\gamma_{m}$
\end_inset

 in the basis function, 
\begin_inset Formula $f(x)=\sum_{m=1}^{M}\beta_{m}h_{m}(X,\gamma_{m})$
\end_inset


\end_layout

\begin_layout Standard
Additive expansions like this are at the heart of many of the learning technique
s covered in this book:
\end_layout

\begin_layout Itemize
In single-hidden-layer neural networks (Chapter 11), 
\begin_inset Formula $b(x;γ)=σ(γ_{0}+γ_{1}^{T}x)$
\end_inset

, where 
\begin_inset Formula $σ(t)=1/(1+e^{−t})$
\end_inset

 is the sigmoid function, and 
\begin_inset Formula $γ$
\end_inset

 parameterizes a linear combination of the input variables.
 
\end_layout

\begin_layout Itemize
In addboost.m1 algorithm, 
\begin_inset Formula $G(x)=sign\left[\sum_{m=1}^{M}a_{m}G_{m}(x)\right]$
\end_inset

 is the final forecasting function.
\end_layout

\begin_layout Itemize
In signal processing, wavelets (Section 5.9.1) are a popular choice with 
\begin_inset Formula $γ$
\end_inset

 parameterizing the location and scale shifts of a “mother” wavelet.
 
\end_layout

\begin_layout Itemize
Multivariate adaptive regression splines (Section 9.4) uses truncatedpower
 spline basis functions where γ parameterizes the variables and values for
 the knots.
\end_layout

\begin_layout Subsection
Forward Stagewise Additive Modeling (see Boosting)
\end_layout

\begin_layout Standard
Forward stagewise modeling approximates the solution to (10.4) by sequentially
 adding new basis functions to the expansion without adjusting the parameters
 and coefficients of those that have already been added.
 
\end_layout

\begin_layout Enumerate
Initialize 
\begin_inset Formula $f_{0}(x)=0$
\end_inset

.
\end_layout

\begin_layout Enumerate
For m = 1 to M:
\end_layout

\begin_deeper
\begin_layout Enumerate
Compute
\end_layout

\begin_deeper
\begin_layout Standard
\begin_inset Formula 
\[
(β_{m},γ_{m})=argmin_{\beta,\gamma}\sum_{i=1}^{N}L(y_{i},f_{m-1}(x_{i})+\beta(x_{i};\gamma))
\]

\end_inset


\end_layout

\begin_layout Enumerate
That simpliy requires to fit a model with one basis function to the residual
 
\begin_inset Formula $y_{i}-f_{m-1}(x_{i})$
\end_inset


\end_layout

\end_deeper
\begin_layout Enumerate
Set 
\begin_inset Formula $f_{m}(x)=f_{m-1}(x_{i})+\beta_{m}b(x;\gamma_{m})$
\end_inset


\end_layout

\end_deeper
\begin_layout Standard
We now show that AdaBoost.M1 (Algorithm 10.1) is equivalent to forward stagewise
 additive modeling (Algorithm 10.2) using the loss function (see ESL P359
 for proof) 
\begin_inset Formula 
\[
L(y,f(x))=exp(−yf(x))
\]

\end_inset


\end_layout

\begin_layout Section
Piecewise Polynomials and Splines
\end_layout

\begin_layout Standard
ESL P160.
\end_layout

\begin_layout Subsection
Piecewise function
\end_layout

\begin_layout Standard
A typical piecewise function could be 
\begin_inset Formula $h_{m}(X)=I(\epsilon\le X)$
\end_inset

 or 
\begin_inset Formula $h_{m}(X)=(X-\epsilon)_{+}$
\end_inset

 where 
\begin_inset Formula $+$
\end_inset

 represents the postive parts, where 
\begin_inset Formula $\epsilon$
\end_inset

 is called 
\series bold
knot.
\end_layout

\begin_layout Subsection
Cubic spline
\end_layout

\begin_layout Standard
See ISLA P287 for ideas.
\end_layout

\begin_layout Standard
cubic spline is the line that has continuous first (to insure continuity)
 and second derivatives (to insure smooth) at the knots.
 
\end_layout

\begin_layout Standard
It is claimed that 
\series bold
cubic splines
\series default
 are the lowest-order spline for which the knot-discontinuity is not visible
 to the human eye.
 There is seldom any good reason to go beyond cubic-splines, unless one
 is interested in smooth derivatives.
\end_layout

\begin_layout Standard
The most direct way to represent a cubic spline using (7.9) is to start off
 with a basis for a cubic polynomial—namely, x, x2, x3—and then add one
 truncated power basis function per knot.
 truncated A truncated power basis function is defined as
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
h(x,\xi)=(x-\xi)_{+}^{3}
\]

\end_inset


\end_layout

\begin_layout Standard
A cubic spline with K knots can be modeled using basis 
\begin_inset Formula $1$
\end_inset

, 
\begin_inset Formula $x$
\end_inset

, 
\begin_inset Formula $x^{2}$
\end_inset

, 
\begin_inset Formula $x^{3}$
\end_inset

 and 
\begin_inset Formula $h(x,\xi_{k})$
\end_inset

 where 
\begin_inset Formula $k$
\end_inset

 is for each knots.
 Therefore, we costs 4 + K degree of freedoms
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
y=\beta_{0}+\sum_{1}^{3}\beta_{i}x^{i}+\sum_{1}^{k}h(x,\xi_{k})\label{eq:cubic_line_model}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
See ESL P162 for graph.
\end_layout

\begin_layout Subsection
General Case
\end_layout

\begin_layout Standard
the general form for the truncated-power basis set would be
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
h_{j}(X)=X^{j-1},j=1...M
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
h_{M+l}(X)=(X-\xi_{l})^{M-1},l=1...K
\]

\end_inset


\end_layout

\begin_layout Standard
Normally, when we call a M-order smooth line, when 
\begin_inset Formula $M=4$
\end_inset

, that mean it is a cubic spline
\end_layout

\begin_layout Subsection
Regression Splines
\end_layout

\begin_layout Standard
Regression Splines are just using OLS method with those basis functions
 as new sets.
\end_layout

\begin_layout Standard

\series bold
These fixed-knot splines are also known as regression splines.
 So the equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:cubic_line_model"

\end_inset

 represents a most common cubic splines, or regression splines.
 
\end_layout

\begin_layout Standard
ISL p285
\end_layout

\begin_layout Standard
To solve the problem: when there is time-series like 
\begin_inset Formula $y$
\end_inset

 (a line), try to explain it using features 
\begin_inset Formula $x$
\end_inset

.
\end_layout

\begin_layout Enumerate
Separate 
\begin_inset Formula $y$
\end_inset

 into several intervals
\end_layout

\begin_layout Enumerate
Constuct the basis sets as new variables, do the simple regression.
 You got the model!
\end_layout

\begin_layout Subsection
Natural Spline
\end_layout

\begin_layout Standard
Unfortunately, splines can have high variance at the outer range of the
 predictors—that is, when X takes on either a very small or very large value.
 Figure 7.4 shows a fit to the Wage data with three knots.
 We see that the confidence bands in the boundary region (when 
\begin_inset Formula $x$
\end_inset

 is smaller than the smallest knot and larger than the largest) appear fairly
 wild.
 
\end_layout

\begin_layout Standard
A natural spline is a regression spline with additional boundary constraints:
 the function is required to be linear at the boundary (in the region where
 X is smaller than the smallest knot, or larger than the largest knot).
 
\end_layout

\begin_layout Standard
For a cubic spline, Mathematically, that means
\end_layout

\begin_layout Itemize
After 
\begin_inset Formula $K$
\end_inset

 the largest knot, and before the 
\begin_inset Formula $k$
\end_inset

, the smallest knot, second or larger derivative is 0 and first derivative
 is a costant.
\end_layout

\begin_layout Standard
Therefore, you need to carefully construct the basis function to fit that
 condition (see p165 ESL)
\end_layout

\begin_layout Subsection
Choosing the Number and Locations of the Knots
\end_layout

\begin_layout Standard
ISL p288
\end_layout

\begin_layout Standard
One option is to place more knots in places where we feel the function might
 vary most rapidly, and to place fewer knots where it seems more stable.
\end_layout

\begin_layout Subsection
Smoothing Splines
\end_layout

\begin_layout Standard
Find the function g that minimizes
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
RSS=\sum(y_{i}-g(x_{i}))^{2}+\lambda\int g^{''}(t)^{2}dt
\]

\end_inset


\end_layout

\begin_layout Standard
where 
\begin_inset Formula $x$
\end_inset

 is more like the timing factor, like month on book for accounts, and 
\begin_inset Formula $g^{''}(t)^{2}$
\end_inset

 is the punishment factor to insure smoothness.
 Without it the function will choose 
\begin_inset Formula $g$
\end_inset

 such that it interpolates all of the 
\begin_inset Formula $y_{i}$
\end_inset

.
 
\end_layout

\begin_layout Standard
The notation g′′(t) indicates the second derivative of the function g.
 The first derivative g′(t) measures the slope of a function at t, and the
 second derivative corresponds to the amount by which the slope is changing.
 Hence, broadly speaking, the second derivative of a function is a measure
 of its roughness: it is large in absolute value if g(t) is very wiggly
 near t, and it is close to zero otherwise.
\end_layout

\begin_layout Subsection
\begin_inset Index idx
status open

\begin_layout Plain Layout
Local Weigted Regression (Loess)
\end_layout

\end_inset

Local Weigted Regression (Loess) MOVE TO GLM
\end_layout

\begin_layout Standard
\begin_inset Foot
status open

\begin_layout Plain Layout
Stanford Lecture Vedio 3.
 Lecture Notes 1: ML_1_OLS_GradientDescent_Normal
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Only look at the local vicinity 附近 of 
\begin_inset Formula $x$
\end_inset

effwfw_23r3445
\end_layout

\begin_layout Enumerate
Plug all the samples 
\begin_inset Formula $x^{i}$
\end_inset

 into 
\begin_inset Formula $w^{a}(x^{i})$
\end_inset

 to get the weights around 
\begin_inset Formula $x^{a}$
\end_inset

 anchor point.
\end_layout

\begin_deeper
\begin_layout Standard
\begin_inset Formula 
\[
w^{a}(x^{i})=exp(\frac{x^{i}-x^{a}}{2\tau})^{2}
\]

\end_inset


\end_layout

\begin_layout Itemize
where 
\begin_inset Formula $w$
\end_inset

 follows the exponential decay function (a bell shape) of 
\begin_inset Formula $x$
\end_inset

.
 
\end_layout

\begin_layout Itemize
\begin_inset Formula $\tau$
\end_inset

 is band width factor, controls the decaying speed.
 When 
\begin_inset Formula $\tau$
\end_inset

 is small, then the shape of 
\begin_inset Formula $w(x)$
\end_inset

 is a very narrow and concentrated shape around 
\begin_inset Formula $x$
\end_inset

.
 When 
\begin_inset Formula $\tau$
\end_inset

 is large, the weight 
\begin_inset Formula $w(x)$
\end_inset

 has a fat tails.
\end_layout

\begin_layout Itemize
If 
\begin_inset Formula $|x^{i}-x|$
\end_inset

 is small, then 
\begin_inset Formula $w^{i}\approx1$
\end_inset

.
 If 
\begin_inset Formula $|x^{i}-x|$
\end_inset

 is large, then 
\begin_inset Formula $w^{i}\approx0$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Enumerate
Fit 
\begin_inset Formula $\theta^{a}$
\end_inset

 to minimize 
\end_layout

\begin_deeper
\begin_layout Standard
\begin_inset Formula 
\[
\sum w^{i}(y^{i}-(\theta^{a})^{T}x^{i})^{t}
\]

\end_inset


\end_layout

\end_deeper
\begin_layout Enumerate
Thus for every data point, you do step 1-2 and get 
\begin_inset Formula $\theta^{a}$
\end_inset

 for every anchor point 
\begin_inset Formula $x^{a}$
\end_inset

.
 In this way you biuld the model.
\end_layout

\begin_deeper
\begin_layout Enumerate
LOESS curve fitted to a population sampled from a sine wave with uniform
 noise added.
 The LOESS curve approximates the original sine wave.
\end_layout

\begin_layout Enumerate
Note that LOESS is a non-parametic method, as you will have different 
\begin_inset Formula $\theta$
\end_inset

 for each observation.
\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename Lyx_Picture/Loess_curve.png
	lyxscale 50
	scale 30

\end_inset


\end_layout

\end_deeper
\begin_layout Subsection
Generalized Additive Models
\end_layout

\begin_layout Standard
instead of fitting with features directly, we fit the splines of each feature
 
\begin_inset Formula $y=\beta+\sum f_{j}(x_{j})+\epsilon$
\end_inset


\end_layout

\begin_layout Standard
Tree Based Models
\end_layout

\end_body
\end_document
